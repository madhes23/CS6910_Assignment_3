{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'tam'\n",
    "start_token = '<'\n",
    "end_token = '>'\n",
    "pad_token = ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'aksharantar_sampled/' + language\n",
    "\n",
    "train_df = pd.read_csv(path+'/'+language+'_train.csv', header=None)\n",
    "test_df = pd.read_csv(path+'/'+language+'_test.csv', header=None)\n",
    "val_df = pd.read_csv(path+'/'+language+'_valid.csv', header=None)\n",
    "\n",
    "train_source, train_target = train_df[0].tolist(), train_df[1].tolist()\n",
    "test_source, test_target = test_df[0].tolist(), test_df[1].tolist()\n",
    "val_source, val_target = val_df[0].tolist(), val_df[1].tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thottacharya  -->  தொட்டாச்சார்ய\n",
      "menmaithaan  -->  மென்மைதான்\n",
      "avarantri  -->  அவரன்றி\n",
      "mudiyarathu  -->  முடியறது\n",
      "aadaiyanigalaal  -->  ஆடையணிகளால்\n"
     ]
    }
   ],
   "source": [
    "num_sample = 5\n",
    "for i in range(num_sample):\n",
    "    print(f'{train_source[i]}  -->  {train_target[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Charecters :  49\n",
      "Target Charecters: \n",
      "0 த\t\n",
      "1  \t2 ெ\t3 ஈ\t4 ப\t5 ஹ\t\n",
      "6 வ\t7 ல\t8 ே\t9 ஞ\t10 ர\t\n",
      "11 ச\t12 எ\t13 ஐ\t14 ஸ\t15 ௌ\t\n",
      "16 ஓ\t17 >\t18 ீ\t19 ங\t20 ஆ\t\n",
      "21 ோ\t22 அ\t23 ய\t24 ி\t25 ு\t\n",
      "26 ஒ\t27 ை\t28 ந\t29 ஜ\t30 <\t\n",
      "31 ஊ\t32 க\t33 ா\t34 ள\t35 ழ\t\n",
      "36 ம\t37 உ\t38 ொ\t39 ஷ\t40 ஏ\t\n",
      "41 ற\t42 ட\t43 ண\t44 ஃ\t45 ்\t\n",
      "46 ூ\t47 இ\t48 ன\t"
     ]
    }
   ],
   "source": [
    "english_chars = list(set(''.join(train_source) + start_token + end_token + pad_token))\n",
    "target_chars = list(set(''.join(train_target) + start_token + end_token + pad_token))\n",
    "\n",
    "english_dict_count = len(english_chars)\n",
    "target_dict_count = len(target_chars)\n",
    "\n",
    "print(\"Number of Charecters : \", target_dict_count)\n",
    "\n",
    "print(\"Target Charecters: \")\n",
    "for i, c in enumerate(target_chars):\n",
    "    print(i, c, end='\\t')\n",
    "    if i % 5 == 0:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_i2l, en_l2i = {}, {}\n",
    "tr_i2l, tr_l2i = {}, {}\n",
    "\n",
    "for i, x in enumerate(english_chars):\n",
    "    en_l2i[x] = i\n",
    "    en_i2l[i] = x\n",
    "\n",
    "for i, x in enumerate(target_chars):\n",
    "    tr_l2i[x] = i\n",
    "    tr_i2l[i] = x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(source, target, batch_size):\n",
    "    \"\"\"\n",
    "    returns: list of list which contains padded (source, target) pairs\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for i in range(0, len(source), batch_size):\n",
    "        batch = []\n",
    "        src_batch = source[i:i+batch_size]\n",
    "        tar_batch = target[i:i+batch_size]\n",
    "        scr_max_len = len(max(src_batch, key=len))\n",
    "        tar_max_len = len(max(tar_batch, key=len))\n",
    "\n",
    "        for j in range(len(src_batch)):\n",
    "            padded_scr, padded_tar = src_batch[j].ljust(scr_max_len), tar_batch[j].ljust(tar_max_len)\n",
    "            batch.append((padded_scr, padded_tar))\n",
    "\n",
    "        res.append(batch)\n",
    "    \n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(strings):\n",
    "    \"\"\"Adds start and end token and adds padding\"\"\"\n",
    "    res = []\n",
    "    max_len = len(max(strings, key=len))\n",
    "\n",
    "    for item in strings:\n",
    "        temp = start_token + item + end_token\n",
    "        temp = temp.ljust(max_len+2, pad_token) #2 is added, because we added start and end token to each word\n",
    "        res.append(temp)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_tensor(strings, l2i_dict):\n",
    "    \"\"\"\n",
    "    replaces the chareceters of the sting with corrospong ix (by refering l2i_dict) and returns as int tensor\n",
    "    \"\"\"\n",
    "    res = torch.zeros(len(strings), len(strings[0]))\n",
    "    \n",
    "    for i in range(len(strings)):\n",
    "        for j in range(len(strings[i])):\n",
    "            if strings[i][j] not in l2i_dict :\n",
    "                continue #ignoring the charecters that are not in the dictionary\n",
    "            res[i][j] = l2i_dict[strings[i][j]]\n",
    "        \n",
    "    return res.type(torch.LongTensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing test and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_source_tensor = string_to_tensor(preprocess(val_source), en_l2i).transpose(0,1).requires_grad_(False)\n",
    "val_target_tensor = string_to_tensor(preprocess(val_target), tr_l2i).transpose(0,1).requires_grad_(False)\n",
    "\n",
    "test_source_tensor = string_to_tensor(preprocess(test_source), en_l2i).transpose(0,1).requires_grad_(False)\n",
    "test_target_tensor = string_to_tensor(preprocess(test_target), tr_l2i).transpose(0,1).requires_grad_(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers = 1, p = 0, bi_dir = False):\n",
    "        \"\"\"\n",
    "        Init Parameters:\n",
    "        input_size : english_dict_count\n",
    "        embedding_size : size of each embedding vector\n",
    "        hidden_size : size of hidden state vector\n",
    "        num_layers : number of recurrent layers of RNN\n",
    "        p : dropout probability\n",
    "\n",
    "        Input:\n",
    "        x : torch.Tensor of shape (seq_length, N)\n",
    "            where seq_length - len of longest string in the batch\n",
    "            N - batch size\n",
    "        \n",
    "        Outpus:\n",
    "        outputs: torch.Tensor of shape (seq_len, N, hidden_size * D), where D = 2 if bi_dir = True else 1\n",
    "        hidden: torch.Tensor of shape (num_layers * D, N, hidden_size)\n",
    "        cell: torch.Tensor of shape (num_layers * D, N, hidden_size)\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p, bidirectional = bi_dir)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_length, N) where N is batch size\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (seq_length, N, embedding_size)\n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(embedding)\n",
    "        # outputs shape: (seq_length, N, hidden_size)\n",
    "        \n",
    "\n",
    "        return outputs, hidden, cell"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers = 1, p = 0, bi_dir = False):\n",
    "        \"\"\"input size = output size = target language charecters\n",
    "        Init Parameters:\n",
    "        input_size: target_dict_count\n",
    "        embedding_size: size of each embedding vector\n",
    "        hidden_size: size of hidden state vector\n",
    "        output_size: number of output features in fully connected layer\n",
    "        num_layers : number of recurrent layers of RNN\n",
    "        p : dropout probability\n",
    "\n",
    "        Input:\n",
    "        x: torch.Tensor of shape (N)\n",
    "        hidden: torch.Tensor of shape (num_layers * D, N, hidden_size)\n",
    "        cell: torch.Tensor of shape (num_layers * D, N, hidden_size)\n",
    "\n",
    "        Outputs:\n",
    "        predications: torch.Tensor of shape (N, target_dict_count), where D = 2 if bi_dir = True else 1\n",
    "        hidden: torch.Tensor of shape (num_layers * D, N, hidden_size)\n",
    "        cell: torch.Tensor of shape (num_layers * D, N, hidden_size)\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p, bidirectional = bi_dir)\n",
    "        D = 1\n",
    "        if(bi_dir == True):\n",
    "            D = 2\n",
    "        self.fc = nn.Linear(hidden_size * D, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n",
    "        # is 1 here because we are sending in a single word and not a sentence\n",
    "        # print(x.shape, hidden.shape, cell.shape)\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (1, N, embedding_size)\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
    "        # outputs shape: (1, N, hidden_size * D)\n",
    "\n",
    "        predictions = self.fc(outputs)\n",
    "\n",
    "        # predictions shape: (1, N, length_target_vocabulary) to send it to\n",
    "        # loss function we want it to be (N, length_target_vocabulary) so we're\n",
    "        # just gonna remove the first dim\n",
    "        predictions = predictions.squeeze(0)\n",
    "\n",
    "        return predictions, hidden, cell"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        \"\"\"source : (source_len, N) - not sure\n",
    "        \"\"\"\n",
    "        batch_size = source.shape[1] \n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = target_dict_count\n",
    "\n",
    "        # print(\"source shape \", source.shape)\n",
    "        # print(\"target shape \", target.shape)\n",
    "        # print(\"N : \", batch_size)\n",
    "        # print(\"tar len : \", target_len)\n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size)\n",
    "        # print(\"outputs shape : \", outputs.shape)\n",
    "\n",
    "        _, hidden, cell = self.encoder(source)\n",
    "\n",
    "        # Grab the first input to the Decoder which will be <SOS> token\n",
    "        x = target[0]\n",
    "        outputs[:, :, tr_l2i[start_token]] = 1 #setting prob = 1 for starting token \n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            # Use previous hidden, cell as context from encoder at start\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "\n",
    "            # Store next output prediction\n",
    "            outputs[t] = output\n",
    "\n",
    "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
    "            best_guess = output.argmax(1)\n",
    "\n",
    "            # With probability of teacher_force_ratio we take the actual next word\n",
    "            # otherwise we take the word that the Decoder predicted it to be.\n",
    "            # Teacher Forcing is used so that the model gets used to seeing\n",
    "            # similar inputs at training and testing time, if teacher forcing is 1\n",
    "            # then inputs at test time might be completely different than what the\n",
    "            # network is used to. This was a long comment.\n",
    "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "        # print(\"OUTPUTS: \", outputs)\n",
    "        return outputs\n",
    "    \n",
    "    def calc_evaluation_loss(self, soruce_strings, target_strings, batch_size = 32):\n",
    "        running_loss  = 0\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(soruce_strings), batch_size):\n",
    "                inp_data = string_to_tensor(preprocess(soruce_strings[i:i+batch_size]), en_l2i).transpose(0,1)\n",
    "                target = string_to_tensor(preprocess(target_strings[i:i+batch_size]), tr_l2i).transpose(0,1)\n",
    "\n",
    "                output = self(inp_data, target)\n",
    "                output = output.reshape(-1, output.shape[2])\n",
    "                target = target.reshape(-1)\n",
    "\n",
    "                loss = criterion(output, target)\n",
    "                running_loss += loss.item()\n",
    "\n",
    "        return running_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "batch_size = 2\n",
    "\n",
    "# Model hyperparameters\n",
    "load_model = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_size_encoder = english_dict_count\n",
    "input_size_decoder = target_dict_count\n",
    "output_size = target_dict_count\n",
    "encoder_embedding_size = 10\n",
    "decoder_embedding_size = 10\n",
    "hidden_size = 512  # Needs to be the same for both RNN's\n",
    "encoder_layers = 1\n",
    "decoder_layers = 1\n",
    "enc_dropout = 0\n",
    "dec_dropout = 0\n",
    "embedding_size= 16\n",
    "hidden_size = 10\n",
    "bi_directional = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(english_dict_count, embedding_size, hidden_size, num_layers=encoder_layers, bi_dir=bi_directional)\n",
    "dec = Decoder(target_dict_count, embedding_size, hidden_size, target_dict_count, num_layers=decoder_layers, bi_dir=bi_directional)\n",
    "\n",
    "mod = Seq2Seq(enc, dec)\n",
    "\n",
    "optimizer = optim.Adam(mod.parameters(), lr=learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_print(output):\n",
    "    \"\"\"output shape: target_seq_length * N\"\"\"\n",
    "    res = []\n",
    "    for j in range(output.shape[1]):\n",
    "        temp = \"\"\n",
    "        for i in range(output.shape[0]):\n",
    "            temp += tr_i2l[output[i,j].item()]\n",
    "        \n",
    "        res.append(temp)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "[Epoch   1 / 50] \t Loss: 3.8651 \t Val Loss: 3.9313\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20b6c1bcdc0>]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    no_of_batch = 0\n",
    "    for i in range(0, len(train_source[:2]), batch_size):\n",
    "        src_strings = train_source[i: i+batch_size]\n",
    "        tar_strings = train_target[i: i+batch_size]\n",
    "\n",
    "        src_strings = preprocess(src_strings)\n",
    "        tar_strings = preprocess(tar_strings)\n",
    "\n",
    "        #transposing to make the shape as expected\n",
    "        inp_data = string_to_tensor(src_strings, en_l2i).transpose(0,1)\n",
    "        target = string_to_tensor(tar_strings, tr_l2i).transpose(0,1)\n",
    "\n",
    "\n",
    "        output = mod(inp_data, target)\n",
    "        # result = temp_print(output.argmax(2))\n",
    "        # print(\"result: \", result)\n",
    "        # print(\"target: \", tar_strings)\n",
    "        \n",
    "        # print(\"output: \",result)\n",
    "        # print(\"target:\", tar_strings)\n",
    "\n",
    "        # print(\"op before reshopsed \", output.shape)\n",
    "        # print(\"tar before reshape: \", target.shape)\n",
    "\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        target = target.reshape(-1)\n",
    "\n",
    "        # print(\"op: \", output.shape)\n",
    "        # print(\"tar: \", target.shape)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Back prop\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(mod.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        no_of_batch += 1\n",
    "\n",
    "    # val_output = mod(val_source_tensor, val_target_tensor)\n",
    "    # val_output = val_output.reshape(-1, val_output.shape[2])\n",
    "    # val_target_tensor = val_target_tensor.reshape(-1)\n",
    "    # # print(f\"val output : {val_output.shape} \\t val_target: {val_target_tensor.shape}\")\n",
    "    # val_loss = criterion(val_output, val_target_tensor)\n",
    "\n",
    "    val_loss= mod.calc_evaluation_loss(val_source[:2], val_target[:2])\n",
    "    # val_loss = 0\n",
    "    \n",
    "    print(f\"[Epoch {epoch+1:3d} / {num_epochs}] \\t Loss: {(running_loss/no_of_batch):.4f} \\t Val Loss: {val_loss:2.4f}\")\n",
    "    print()\n",
    "    loss_list.append(running_loss/no_of_batch)\n",
    "    break\n",
    "\n",
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
