{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "from Utils import plot_graphs\n",
    "from AkshrantarDataset import AksharantarDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'tam'\n",
    "start_token = '<'\n",
    "end_token = '>'\n",
    "pad_token = ' '\n",
    "unk_token = '~'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = AksharantarDataset(language, start_token, end_token, pad_token, unk_token)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=data.tr_l2i[pad_token])\n",
    "\n",
    "target_dict_count = len(data.tr_l2i)\n",
    "english_dict_count = len(data.en_l2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers = 1, p = 0, bi_dir = False, rnn_class = nn.GRU):\n",
    "        \"\"\"\n",
    "        Init Parameters:\n",
    "        input_size : english_dict_count\n",
    "        embedding_size : size of each embedding vector\n",
    "        hidden_size : size of hidden state vector\n",
    "        num_layers : number of recurrent layers of RNN\n",
    "        p : dropout probability\n",
    "        rnn_class: type of RNN to be used in the encoder\n",
    "\n",
    "        Input:\n",
    "        x : torch.Tensor of shape (seq_length, N)\n",
    "            where seq_length - len of longest string in the batch\n",
    "            N - batch size\n",
    "        \n",
    "        Outpus:\n",
    "        outputs: torch.Tensor of shape (seq_len, N, hidden_size * D), where D = 2 if bi_dir = True else 1\n",
    "        hidden: torch.Tensor of shape (num_layers * D, N, hidden_size)\n",
    "        \n",
    "        cell: torch.Tensor of shape (num_layers * D, N, hidden_size) if(rnn_class == \"LSTM\")\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn_class= rnn_class\n",
    "        self.rnn = rnn_class(embedding_size, hidden_size, num_layers, dropout=p, bidirectional = bi_dir)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_length, N) where N is batch size\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (seq_length, N, embedding_size)\n",
    "\n",
    "        if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "            outputs, (hidden, cell) = self.rnn(embedding)\n",
    "            # outputs shape: (seq_length, N, hidden_size)\n",
    "        else:\n",
    "            outputs, hidden = self.rnn(embedding)\n",
    "        \n",
    "        if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "            return outputs, hidden, cell\n",
    "        else:\n",
    "            return outputs, hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers = 1, p = 0, bi_dir = False, rnn_class = nn.GRU):\n",
    "        \"\"\"input size = output size = target language charecters\n",
    "        Init Parameters:\n",
    "        input_size: target_dict_count\n",
    "        embedding_size: size of each embedding vector\n",
    "        hidden_size: size of hidden state vector\n",
    "        output_size: number of output features in fully connected layer\n",
    "        num_layers : number of recurrent layers of RNN\n",
    "        p : dropout probability\n",
    "        rnn_class: type of RNN to be used in the encoder\n",
    "\n",
    "        Input:\n",
    "        x: torch.Tensor of shape (N)\n",
    "        hidden: torch.Tensor of shape (num_layers * D, N, hidden_size)\n",
    "        cell: torch.Tensor of shape (num_layers * D, N, hidden_size)\n",
    "\n",
    "        Outputs:\n",
    "        predications: torch.Tensor of shape (N, target_dict_count), where D = 2 if bi_dir = True else 1\n",
    "        hidden: torch.Tensor of shape (num_layers * D, N, hidden_size)\n",
    "        \n",
    "        cell: torch.Tensor of shape (num_layers * D, N, hidden_size) if(rnn_class == \"LSTM\")\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn_class = rnn_class\n",
    "        self.rnn = rnn_class(embedding_size, hidden_size, num_layers, dropout=p, bidirectional = bi_dir)\n",
    "\n",
    "        self.D = 1\n",
    "        if(bi_dir == True):\n",
    "            self.D = 2\n",
    "        self.fc = nn.Linear(hidden_size * self.D, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden, cell = None):\n",
    "        #cell is set to none, for GRU and RNN\n",
    "\n",
    "        # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n",
    "        # is 1 here because we are sending in a single word and not a sentence\n",
    "        # print(x.shape, hidden.shape, cell.shape)\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (1, N, embedding_size)\n",
    "        \n",
    "        if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "            outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
    "            # outputs shape: (1, N, hidden_size * D)\n",
    "        else:\n",
    "            outputs, hidden = self.rnn(embedding, hidden)\n",
    "            \n",
    "\n",
    "        predictions = self.fc(outputs)\n",
    "\n",
    "        # predictions shape: (1, N, length_target_vocabulary) to send it to\n",
    "        # loss function we want it to be (N, length_target_vocabulary) so we're\n",
    "        # just gonna remove the first dim\n",
    "        predictions = predictions.squeeze(0)\n",
    "\n",
    "        if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "            return predictions, hidden, cell\n",
    "        else:\n",
    "            return predictions, hidden"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, output_size, num_layers = 1, dropout_p=0.1, max_length=50, bi_dir = False, rnn_class = nn.GRU):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size= embedding_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        self.num_layers = num_layers\n",
    "        self.D = 1\n",
    "        if(bi_dir == True):\n",
    "            D = 2\n",
    "        self.rnn_class = rnn_class\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.embedding_size)\n",
    "        self.attn = nn.Linear(self.hidden_size + self.embedding_size, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size + self.embedding_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input)\n",
    "        embedded = self.dropout(embedded).unsqueeze(0)\n",
    "        # print(\"embed \",embedded.shape)\n",
    "        # (1, N, es)\n",
    "\n",
    "        temp = torch.cat((embedded, hidden), 2)\n",
    "        # print(\"temp \", temp.shape) # (1, N, hs+es)\n",
    "\n",
    "        temp = self.attn(temp)\n",
    "        # print(\"after attn \", temp.shape) # (1, N, max)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            temp, dim=1)\n",
    "        # print(\"attn_weights :\",attn_weights.shape) # (1, N, max)\n",
    "        # print(\"ecn_op: \", encoder_outputs.shape)\n",
    "        attn_applied = torch.bmm(attn_weights.transpose(0,1),\n",
    "                                 encoder_outputs.transpose(0,1))\n",
    "        #attn_applied (N, 1, hs)\n",
    "\n",
    "        attn_applied = attn_applied.transpose(0,1) #(1, N, hs)\n",
    "        \n",
    "        # print(\"attn appld \", attn_applied.shape) # (1, N, hs)\n",
    "\n",
    "        output = torch.cat((embedded, attn_applied), 2)\n",
    "        # print(\"outpt after cat \",output.shape) # (1, N, hs+es)\n",
    "\n",
    "        output = self.attn_combine(output)\n",
    "        # print(\"after atn comb: \",output.shape) # (1, N, hs)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        # print(\"out \", output.shape, \"hid \", hidden.shape)\n",
    "\n",
    "        prob = self.out(output).squeeze(0) #(1, N, op)\n",
    "        # print(\"prob \", prob.shape)\n",
    "\n",
    "        # output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return prob, hidden, attn_weights\n",
    "\n",
    "    # def initHidden(self):\n",
    "    #     return torch.zeros(1, batch_size, self.hidden_size, device=device)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 2, 10]) torch.Size([1, 2, 10])\n",
      "torch.Size([2, 40]) torch.Size([1, 2, 10]) torch.Size([1, 2, 50])\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 50\n",
    "enc = Encoder(input_size= 20, embedding_size=5, hidden_size=10)\n",
    "o, h = enc(torch.rand(MAX_LEN, 2).int())\n",
    "print(o.shape, h.shape)\n",
    "dec = AttnDecoderRNN(embedding_size = 5, hidden_size=10, output_size=40)\n",
    "p, h, at = dec(torch.zeros(2).int(), h, o)\n",
    "print(p.shape, h.shape, at.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        encoder_layers = encoder.num_layers\n",
    "        decoder_layers = decoder.num_layers\n",
    "        D = decoder.D #we set bidiretion as common in both encoder and decoder, so no need to check for D value seperately\n",
    "        self.enc_to_dec = nn.Linear(encoder_layers*D, decoder_layers*D)\n",
    "        self.rnn_class = decoder.rnn_class #we use same rnn in both encoder and decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_forcing = False):\n",
    "        \"\"\"source : (source_len, N) - not sure\n",
    "        teacher_forching_ratio : probability in which original values is favored over predicted values\n",
    "                                if 0 : predicted values is passed for all chars in target\n",
    "                                if 1 : true values is passed for all chars in target\n",
    "\n",
    "        \"\"\"\n",
    "        batch_size = source.shape[1] \n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = target_dict_count\n",
    "\n",
    "        # print(\"source shape \", source.shape)\n",
    "        # print(\"target shape \", target.shape)\n",
    "        # print(\"N : \", batch_size)\n",
    "        # print(\"tar len : \", target_len)\n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size)\n",
    "        # print(\"outputs shape : \", outputs.shape)\n",
    "\n",
    "        \n",
    "        if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "            enc_ops, hidden, cell = self.encoder(source)\n",
    "        else:\n",
    "            enc_ops, hidden = self.encoder(source)\n",
    "\n",
    "        N = hidden.shape[1]\n",
    "        hidden_size= hidden.shape[2]\n",
    "        # hidden, cell shape: (D*encoder_layers, N, hidden_size)\n",
    "\n",
    "        #UNCOMMENT THIS TO HANDLE dinstinct encoder and decoder layers\n",
    "        # hidden = hidden.transpose(0, 2) # hidden shape: (hidden_size, N, D*encoder_layers)\n",
    "        # hidden = hidden.reshape(-1, hidden.shape[2]) # hidden shape: (hidden_size * N, D*encoder_layers)\n",
    "        # hidden = self.enc_to_dec(hidden) # hidden shape: (hidden_size * N, D*decoder_layers)\n",
    "        # hidden = hidden.reshape(hidden_size, N, hidden.shape[1]) # hidden shape: (hidden_size, N, D*decoder_layers)\n",
    "        # hidden = hidden.transpose(0,2) # hidden shape: (D*decoder_layers, N, hidden_size)\n",
    "\n",
    "        # if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "        #     #at all the below steps, cell will have the shape of hidden\n",
    "        #     cell = cell.transpose(0,2)\n",
    "        #     cell = cell.reshape(-1, cell.shape[2])\n",
    "        #     cell = self.enc_to_dec(cell)\n",
    "        #     cell = cell.reshape(hidden_size, N, cell.shape[1])\n",
    "        #     cell = cell.transpose(0,2)\n",
    "\n",
    "\n",
    "        # Grab the first input to the Decoder\n",
    "        x = target[0]\n",
    "        outputs[:, :, data.tr_l2i[start_token]] = 1 #setting prob = 1 for starting token \n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            # Use previous hidden, cell as context from encoder at start\n",
    "            if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "                # output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "                output, hidden, cell, attention = self.decoder(x, hidden, cell, enc_ops)\n",
    "            else:\n",
    "                output, hidden, atn = self.decoder(x, hidden, enc_ops)\n",
    "\n",
    "            # Store next output prediction\n",
    "            outputs[t] = output\n",
    "\n",
    "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
    "            best_guess = output.argmax(1)\n",
    "\n",
    "            # With probability of teacher_force_ratio we take the actual next word\n",
    "            # otherwise we take the word that the Decoder predicted it to be.\n",
    "            # Teacher Forcing is used so that the model gets used to seeing\n",
    "            # similar inputs at training and testing time, if teacher forcing is 1\n",
    "            # then inputs at test time might be completely different than what the\n",
    "            # network is used to. This was a long comment.\n",
    "            x = target[t] if teacher_forcing == True else best_guess\n",
    "        # print(\"OUTPUTS: \", outputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def calc_accuracy(self, output, target):\n",
    "        \"\"\"\n",
    "        output: torch.Tensor of shape (seq_len, N)\n",
    "        target: torch.Tensor of shape (seq_len, N)\n",
    "        \"\"\"\n",
    "        # batch_size = 32\n",
    "        running_acc = 0\n",
    "        seq_len = output.shape[0]\n",
    "        N = output.shape[1]\n",
    "        matched_strings = 0\n",
    "        with torch.no_grad():\n",
    "            for j in range(N):\n",
    "                current_word_matched = True\n",
    "                for i in range(seq_len):\n",
    "                    if(target[i][j] == data.tr_l2i[pad_token]): #we dont care whatever prediction in the pad_token place\n",
    "                        break\n",
    "                    if(output[i][j] != target[i][j]): #compare the predictions of charecters, start and end_token places\n",
    "                        current_word_matched = False\n",
    "                        break\n",
    "                if(current_word_matched == True):\n",
    "                    matched_strings += 1\n",
    "        return matched_strings*100 / N \n",
    "    \n",
    "\n",
    "    def calc_evaluation_metrics(self, src_tar_pair):\n",
    "        \"\"\"\n",
    "        src_tar_pair: a list of tuples, where each tuple consists of two tensors (source, target)\n",
    "                      source:tensors of shape (seq_len * N), note: seq_len might not be same across all the tensors in the list\n",
    "                      target:tensors of shape (seq_len * N) \n",
    "        \"\"\"\n",
    "        num_batches = len(src_tar_pair)\n",
    "        with torch.no_grad():\n",
    "            acc = 0\n",
    "            running_loss = 0\n",
    "            for source, target in src_tar_pair:\n",
    "                source = source.to(device)\n",
    "                target = target.to(device)\n",
    "                \n",
    "                output = self(source, target).to(device)\n",
    "                acc += self.calc_accuracy(output.argmax(2), target)\n",
    "\n",
    "                output = output.reshape(-1, output.shape[2])\n",
    "                target = target.reshape(-1)\n",
    "\n",
    "                loss = criterion(output, target)\n",
    "                running_loss += loss.item()\n",
    "        return running_loss/num_batches, acc/num_batches\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 500\n",
    "learning_rate = 0.01\n",
    "batch_size = 2\n",
    "\n",
    "# Model hyperparameters\n",
    "input_size_encoder = english_dict_count\n",
    "input_size_decoder = target_dict_count\n",
    "output_size = target_dict_count\n",
    "embedding_size = 5\n",
    "encoder_layers = 1\n",
    "decoder_layers = 1\n",
    "enc_dropout = 0.3\n",
    "dec_dropout = 0.3\n",
    "hidden_size = 10\n",
    "bi_directional = False\n",
    "rnn = nn.GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\madhe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "enc = Encoder(english_dict_count, embedding_size, hidden_size, \n",
    "              num_layers=encoder_layers, \n",
    "              bi_dir=bi_directional,\n",
    "              p=enc_dropout,\n",
    "              rnn_class=rnn).to(device)\n",
    "# dec = Decoder(target_dict_count, embedding_size, hidden_size, target_dict_count, \n",
    "#               num_layers=decoder_layers, \n",
    "#               bi_dir=bi_directional, \n",
    "#               p = dec_dropout,\n",
    "#               rnn_class=rnn).to(device)\n",
    "\n",
    "dec = AttnDecoderRNN(embedding_size, hidden_size, output_size=target_dict_count, \n",
    "                     num_layers=1,\n",
    "                     bi_dir = bi_directional,\n",
    "                     rnn_class= rnn,\n",
    "                     max_length=MAX_LEN).to(device)\n",
    "\n",
    "mod = Seq2Seq(enc, dec).to(device)\n",
    "\n",
    "optimizer = optim.Adam(mod.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.load_data(\"train\", batch_size, num_batches=1, padding_lower_bound=MAX_LEN)\n",
    "valid = data.load_data(\"valid\", batch_size, num_batches=1, padding_lower_bound=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch   1/500] \t Loss: 4.070\t Acc: 0.00 \t Val Loss: 3.902\t Val Acc: 0.00\n",
      "[Epoch   2/500] \t Loss: 3.869\t Acc: 0.00 \t Val Loss: 3.980\t Val Acc: 0.00\n",
      "[Epoch   3/500] \t Loss: 3.726\t Acc: 0.00 \t Val Loss: 3.997\t Val Acc: 0.00\n",
      "[Epoch   4/500] \t Loss: 3.657\t Acc: 0.00 \t Val Loss: 3.951\t Val Acc: 0.00\n",
      "[Epoch   5/500] \t Loss: 3.584\t Acc: 0.00 \t Val Loss: 3.911\t Val Acc: 0.00\n",
      "[Epoch   6/500] \t Loss: 3.492\t Acc: 0.00 \t Val Loss: 3.905\t Val Acc: 0.00\n",
      "[Epoch   7/500] \t Loss: 3.399\t Acc: 0.00 \t Val Loss: 3.906\t Val Acc: 0.00\n",
      "[Epoch   8/500] \t Loss: 3.350\t Acc: 0.00 \t Val Loss: 3.914\t Val Acc: 0.00\n",
      "[Epoch   9/500] \t Loss: 3.283\t Acc: 0.00 \t Val Loss: 3.919\t Val Acc: 0.00\n",
      "[Epoch  10/500] \t Loss: 3.195\t Acc: 0.00 \t Val Loss: 3.932\t Val Acc: 0.00\n",
      "[Epoch  11/500] \t Loss: 3.128\t Acc: 0.00 \t Val Loss: 3.951\t Val Acc: 0.00\n",
      "[Epoch  12/500] \t Loss: 3.054\t Acc: 0.00 \t Val Loss: 3.968\t Val Acc: 0.00\n",
      "[Epoch  13/500] \t Loss: 2.989\t Acc: 0.00 \t Val Loss: 3.976\t Val Acc: 0.00\n",
      "[Epoch  14/500] \t Loss: 2.937\t Acc: 0.00 \t Val Loss: 3.999\t Val Acc: 0.00\n",
      "[Epoch  15/500] \t Loss: 2.882\t Acc: 0.00 \t Val Loss: 4.003\t Val Acc: 0.00\n",
      "[Epoch  16/500] \t Loss: 2.832\t Acc: 0.00 \t Val Loss: 4.044\t Val Acc: 0.00\n",
      "[Epoch  17/500] \t Loss: 2.793\t Acc: 0.00 \t Val Loss: 4.075\t Val Acc: 0.00\n",
      "[Epoch  18/500] \t Loss: 2.740\t Acc: 0.00 \t Val Loss: 4.107\t Val Acc: 0.00\n",
      "[Epoch  19/500] \t Loss: 2.694\t Acc: 0.00 \t Val Loss: 4.152\t Val Acc: 0.00\n",
      "[Epoch  20/500] \t Loss: 2.673\t Acc: 0.00 \t Val Loss: 4.191\t Val Acc: 0.00\n",
      "[Epoch  21/500] \t Loss: 2.630\t Acc: 0.00 \t Val Loss: 4.233\t Val Acc: 0.00\n",
      "[Epoch  22/500] \t Loss: 2.610\t Acc: 0.00 \t Val Loss: 4.274\t Val Acc: 0.00\n",
      "[Epoch  23/500] \t Loss: 2.564\t Acc: 0.00 \t Val Loss: 4.320\t Val Acc: 0.00\n",
      "[Epoch  24/500] \t Loss: 2.551\t Acc: 0.00 \t Val Loss: 4.360\t Val Acc: 0.00\n",
      "[Epoch  25/500] \t Loss: 2.530\t Acc: 0.00 \t Val Loss: 4.409\t Val Acc: 0.00\n",
      "[Epoch  26/500] \t Loss: 2.512\t Acc: 0.00 \t Val Loss: 4.444\t Val Acc: 0.00\n",
      "[Epoch  27/500] \t Loss: 2.497\t Acc: 0.00 \t Val Loss: 4.478\t Val Acc: 0.00\n",
      "[Epoch  28/500] \t Loss: 2.485\t Acc: 0.00 \t Val Loss: 4.533\t Val Acc: 0.00\n",
      "[Epoch  29/500] \t Loss: 2.470\t Acc: 0.00 \t Val Loss: 4.575\t Val Acc: 0.00\n",
      "[Epoch  30/500] \t Loss: 2.455\t Acc: 0.00 \t Val Loss: 4.614\t Val Acc: 0.00\n",
      "[Epoch  31/500] \t Loss: 2.448\t Acc: 0.00 \t Val Loss: 4.646\t Val Acc: 0.00\n",
      "[Epoch  32/500] \t Loss: 2.430\t Acc: 0.00 \t Val Loss: 4.688\t Val Acc: 0.00\n",
      "[Epoch  33/500] \t Loss: 2.427\t Acc: 0.00 \t Val Loss: 4.723\t Val Acc: 0.00\n",
      "[Epoch  34/500] \t Loss: 2.410\t Acc: 0.00 \t Val Loss: 4.757\t Val Acc: 0.00\n",
      "[Epoch  35/500] \t Loss: 2.394\t Acc: 0.00 \t Val Loss: 4.799\t Val Acc: 0.00\n",
      "[Epoch  36/500] \t Loss: 2.385\t Acc: 0.00 \t Val Loss: 4.824\t Val Acc: 0.00\n",
      "[Epoch  37/500] \t Loss: 2.370\t Acc: 0.00 \t Val Loss: 4.847\t Val Acc: 0.00\n",
      "[Epoch  38/500] \t Loss: 2.349\t Acc: 0.00 \t Val Loss: 4.871\t Val Acc: 0.00\n",
      "[Epoch  39/500] \t Loss: 2.367\t Acc: 0.00 \t Val Loss: 4.908\t Val Acc: 0.00\n",
      "[Epoch  40/500] \t Loss: 2.373\t Acc: 0.00 \t Val Loss: 4.927\t Val Acc: 0.00\n",
      "[Epoch  41/500] \t Loss: 2.374\t Acc: 0.00 \t Val Loss: 4.939\t Val Acc: 0.00\n",
      "[Epoch  42/500] \t Loss: 2.318\t Acc: 0.00 \t Val Loss: 4.975\t Val Acc: 0.00\n",
      "[Epoch  43/500] \t Loss: 2.294\t Acc: 0.00 \t Val Loss: 5.001\t Val Acc: 0.00\n",
      "[Epoch  44/500] \t Loss: 2.316\t Acc: 0.00 \t Val Loss: 5.015\t Val Acc: 0.00\n",
      "[Epoch  45/500] \t Loss: 2.285\t Acc: 0.00 \t Val Loss: 5.026\t Val Acc: 0.00\n",
      "[Epoch  46/500] \t Loss: 2.258\t Acc: 0.00 \t Val Loss: 5.036\t Val Acc: 0.00\n",
      "[Epoch  47/500] \t Loss: 2.339\t Acc: 0.00 \t Val Loss: 5.095\t Val Acc: 0.00\n",
      "[Epoch  48/500] \t Loss: 2.374\t Acc: 0.00 \t Val Loss: 5.114\t Val Acc: 0.00\n",
      "[Epoch  49/500] \t Loss: 2.440\t Acc: 0.00 \t Val Loss: 5.136\t Val Acc: 0.00\n",
      "[Epoch  50/500] \t Loss: 2.337\t Acc: 0.00 \t Val Loss: 5.132\t Val Acc: 0.00\n",
      "[Epoch  51/500] \t Loss: 2.267\t Acc: 0.00 \t Val Loss: 5.105\t Val Acc: 0.00\n",
      "[Epoch  52/500] \t Loss: 2.287\t Acc: 0.00 \t Val Loss: 5.134\t Val Acc: 0.00\n",
      "[Epoch  53/500] \t Loss: 2.201\t Acc: 0.00 \t Val Loss: 5.153\t Val Acc: 0.00\n",
      "[Epoch  54/500] \t Loss: 2.197\t Acc: 0.00 \t Val Loss: 5.145\t Val Acc: 0.00\n",
      "[Epoch  55/500] \t Loss: 2.182\t Acc: 0.00 \t Val Loss: 5.165\t Val Acc: 0.00\n",
      "[Epoch  56/500] \t Loss: 2.175\t Acc: 0.00 \t Val Loss: 5.187\t Val Acc: 0.00\n",
      "[Epoch  57/500] \t Loss: 2.277\t Acc: 0.00 \t Val Loss: 5.223\t Val Acc: 0.00\n",
      "[Epoch  58/500] \t Loss: 2.159\t Acc: 0.00 \t Val Loss: 5.208\t Val Acc: 0.00\n",
      "[Epoch  59/500] \t Loss: 2.250\t Acc: 0.00 \t Val Loss: 5.236\t Val Acc: 0.00\n",
      "[Epoch  60/500] \t Loss: 2.210\t Acc: 0.00 \t Val Loss: 5.282\t Val Acc: 0.00\n",
      "[Epoch  61/500] \t Loss: 2.271\t Acc: 0.00 \t Val Loss: 5.298\t Val Acc: 0.00\n",
      "[Epoch  62/500] \t Loss: 2.239\t Acc: 0.00 \t Val Loss: 5.310\t Val Acc: 0.00\n",
      "[Epoch  63/500] \t Loss: 2.117\t Acc: 0.00 \t Val Loss: 5.358\t Val Acc: 0.00\n",
      "[Epoch  64/500] \t Loss: 2.113\t Acc: 0.00 \t Val Loss: 5.313\t Val Acc: 0.00\n",
      "[Epoch  65/500] \t Loss: 2.228\t Acc: 0.00 \t Val Loss: 5.343\t Val Acc: 0.00\n",
      "[Epoch  66/500] \t Loss: 2.084\t Acc: 0.00 \t Val Loss: 5.415\t Val Acc: 0.00\n",
      "[Epoch  67/500] \t Loss: 2.183\t Acc: 0.00 \t Val Loss: 5.383\t Val Acc: 0.00\n",
      "[Epoch  68/500] \t Loss: 2.068\t Acc: 0.00 \t Val Loss: 5.384\t Val Acc: 0.00\n",
      "[Epoch  69/500] \t Loss: 2.234\t Acc: 0.00 \t Val Loss: 5.399\t Val Acc: 0.00\n",
      "[Epoch  70/500] \t Loss: 2.125\t Acc: 0.00 \t Val Loss: 5.387\t Val Acc: 0.00\n",
      "[Epoch  71/500] \t Loss: 2.144\t Acc: 0.00 \t Val Loss: 5.401\t Val Acc: 0.00\n",
      "[Epoch  72/500] \t Loss: 2.088\t Acc: 0.00 \t Val Loss: 5.456\t Val Acc: 0.00\n",
      "[Epoch  73/500] \t Loss: 2.070\t Acc: 0.00 \t Val Loss: 5.440\t Val Acc: 0.00\n",
      "[Epoch  74/500] \t Loss: 2.055\t Acc: 0.00 \t Val Loss: 5.528\t Val Acc: 0.00\n",
      "[Epoch  75/500] \t Loss: 2.025\t Acc: 0.00 \t Val Loss: 5.622\t Val Acc: 0.00\n",
      "[Epoch  76/500] \t Loss: 2.225\t Acc: 0.00 \t Val Loss: 5.636\t Val Acc: 0.00\n",
      "[Epoch  77/500] \t Loss: 2.133\t Acc: 0.00 \t Val Loss: 5.540\t Val Acc: 0.00\n",
      "[Epoch  78/500] \t Loss: 1.987\t Acc: 0.00 \t Val Loss: 5.547\t Val Acc: 0.00\n",
      "[Epoch  79/500] \t Loss: 2.064\t Acc: 0.00 \t Val Loss: 5.506\t Val Acc: 0.00\n",
      "[Epoch  80/500] \t Loss: 2.001\t Acc: 0.00 \t Val Loss: 5.510\t Val Acc: 0.00\n",
      "[Epoch  81/500] \t Loss: 1.971\t Acc: 0.00 \t Val Loss: 5.541\t Val Acc: 0.00\n",
      "[Epoch  82/500] \t Loss: 2.095\t Acc: 0.00 \t Val Loss: 5.514\t Val Acc: 0.00\n",
      "[Epoch  83/500] \t Loss: 1.934\t Acc: 0.00 \t Val Loss: 5.505\t Val Acc: 0.00\n",
      "[Epoch  84/500] \t Loss: 2.070\t Acc: 0.00 \t Val Loss: 5.463\t Val Acc: 0.00\n",
      "[Epoch  85/500] \t Loss: 1.942\t Acc: 0.00 \t Val Loss: 5.402\t Val Acc: 0.00\n",
      "[Epoch  86/500] \t Loss: 2.016\t Acc: 0.00 \t Val Loss: 5.413\t Val Acc: 0.00\n",
      "[Epoch  87/500] \t Loss: 1.901\t Acc: 0.00 \t Val Loss: 5.449\t Val Acc: 0.00\n",
      "[Epoch  88/500] \t Loss: 1.903\t Acc: 0.00 \t Val Loss: 5.331\t Val Acc: 0.00\n",
      "[Epoch  89/500] \t Loss: 1.982\t Acc: 0.00 \t Val Loss: 5.533\t Val Acc: 0.00\n",
      "[Epoch  90/500] \t Loss: 2.093\t Acc: 0.00 \t Val Loss: 5.473\t Val Acc: 0.00\n",
      "[Epoch  91/500] \t Loss: 2.055\t Acc: 0.00 \t Val Loss: 5.533\t Val Acc: 0.00\n",
      "[Epoch  92/500] \t Loss: 1.979\t Acc: 0.00 \t Val Loss: 5.639\t Val Acc: 0.00\n",
      "[Epoch  93/500] \t Loss: 1.944\t Acc: 0.00 \t Val Loss: 5.489\t Val Acc: 0.00\n",
      "[Epoch  94/500] \t Loss: 1.846\t Acc: 0.00 \t Val Loss: 5.470\t Val Acc: 0.00\n",
      "[Epoch  95/500] \t Loss: 1.839\t Acc: 0.00 \t Val Loss: 5.511\t Val Acc: 0.00\n",
      "[Epoch  96/500] \t Loss: 1.917\t Acc: 0.00 \t Val Loss: 5.389\t Val Acc: 0.00\n",
      "[Epoch  97/500] \t Loss: 1.969\t Acc: 0.00 \t Val Loss: 5.520\t Val Acc: 0.00\n",
      "[Epoch  98/500] \t Loss: 1.806\t Acc: 0.00 \t Val Loss: 5.577\t Val Acc: 0.00\n",
      "[Epoch  99/500] \t Loss: 1.804\t Acc: 0.00 \t Val Loss: 5.519\t Val Acc: 0.00\n",
      "[Epoch 100/500] \t Loss: 1.763\t Acc: 0.00 \t Val Loss: 5.578\t Val Acc: 0.00\n",
      "[Epoch 101/500] \t Loss: 1.767\t Acc: 0.00 \t Val Loss: 5.485\t Val Acc: 0.00\n",
      "[Epoch 102/500] \t Loss: 1.915\t Acc: 0.00 \t Val Loss: 5.478\t Val Acc: 0.00\n",
      "[Epoch 103/500] \t Loss: 1.874\t Acc: 0.00 \t Val Loss: 5.368\t Val Acc: 0.00\n",
      "[Epoch 104/500] \t Loss: 1.748\t Acc: 0.00 \t Val Loss: 5.468\t Val Acc: 0.00\n",
      "[Epoch 105/500] \t Loss: 1.943\t Acc: 0.00 \t Val Loss: 5.472\t Val Acc: 0.00\n",
      "[Epoch 106/500] \t Loss: 1.857\t Acc: 0.00 \t Val Loss: 5.451\t Val Acc: 0.00\n",
      "[Epoch 107/500] \t Loss: 1.794\t Acc: 0.00 \t Val Loss: 5.495\t Val Acc: 0.00\n",
      "[Epoch 108/500] \t Loss: 1.683\t Acc: 0.00 \t Val Loss: 5.726\t Val Acc: 0.00\n",
      "[Epoch 109/500] \t Loss: 1.708\t Acc: 0.00 \t Val Loss: 5.686\t Val Acc: 0.00\n",
      "[Epoch 110/500] \t Loss: 1.690\t Acc: 0.00 \t Val Loss: 5.667\t Val Acc: 0.00\n",
      "[Epoch 111/500] \t Loss: 1.702\t Acc: 0.00 \t Val Loss: 5.714\t Val Acc: 0.00\n",
      "[Epoch 112/500] \t Loss: 1.988\t Acc: 0.00 \t Val Loss: 5.673\t Val Acc: 0.00\n",
      "[Epoch 113/500] \t Loss: 1.616\t Acc: 0.00 \t Val Loss: 5.471\t Val Acc: 0.00\n",
      "[Epoch 114/500] \t Loss: 1.922\t Acc: 0.00 \t Val Loss: 5.562\t Val Acc: 0.00\n",
      "[Epoch 115/500] \t Loss: 1.669\t Acc: 0.00 \t Val Loss: 5.779\t Val Acc: 0.00\n",
      "[Epoch 116/500] \t Loss: 1.712\t Acc: 0.00 \t Val Loss: 5.597\t Val Acc: 0.00\n",
      "[Epoch 117/500] \t Loss: 1.642\t Acc: 0.00 \t Val Loss: 5.639\t Val Acc: 0.00\n",
      "[Epoch 118/500] \t Loss: 1.764\t Acc: 0.00 \t Val Loss: 5.573\t Val Acc: 0.00\n",
      "[Epoch 119/500] \t Loss: 1.545\t Acc: 0.00 \t Val Loss: 5.604\t Val Acc: 0.00\n",
      "[Epoch 120/500] \t Loss: 1.506\t Acc: 0.00 \t Val Loss: 5.472\t Val Acc: 0.00\n",
      "[Epoch 121/500] \t Loss: 1.696\t Acc: 0.00 \t Val Loss: 5.635\t Val Acc: 0.00\n",
      "[Epoch 122/500] \t Loss: 1.494\t Acc: 0.00 \t Val Loss: 5.514\t Val Acc: 0.00\n",
      "[Epoch 123/500] \t Loss: 1.712\t Acc: 0.00 \t Val Loss: 5.767\t Val Acc: 0.00\n",
      "[Epoch 124/500] \t Loss: 1.642\t Acc: 0.00 \t Val Loss: 5.478\t Val Acc: 0.00\n",
      "[Epoch 125/500] \t Loss: 1.505\t Acc: 0.00 \t Val Loss: 5.990\t Val Acc: 0.00\n",
      "[Epoch 126/500] \t Loss: 1.454\t Acc: 0.00 \t Val Loss: 5.432\t Val Acc: 0.00\n",
      "[Epoch 127/500] \t Loss: 1.444\t Acc: 0.00 \t Val Loss: 5.627\t Val Acc: 0.00\n",
      "[Epoch 128/500] \t Loss: 1.464\t Acc: 0.00 \t Val Loss: 6.113\t Val Acc: 0.00\n",
      "[Epoch 129/500] \t Loss: 1.438\t Acc: 0.00 \t Val Loss: 5.475\t Val Acc: 0.00\n",
      "[Epoch 130/500] \t Loss: 1.614\t Acc: 0.00 \t Val Loss: 5.737\t Val Acc: 0.00\n",
      "[Epoch 131/500] \t Loss: 1.620\t Acc: 0.00 \t Val Loss: 5.582\t Val Acc: 0.00\n",
      "[Epoch 132/500] \t Loss: 1.365\t Acc: 0.00 \t Val Loss: 5.502\t Val Acc: 0.00\n",
      "[Epoch 133/500] \t Loss: 1.355\t Acc: 0.00 \t Val Loss: 5.599\t Val Acc: 0.00\n",
      "[Epoch 134/500] \t Loss: 1.348\t Acc: 0.00 \t Val Loss: 6.074\t Val Acc: 0.00\n",
      "[Epoch 135/500] \t Loss: 1.325\t Acc: 0.00 \t Val Loss: 5.610\t Val Acc: 0.00\n",
      "[Epoch 136/500] \t Loss: 1.533\t Acc: 0.00 \t Val Loss: 5.605\t Val Acc: 0.00\n",
      "[Epoch 137/500] \t Loss: 1.514\t Acc: 0.00 \t Val Loss: 5.574\t Val Acc: 0.00\n",
      "[Epoch 138/500] \t Loss: 1.505\t Acc: 0.00 \t Val Loss: 6.334\t Val Acc: 0.00\n",
      "[Epoch 139/500] \t Loss: 1.331\t Acc: 0.00 \t Val Loss: 5.814\t Val Acc: 0.00\n",
      "[Epoch 140/500] \t Loss: 1.305\t Acc: 0.00 \t Val Loss: 6.263\t Val Acc: 0.00\n",
      "[Epoch 141/500] \t Loss: 1.275\t Acc: 0.00 \t Val Loss: 6.297\t Val Acc: 0.00\n",
      "[Epoch 142/500] \t Loss: 1.446\t Acc: 0.00 \t Val Loss: 6.266\t Val Acc: 0.00\n",
      "[Epoch 143/500] \t Loss: 1.452\t Acc: 0.00 \t Val Loss: 6.335\t Val Acc: 0.00\n",
      "[Epoch 144/500] \t Loss: 1.293\t Acc: 0.00 \t Val Loss: 5.796\t Val Acc: 0.00\n",
      "[Epoch 145/500] \t Loss: 1.608\t Acc: 0.00 \t Val Loss: 5.696\t Val Acc: 0.00\n",
      "[Epoch 146/500] \t Loss: 1.528\t Acc: 0.00 \t Val Loss: 5.667\t Val Acc: 0.00\n",
      "[Epoch 147/500] \t Loss: 1.279\t Acc: 0.00 \t Val Loss: 6.244\t Val Acc: 0.00\n",
      "[Epoch 148/500] \t Loss: 1.372\t Acc: 0.00 \t Val Loss: 6.369\t Val Acc: 0.00\n",
      "[Epoch 149/500] \t Loss: 1.430\t Acc: 0.00 \t Val Loss: 5.835\t Val Acc: 0.00\n",
      "[Epoch 150/500] \t Loss: 1.390\t Acc: 0.00 \t Val Loss: 5.571\t Val Acc: 0.00\n",
      "[Epoch 151/500] \t Loss: 1.365\t Acc: 0.00 \t Val Loss: 5.867\t Val Acc: 0.00\n",
      "[Epoch 152/500] \t Loss: 1.351\t Acc: 0.00 \t Val Loss: 6.149\t Val Acc: 0.00\n",
      "[Epoch 153/500] \t Loss: 1.229\t Acc: 0.00 \t Val Loss: 6.116\t Val Acc: 0.00\n",
      "[Epoch 154/500] \t Loss: 1.292\t Acc: 0.00 \t Val Loss: 6.367\t Val Acc: 0.00\n",
      "[Epoch 155/500] \t Loss: 1.198\t Acc: 0.00 \t Val Loss: 6.268\t Val Acc: 0.00\n",
      "[Epoch 156/500] \t Loss: 1.195\t Acc: 0.00 \t Val Loss: 5.971\t Val Acc: 0.00\n",
      "[Epoch 157/500] \t Loss: 1.162\t Acc: 0.00 \t Val Loss: 6.417\t Val Acc: 0.00\n",
      "[Epoch 158/500] \t Loss: 1.140\t Acc: 0.00 \t Val Loss: 6.507\t Val Acc: 0.00\n",
      "[Epoch 159/500] \t Loss: 1.167\t Acc: 0.00 \t Val Loss: 6.448\t Val Acc: 0.00\n",
      "[Epoch 160/500] \t Loss: 1.127\t Acc: 0.00 \t Val Loss: 5.930\t Val Acc: 0.00\n",
      "[Epoch 161/500] \t Loss: 1.345\t Acc: 0.00 \t Val Loss: 6.308\t Val Acc: 0.00\n",
      "[Epoch 162/500] \t Loss: 1.119\t Acc: 0.00 \t Val Loss: 6.510\t Val Acc: 0.00\n",
      "[Epoch 163/500] \t Loss: 1.341\t Acc: 0.00 \t Val Loss: 6.303\t Val Acc: 0.00\n",
      "[Epoch 164/500] \t Loss: 1.103\t Acc: 0.00 \t Val Loss: 6.275\t Val Acc: 0.00\n",
      "[Epoch 165/500] \t Loss: 1.283\t Acc: 0.00 \t Val Loss: 6.214\t Val Acc: 0.00\n",
      "[Epoch 166/500] \t Loss: 1.103\t Acc: 0.00 \t Val Loss: 5.889\t Val Acc: 0.00\n",
      "[Epoch 167/500] \t Loss: 1.079\t Acc: 0.00 \t Val Loss: 6.074\t Val Acc: 0.00\n",
      "[Epoch 168/500] \t Loss: 1.240\t Acc: 0.00 \t Val Loss: 5.982\t Val Acc: 0.00\n",
      "[Epoch 169/500] \t Loss: 1.079\t Acc: 0.00 \t Val Loss: 6.183\t Val Acc: 0.00\n",
      "[Epoch 170/500] \t Loss: 1.096\t Acc: 0.00 \t Val Loss: 6.172\t Val Acc: 0.00\n",
      "[Epoch 171/500] \t Loss: 1.047\t Acc: 0.00 \t Val Loss: 6.470\t Val Acc: 0.00\n",
      "[Epoch 172/500] \t Loss: 1.249\t Acc: 0.00 \t Val Loss: 6.412\t Val Acc: 0.00\n",
      "[Epoch 173/500] \t Loss: 1.187\t Acc: 0.00 \t Val Loss: 6.462\t Val Acc: 0.00\n",
      "[Epoch 174/500] \t Loss: 1.053\t Acc: 0.00 \t Val Loss: 6.400\t Val Acc: 0.00\n",
      "[Epoch 175/500] \t Loss: 1.226\t Acc: 0.00 \t Val Loss: 6.325\t Val Acc: 0.00\n",
      "[Epoch 176/500] \t Loss: 1.181\t Acc: 0.00 \t Val Loss: 6.549\t Val Acc: 0.00\n",
      "[Epoch 177/500] \t Loss: 1.163\t Acc: 0.00 \t Val Loss: 6.541\t Val Acc: 0.00\n",
      "[Epoch 178/500] \t Loss: 1.288\t Acc: 0.00 \t Val Loss: 6.247\t Val Acc: 0.00\n",
      "[Epoch 179/500] \t Loss: 1.014\t Acc: 0.00 \t Val Loss: 6.275\t Val Acc: 0.00\n",
      "[Epoch 180/500] \t Loss: 1.057\t Acc: 0.00 \t Val Loss: 6.530\t Val Acc: 0.00\n",
      "[Epoch 181/500] \t Loss: 0.999\t Acc: 0.00 \t Val Loss: 6.895\t Val Acc: 0.00\n",
      "[Epoch 182/500] \t Loss: 1.346\t Acc: 0.00 \t Val Loss: 6.756\t Val Acc: 0.00\n",
      "[Epoch 183/500] \t Loss: 1.003\t Acc: 0.00 \t Val Loss: 6.618\t Val Acc: 0.00\n",
      "[Epoch 184/500] \t Loss: 1.232\t Acc: 0.00 \t Val Loss: 6.817\t Val Acc: 0.00\n",
      "[Epoch 185/500] \t Loss: 1.029\t Acc: 0.00 \t Val Loss: 6.774\t Val Acc: 0.00\n",
      "[Epoch 186/500] \t Loss: 1.002\t Acc: 0.00 \t Val Loss: 6.882\t Val Acc: 0.00\n",
      "[Epoch 187/500] \t Loss: 1.019\t Acc: 0.00 \t Val Loss: 6.757\t Val Acc: 0.00\n",
      "[Epoch 188/500] \t Loss: 1.030\t Acc: 0.00 \t Val Loss: 6.943\t Val Acc: 0.00\n",
      "[Epoch 189/500] \t Loss: 1.013\t Acc: 0.00 \t Val Loss: 6.873\t Val Acc: 0.00\n",
      "[Epoch 190/500] \t Loss: 0.967\t Acc: 0.00 \t Val Loss: 6.927\t Val Acc: 0.00\n",
      "[Epoch 191/500] \t Loss: 1.174\t Acc: 0.00 \t Val Loss: 6.864\t Val Acc: 0.00\n",
      "[Epoch 192/500] \t Loss: 1.161\t Acc: 0.00 \t Val Loss: 6.914\t Val Acc: 0.00\n",
      "[Epoch 193/500] \t Loss: 1.162\t Acc: 0.00 \t Val Loss: 6.955\t Val Acc: 0.00\n",
      "[Epoch 194/500] \t Loss: 1.205\t Acc: 0.00 \t Val Loss: 6.910\t Val Acc: 0.00\n",
      "[Epoch 195/500] \t Loss: 0.928\t Acc: 0.00 \t Val Loss: 6.875\t Val Acc: 0.00\n",
      "[Epoch 196/500] \t Loss: 0.939\t Acc: 0.00 \t Val Loss: 6.918\t Val Acc: 0.00\n",
      "[Epoch 197/500] \t Loss: 0.936\t Acc: 0.00 \t Val Loss: 6.110\t Val Acc: 0.00\n",
      "[Epoch 198/500] \t Loss: 0.960\t Acc: 0.00 \t Val Loss: 6.939\t Val Acc: 0.00\n",
      "[Epoch 199/500] \t Loss: 0.944\t Acc: 0.00 \t Val Loss: 6.803\t Val Acc: 0.00\n",
      "[Epoch 200/500] \t Loss: 1.104\t Acc: 0.00 \t Val Loss: 6.983\t Val Acc: 0.00\n",
      "[Epoch 201/500] \t Loss: 0.911\t Acc: 0.00 \t Val Loss: 6.674\t Val Acc: 0.00\n",
      "[Epoch 202/500] \t Loss: 1.113\t Acc: 0.00 \t Val Loss: 6.843\t Val Acc: 0.00\n",
      "[Epoch 203/500] \t Loss: 1.104\t Acc: 0.00 \t Val Loss: 6.793\t Val Acc: 0.00\n",
      "[Epoch 204/500] \t Loss: 1.028\t Acc: 0.00 \t Val Loss: 7.170\t Val Acc: 0.00\n",
      "[Epoch 205/500] \t Loss: 0.887\t Acc: 0.00 \t Val Loss: 7.089\t Val Acc: 0.00\n",
      "[Epoch 206/500] \t Loss: 0.953\t Acc: 0.00 \t Val Loss: 6.922\t Val Acc: 0.00\n",
      "[Epoch 207/500] \t Loss: 0.905\t Acc: 0.00 \t Val Loss: 7.019\t Val Acc: 0.00\n",
      "[Epoch 208/500] \t Loss: 0.884\t Acc: 0.00 \t Val Loss: 6.884\t Val Acc: 0.00\n",
      "[Epoch 209/500] \t Loss: 1.132\t Acc: 0.00 \t Val Loss: 6.909\t Val Acc: 0.00\n",
      "[Epoch 210/500] \t Loss: 1.206\t Acc: 0.00 \t Val Loss: 6.930\t Val Acc: 0.00\n",
      "[Epoch 211/500] \t Loss: 1.148\t Acc: 0.00 \t Val Loss: 7.235\t Val Acc: 0.00\n",
      "[Epoch 212/500] \t Loss: 1.171\t Acc: 0.00 \t Val Loss: 7.024\t Val Acc: 0.00\n",
      "[Epoch 213/500] \t Loss: 1.161\t Acc: 0.00 \t Val Loss: 7.181\t Val Acc: 0.00\n",
      "[Epoch 214/500] \t Loss: 1.127\t Acc: 0.00 \t Val Loss: 6.655\t Val Acc: 0.00\n",
      "[Epoch 215/500] \t Loss: 1.128\t Acc: 0.00 \t Val Loss: 6.693\t Val Acc: 0.00\n",
      "[Epoch 216/500] \t Loss: 0.893\t Acc: 0.00 \t Val Loss: 6.823\t Val Acc: 0.00\n",
      "[Epoch 217/500] \t Loss: 1.085\t Acc: 0.00 \t Val Loss: 6.662\t Val Acc: 0.00\n",
      "[Epoch 218/500] \t Loss: 0.904\t Acc: 0.00 \t Val Loss: 6.645\t Val Acc: 0.00\n",
      "[Epoch 219/500] \t Loss: 0.887\t Acc: 0.00 \t Val Loss: 6.812\t Val Acc: 0.00\n",
      "[Epoch 220/500] \t Loss: 0.895\t Acc: 0.00 \t Val Loss: 7.249\t Val Acc: 0.00\n",
      "[Epoch 221/500] \t Loss: 1.056\t Acc: 0.00 \t Val Loss: 7.250\t Val Acc: 0.00\n",
      "[Epoch 222/500] \t Loss: 0.945\t Acc: 0.00 \t Val Loss: 6.806\t Val Acc: 0.00\n",
      "[Epoch 223/500] \t Loss: 0.895\t Acc: 0.00 \t Val Loss: 7.319\t Val Acc: 0.00\n",
      "[Epoch 224/500] \t Loss: 0.861\t Acc: 0.00 \t Val Loss: 7.340\t Val Acc: 0.00\n",
      "[Epoch 225/500] \t Loss: 0.883\t Acc: 0.00 \t Val Loss: 7.378\t Val Acc: 0.00\n",
      "[Epoch 226/500] \t Loss: 0.975\t Acc: 0.00 \t Val Loss: 7.119\t Val Acc: 0.00\n",
      "[Epoch 227/500] \t Loss: 1.176\t Acc: 0.00 \t Val Loss: 7.039\t Val Acc: 0.00\n",
      "[Epoch 228/500] \t Loss: 0.852\t Acc: 0.00 \t Val Loss: 6.893\t Val Acc: 0.00\n",
      "[Epoch 229/500] \t Loss: 0.982\t Acc: 0.00 \t Val Loss: 7.189\t Val Acc: 0.00\n",
      "[Epoch 230/500] \t Loss: 0.890\t Acc: 0.00 \t Val Loss: 7.055\t Val Acc: 0.00\n",
      "[Epoch 231/500] \t Loss: 0.877\t Acc: 0.00 \t Val Loss: 7.458\t Val Acc: 0.00\n",
      "[Epoch 232/500] \t Loss: 1.027\t Acc: 0.00 \t Val Loss: 7.129\t Val Acc: 0.00\n",
      "[Epoch 233/500] \t Loss: 0.985\t Acc: 0.00 \t Val Loss: 7.370\t Val Acc: 0.00\n",
      "[Epoch 234/500] \t Loss: 0.982\t Acc: 0.00 \t Val Loss: 7.183\t Val Acc: 0.00\n",
      "[Epoch 235/500] \t Loss: 0.881\t Acc: 0.00 \t Val Loss: 6.984\t Val Acc: 0.00\n",
      "[Epoch 236/500] \t Loss: 1.214\t Acc: 0.00 \t Val Loss: 7.194\t Val Acc: 0.00\n",
      "[Epoch 237/500] \t Loss: 1.039\t Acc: 0.00 \t Val Loss: 7.210\t Val Acc: 0.00\n",
      "[Epoch 238/500] \t Loss: 0.819\t Acc: 0.00 \t Val Loss: 7.100\t Val Acc: 0.00\n",
      "[Epoch 239/500] \t Loss: 0.830\t Acc: 0.00 \t Val Loss: 6.795\t Val Acc: 0.00\n",
      "[Epoch 240/500] \t Loss: 0.901\t Acc: 0.00 \t Val Loss: 6.784\t Val Acc: 0.00\n",
      "[Epoch 241/500] \t Loss: 0.923\t Acc: 0.00 \t Val Loss: 6.705\t Val Acc: 0.00\n",
      "[Epoch 242/500] \t Loss: 0.800\t Acc: 0.00 \t Val Loss: 7.041\t Val Acc: 0.00\n",
      "[Epoch 243/500] \t Loss: 0.937\t Acc: 0.00 \t Val Loss: 7.161\t Val Acc: 0.00\n",
      "[Epoch 244/500] \t Loss: 1.039\t Acc: 0.00 \t Val Loss: 6.772\t Val Acc: 0.00\n",
      "[Epoch 245/500] \t Loss: 0.910\t Acc: 0.00 \t Val Loss: 6.995\t Val Acc: 0.00\n",
      "[Epoch 246/500] \t Loss: 1.104\t Acc: 0.00 \t Val Loss: 7.036\t Val Acc: 0.00\n",
      "[Epoch 247/500] \t Loss: 0.797\t Acc: 0.00 \t Val Loss: 7.460\t Val Acc: 0.00\n",
      "[Epoch 248/500] \t Loss: 0.822\t Acc: 0.00 \t Val Loss: 6.821\t Val Acc: 0.00\n",
      "[Epoch 249/500] \t Loss: 0.801\t Acc: 0.00 \t Val Loss: 7.036\t Val Acc: 0.00\n",
      "[Epoch 250/500] \t Loss: 1.077\t Acc: 0.00 \t Val Loss: 6.952\t Val Acc: 0.00\n",
      "[Epoch 251/500] \t Loss: 0.932\t Acc: 0.00 \t Val Loss: 7.056\t Val Acc: 0.00\n",
      "[Epoch 252/500] \t Loss: 0.985\t Acc: 0.00 \t Val Loss: 6.982\t Val Acc: 0.00\n",
      "[Epoch 253/500] \t Loss: 0.954\t Acc: 0.00 \t Val Loss: 6.929\t Val Acc: 0.00\n",
      "[Epoch 254/500] \t Loss: 1.006\t Acc: 0.00 \t Val Loss: 6.967\t Val Acc: 0.00\n",
      "[Epoch 255/500] \t Loss: 0.923\t Acc: 0.00 \t Val Loss: 7.379\t Val Acc: 0.00\n",
      "[Epoch 256/500] \t Loss: 0.898\t Acc: 0.00 \t Val Loss: 6.976\t Val Acc: 0.00\n",
      "[Epoch 257/500] \t Loss: 0.863\t Acc: 0.00 \t Val Loss: 7.045\t Val Acc: 0.00\n",
      "[Epoch 258/500] \t Loss: 0.928\t Acc: 0.00 \t Val Loss: 7.102\t Val Acc: 0.00\n",
      "[Epoch 259/500] \t Loss: 1.039\t Acc: 0.00 \t Val Loss: 6.938\t Val Acc: 0.00\n",
      "[Epoch 260/500] \t Loss: 1.096\t Acc: 0.00 \t Val Loss: 7.033\t Val Acc: 0.00\n",
      "[Epoch 261/500] \t Loss: 1.031\t Acc: 0.00 \t Val Loss: 7.624\t Val Acc: 0.00\n",
      "[Epoch 262/500] \t Loss: 0.937\t Acc: 0.00 \t Val Loss: 7.318\t Val Acc: 0.00\n",
      "[Epoch 263/500] \t Loss: 0.979\t Acc: 0.00 \t Val Loss: 7.229\t Val Acc: 0.00\n",
      "[Epoch 264/500] \t Loss: 0.979\t Acc: 0.00 \t Val Loss: 7.335\t Val Acc: 0.00\n",
      "[Epoch 265/500] \t Loss: 0.990\t Acc: 0.00 \t Val Loss: 6.960\t Val Acc: 0.00\n",
      "[Epoch 266/500] \t Loss: 0.886\t Acc: 0.00 \t Val Loss: 6.997\t Val Acc: 0.00\n",
      "[Epoch 267/500] \t Loss: 0.862\t Acc: 0.00 \t Val Loss: 7.619\t Val Acc: 0.00\n",
      "[Epoch 268/500] \t Loss: 0.820\t Acc: 0.00 \t Val Loss: 7.034\t Val Acc: 0.00\n",
      "[Epoch 269/500] \t Loss: 1.000\t Acc: 0.00 \t Val Loss: 7.118\t Val Acc: 0.00\n",
      "[Epoch 270/500] \t Loss: 1.007\t Acc: 0.00 \t Val Loss: 7.117\t Val Acc: 0.00\n",
      "[Epoch 271/500] \t Loss: 1.027\t Acc: 0.00 \t Val Loss: 7.495\t Val Acc: 0.00\n",
      "[Epoch 272/500] \t Loss: 1.005\t Acc: 0.00 \t Val Loss: 7.277\t Val Acc: 0.00\n",
      "[Epoch 273/500] \t Loss: 1.006\t Acc: 0.00 \t Val Loss: 7.036\t Val Acc: 0.00\n",
      "[Epoch 274/500] \t Loss: 0.834\t Acc: 0.00 \t Val Loss: 7.712\t Val Acc: 0.00\n",
      "[Epoch 275/500] \t Loss: 0.937\t Acc: 0.00 \t Val Loss: 7.348\t Val Acc: 0.00\n",
      "[Epoch 276/500] \t Loss: 0.895\t Acc: 0.00 \t Val Loss: 7.084\t Val Acc: 0.00\n",
      "[Epoch 277/500] \t Loss: 0.845\t Acc: 0.00 \t Val Loss: 7.007\t Val Acc: 0.00\n",
      "[Epoch 278/500] \t Loss: 0.825\t Acc: 0.00 \t Val Loss: 7.260\t Val Acc: 0.00\n",
      "[Epoch 279/500] \t Loss: 0.853\t Acc: 0.00 \t Val Loss: 7.534\t Val Acc: 0.00\n",
      "[Epoch 280/500] \t Loss: 0.869\t Acc: 0.00 \t Val Loss: 7.097\t Val Acc: 0.00\n",
      "[Epoch 281/500] \t Loss: 0.827\t Acc: 0.00 \t Val Loss: 7.019\t Val Acc: 0.00\n",
      "[Epoch 282/500] \t Loss: 0.956\t Acc: 0.00 \t Val Loss: 7.123\t Val Acc: 0.00\n",
      "[Epoch 283/500] \t Loss: 0.982\t Acc: 0.00 \t Val Loss: 7.385\t Val Acc: 0.00\n",
      "[Epoch 284/500] \t Loss: 0.980\t Acc: 0.00 \t Val Loss: 7.748\t Val Acc: 0.00\n",
      "[Epoch 285/500] \t Loss: 0.921\t Acc: 0.00 \t Val Loss: 7.421\t Val Acc: 0.00\n",
      "[Epoch 286/500] \t Loss: 0.832\t Acc: 0.00 \t Val Loss: 7.027\t Val Acc: 0.00\n",
      "[Epoch 287/500] \t Loss: 0.823\t Acc: 0.00 \t Val Loss: 7.316\t Val Acc: 0.00\n",
      "[Epoch 288/500] \t Loss: 0.830\t Acc: 0.00 \t Val Loss: 7.219\t Val Acc: 0.00\n",
      "[Epoch 289/500] \t Loss: 0.963\t Acc: 0.00 \t Val Loss: 7.328\t Val Acc: 0.00\n",
      "[Epoch 290/500] \t Loss: 0.825\t Acc: 0.00 \t Val Loss: 7.042\t Val Acc: 0.00\n",
      "[Epoch 291/500] \t Loss: 0.986\t Acc: 0.00 \t Val Loss: 7.344\t Val Acc: 0.00\n",
      "[Epoch 292/500] \t Loss: 0.858\t Acc: 0.00 \t Val Loss: 7.500\t Val Acc: 0.00\n",
      "[Epoch 293/500] \t Loss: 0.842\t Acc: 0.00 \t Val Loss: 7.297\t Val Acc: 0.00\n",
      "[Epoch 294/500] \t Loss: 0.833\t Acc: 0.00 \t Val Loss: 7.096\t Val Acc: 0.00\n",
      "[Epoch 295/500] \t Loss: 0.872\t Acc: 0.00 \t Val Loss: 7.464\t Val Acc: 0.00\n",
      "[Epoch 296/500] \t Loss: 0.967\t Acc: 0.00 \t Val Loss: 7.464\t Val Acc: 0.00\n",
      "[Epoch 297/500] \t Loss: 0.829\t Acc: 0.00 \t Val Loss: 7.533\t Val Acc: 0.00\n",
      "[Epoch 298/500] \t Loss: 0.868\t Acc: 0.00 \t Val Loss: 7.276\t Val Acc: 0.00\n",
      "[Epoch 299/500] \t Loss: 0.798\t Acc: 0.00 \t Val Loss: 7.282\t Val Acc: 0.00\n",
      "[Epoch 300/500] \t Loss: 0.798\t Acc: 0.00 \t Val Loss: 7.168\t Val Acc: 0.00\n",
      "[Epoch 301/500] \t Loss: 0.865\t Acc: 0.00 \t Val Loss: 7.230\t Val Acc: 0.00\n",
      "[Epoch 302/500] \t Loss: 1.063\t Acc: 0.00 \t Val Loss: 7.610\t Val Acc: 0.00\n",
      "[Epoch 303/500] \t Loss: 0.898\t Acc: 0.00 \t Val Loss: 7.286\t Val Acc: 0.00\n",
      "[Epoch 304/500] \t Loss: 0.770\t Acc: 0.00 \t Val Loss: 7.313\t Val Acc: 0.00\n",
      "[Epoch 305/500] \t Loss: 0.871\t Acc: 0.00 \t Val Loss: 7.507\t Val Acc: 0.00\n",
      "[Epoch 306/500] \t Loss: 0.943\t Acc: 0.00 \t Val Loss: 7.342\t Val Acc: 0.00\n",
      "[Epoch 307/500] \t Loss: 0.800\t Acc: 0.00 \t Val Loss: 7.428\t Val Acc: 0.00\n",
      "[Epoch 308/500] \t Loss: 0.798\t Acc: 0.00 \t Val Loss: 7.484\t Val Acc: 0.00\n",
      "[Epoch 309/500] \t Loss: 0.791\t Acc: 0.00 \t Val Loss: 7.353\t Val Acc: 0.00\n",
      "[Epoch 310/500] \t Loss: 0.773\t Acc: 0.00 \t Val Loss: 7.351\t Val Acc: 0.00\n",
      "[Epoch 311/500] \t Loss: 0.831\t Acc: 0.00 \t Val Loss: 7.233\t Val Acc: 0.00\n",
      "[Epoch 312/500] \t Loss: 0.833\t Acc: 0.00 \t Val Loss: 7.688\t Val Acc: 0.00\n",
      "[Epoch 313/500] \t Loss: 0.821\t Acc: 0.00 \t Val Loss: 7.409\t Val Acc: 0.00\n",
      "[Epoch 314/500] \t Loss: 0.761\t Acc: 0.00 \t Val Loss: 7.354\t Val Acc: 0.00\n",
      "[Epoch 315/500] \t Loss: 0.777\t Acc: 0.00 \t Val Loss: 7.572\t Val Acc: 0.00\n",
      "[Epoch 316/500] \t Loss: 0.792\t Acc: 0.00 \t Val Loss: 7.257\t Val Acc: 0.00\n",
      "[Epoch 317/500] \t Loss: 0.813\t Acc: 0.00 \t Val Loss: 7.533\t Val Acc: 0.00\n",
      "[Epoch 318/500] \t Loss: 0.777\t Acc: 0.00 \t Val Loss: 7.602\t Val Acc: 0.00\n",
      "[Epoch 319/500] \t Loss: 0.753\t Acc: 0.00 \t Val Loss: 7.229\t Val Acc: 0.00\n",
      "[Epoch 320/500] \t Loss: 0.757\t Acc: 0.00 \t Val Loss: 7.479\t Val Acc: 0.00\n",
      "[Epoch 321/500] \t Loss: 0.770\t Acc: 0.00 \t Val Loss: 7.137\t Val Acc: 0.00\n",
      "[Epoch 322/500] \t Loss: 0.736\t Acc: 0.00 \t Val Loss: 7.433\t Val Acc: 0.00\n",
      "[Epoch 323/500] \t Loss: 0.940\t Acc: 0.00 \t Val Loss: 7.824\t Val Acc: 0.00\n",
      "[Epoch 324/500] \t Loss: 0.883\t Acc: 0.00 \t Val Loss: 8.011\t Val Acc: 0.00\n",
      "[Epoch 325/500] \t Loss: 0.770\t Acc: 0.00 \t Val Loss: 7.458\t Val Acc: 0.00\n",
      "[Epoch 326/500] \t Loss: 0.777\t Acc: 0.00 \t Val Loss: 7.816\t Val Acc: 0.00\n",
      "[Epoch 327/500] \t Loss: 0.760\t Acc: 0.00 \t Val Loss: 7.466\t Val Acc: 0.00\n",
      "[Epoch 328/500] \t Loss: 0.822\t Acc: 0.00 \t Val Loss: 7.630\t Val Acc: 0.00\n",
      "[Epoch 329/500] \t Loss: 0.783\t Acc: 0.00 \t Val Loss: 7.698\t Val Acc: 0.00\n",
      "[Epoch 330/500] \t Loss: 0.735\t Acc: 0.00 \t Val Loss: 7.476\t Val Acc: 0.00\n",
      "[Epoch 331/500] \t Loss: 0.751\t Acc: 0.00 \t Val Loss: 7.430\t Val Acc: 0.00\n",
      "[Epoch 332/500] \t Loss: 0.715\t Acc: 0.00 \t Val Loss: 7.559\t Val Acc: 0.00\n",
      "[Epoch 333/500] \t Loss: 0.718\t Acc: 0.00 \t Val Loss: 7.605\t Val Acc: 0.00\n",
      "[Epoch 334/500] \t Loss: 1.019\t Acc: 0.00 \t Val Loss: 7.354\t Val Acc: 0.00\n",
      "[Epoch 335/500] \t Loss: 0.960\t Acc: 0.00 \t Val Loss: 7.488\t Val Acc: 0.00\n",
      "[Epoch 336/500] \t Loss: 0.962\t Acc: 0.00 \t Val Loss: 7.195\t Val Acc: 0.00\n",
      "[Epoch 337/500] \t Loss: 0.927\t Acc: 0.00 \t Val Loss: 7.843\t Val Acc: 0.00\n",
      "[Epoch 338/500] \t Loss: 0.713\t Acc: 0.00 \t Val Loss: 7.513\t Val Acc: 0.00\n",
      "[Epoch 339/500] \t Loss: 0.726\t Acc: 0.00 \t Val Loss: 7.607\t Val Acc: 0.00\n",
      "[Epoch 340/500] \t Loss: 0.729\t Acc: 0.00 \t Val Loss: 8.127\t Val Acc: 0.00\n",
      "[Epoch 341/500] \t Loss: 0.718\t Acc: 0.00 \t Val Loss: 7.279\t Val Acc: 0.00\n",
      "[Epoch 342/500] \t Loss: 0.757\t Acc: 0.00 \t Val Loss: 8.072\t Val Acc: 0.00\n",
      "[Epoch 343/500] \t Loss: 1.005\t Acc: 0.00 \t Val Loss: 8.165\t Val Acc: 0.00\n",
      "[Epoch 344/500] \t Loss: 1.012\t Acc: 0.00 \t Val Loss: 8.054\t Val Acc: 0.00\n",
      "[Epoch 345/500] \t Loss: 0.820\t Acc: 0.00 \t Val Loss: 7.639\t Val Acc: 0.00\n",
      "[Epoch 346/500] \t Loss: 0.979\t Acc: 0.00 \t Val Loss: 7.858\t Val Acc: 0.00\n",
      "[Epoch 347/500] \t Loss: 0.749\t Acc: 0.00 \t Val Loss: 7.507\t Val Acc: 0.00\n",
      "[Epoch 348/500] \t Loss: 0.744\t Acc: 0.00 \t Val Loss: 8.147\t Val Acc: 0.00\n",
      "[Epoch 349/500] \t Loss: 0.694\t Acc: 0.00 \t Val Loss: 8.168\t Val Acc: 0.00\n",
      "[Epoch 350/500] \t Loss: 0.693\t Acc: 0.00 \t Val Loss: 7.799\t Val Acc: 0.00\n",
      "[Epoch 351/500] \t Loss: 1.051\t Acc: 0.00 \t Val Loss: 7.969\t Val Acc: 0.00\n",
      "[Epoch 352/500] \t Loss: 1.001\t Acc: 0.00 \t Val Loss: 7.514\t Val Acc: 0.00\n",
      "[Epoch 353/500] \t Loss: 0.983\t Acc: 0.00 \t Val Loss: 7.490\t Val Acc: 0.00\n",
      "[Epoch 354/500] \t Loss: 0.929\t Acc: 0.00 \t Val Loss: 8.418\t Val Acc: 0.00\n",
      "[Epoch 355/500] \t Loss: 0.743\t Acc: 0.00 \t Val Loss: 7.758\t Val Acc: 0.00\n",
      "[Epoch 356/500] \t Loss: 0.752\t Acc: 0.00 \t Val Loss: 7.846\t Val Acc: 0.00\n",
      "[Epoch 357/500] \t Loss: 0.943\t Acc: 0.00 \t Val Loss: 7.865\t Val Acc: 0.00\n",
      "[Epoch 358/500] \t Loss: 0.751\t Acc: 0.00 \t Val Loss: 7.869\t Val Acc: 0.00\n",
      "[Epoch 359/500] \t Loss: 0.707\t Acc: 0.00 \t Val Loss: 7.585\t Val Acc: 0.00\n",
      "[Epoch 360/500] \t Loss: 0.720\t Acc: 0.00 \t Val Loss: 7.874\t Val Acc: 0.00\n",
      "[Epoch 361/500] \t Loss: 0.745\t Acc: 0.00 \t Val Loss: 7.745\t Val Acc: 0.00\n",
      "[Epoch 362/500] \t Loss: 0.746\t Acc: 0.00 \t Val Loss: 7.708\t Val Acc: 0.00\n",
      "[Epoch 363/500] \t Loss: 0.830\t Acc: 0.00 \t Val Loss: 7.600\t Val Acc: 0.00\n",
      "[Epoch 364/500] \t Loss: 0.847\t Acc: 0.00 \t Val Loss: 7.647\t Val Acc: 0.00\n",
      "[Epoch 365/500] \t Loss: 0.771\t Acc: 0.00 \t Val Loss: 7.805\t Val Acc: 0.00\n",
      "[Epoch 366/500] \t Loss: 0.715\t Acc: 0.00 \t Val Loss: 7.695\t Val Acc: 0.00\n",
      "[Epoch 367/500] \t Loss: 0.672\t Acc: 0.00 \t Val Loss: 7.856\t Val Acc: 0.00\n",
      "[Epoch 368/500] \t Loss: 0.703\t Acc: 0.00 \t Val Loss: 7.870\t Val Acc: 0.00\n",
      "[Epoch 369/500] \t Loss: 0.708\t Acc: 0.00 \t Val Loss: 7.539\t Val Acc: 0.00\n",
      "[Epoch 370/500] \t Loss: 0.697\t Acc: 0.00 \t Val Loss: 7.672\t Val Acc: 0.00\n",
      "[Epoch 371/500] \t Loss: 1.059\t Acc: 0.00 \t Val Loss: 8.065\t Val Acc: 0.00\n",
      "[Epoch 372/500] \t Loss: 0.742\t Acc: 0.00 \t Val Loss: 7.686\t Val Acc: 0.00\n",
      "[Epoch 373/500] \t Loss: 0.670\t Acc: 0.00 \t Val Loss: 7.992\t Val Acc: 0.00\n",
      "[Epoch 374/500] \t Loss: 0.667\t Acc: 0.00 \t Val Loss: 7.865\t Val Acc: 0.00\n",
      "[Epoch 375/500] \t Loss: 0.724\t Acc: 0.00 \t Val Loss: 8.315\t Val Acc: 0.00\n",
      "[Epoch 376/500] \t Loss: 0.713\t Acc: 0.00 \t Val Loss: 7.858\t Val Acc: 0.00\n",
      "[Epoch 377/500] \t Loss: 0.690\t Acc: 0.00 \t Val Loss: 8.410\t Val Acc: 0.00\n",
      "[Epoch 378/500] \t Loss: 0.694\t Acc: 0.00 \t Val Loss: 7.748\t Val Acc: 0.00\n",
      "[Epoch 379/500] \t Loss: 0.721\t Acc: 0.00 \t Val Loss: 7.733\t Val Acc: 0.00\n",
      "[Epoch 380/500] \t Loss: 0.687\t Acc: 0.00 \t Val Loss: 8.062\t Val Acc: 0.00\n",
      "[Epoch 381/500] \t Loss: 0.689\t Acc: 0.00 \t Val Loss: 7.890\t Val Acc: 0.00\n",
      "[Epoch 382/500] \t Loss: 0.671\t Acc: 0.00 \t Val Loss: 7.722\t Val Acc: 0.00\n",
      "[Epoch 383/500] \t Loss: 0.655\t Acc: 0.00 \t Val Loss: 7.858\t Val Acc: 0.00\n",
      "[Epoch 384/500] \t Loss: 0.667\t Acc: 0.00 \t Val Loss: 7.793\t Val Acc: 0.00\n",
      "[Epoch 385/500] \t Loss: 0.654\t Acc: 0.00 \t Val Loss: 7.828\t Val Acc: 0.00\n",
      "[Epoch 386/500] \t Loss: 0.681\t Acc: 0.00 \t Val Loss: 7.920\t Val Acc: 0.00\n",
      "[Epoch 387/500] \t Loss: 0.672\t Acc: 0.00 \t Val Loss: 7.723\t Val Acc: 0.00\n",
      "[Epoch 388/500] \t Loss: 0.707\t Acc: 0.00 \t Val Loss: 7.750\t Val Acc: 0.00\n",
      "[Epoch 389/500] \t Loss: 0.680\t Acc: 0.00 \t Val Loss: 7.982\t Val Acc: 0.00\n",
      "[Epoch 390/500] \t Loss: 0.636\t Acc: 0.00 \t Val Loss: 8.110\t Val Acc: 0.00\n",
      "[Epoch 391/500] \t Loss: 0.740\t Acc: 0.00 \t Val Loss: 7.662\t Val Acc: 0.00\n",
      "[Epoch 392/500] \t Loss: 0.642\t Acc: 0.00 \t Val Loss: 8.136\t Val Acc: 0.00\n",
      "[Epoch 393/500] \t Loss: 0.666\t Acc: 0.00 \t Val Loss: 7.539\t Val Acc: 0.00\n",
      "[Epoch 394/500] \t Loss: 0.879\t Acc: 0.00 \t Val Loss: 7.638\t Val Acc: 0.00\n",
      "[Epoch 395/500] \t Loss: 1.234\t Acc: 0.00 \t Val Loss: 8.155\t Val Acc: 0.00\n",
      "[Epoch 396/500] \t Loss: 0.749\t Acc: 0.00 \t Val Loss: 7.962\t Val Acc: 0.00\n",
      "[Epoch 397/500] \t Loss: 0.811\t Acc: 0.00 \t Val Loss: 8.556\t Val Acc: 0.00\n",
      "[Epoch 398/500] \t Loss: 0.790\t Acc: 0.00 \t Val Loss: 7.930\t Val Acc: 0.00\n",
      "[Epoch 399/500] \t Loss: 0.659\t Acc: 0.00 \t Val Loss: 7.695\t Val Acc: 0.00\n",
      "[Epoch 400/500] \t Loss: 0.626\t Acc: 0.00 \t Val Loss: 8.089\t Val Acc: 0.00\n",
      "[Epoch 401/500] \t Loss: 0.648\t Acc: 0.00 \t Val Loss: 8.043\t Val Acc: 0.00\n",
      "[Epoch 402/500] \t Loss: 0.739\t Acc: 0.00 \t Val Loss: 7.964\t Val Acc: 0.00\n",
      "[Epoch 403/500] \t Loss: 0.704\t Acc: 0.00 \t Val Loss: 7.941\t Val Acc: 0.00\n",
      "[Epoch 404/500] \t Loss: 0.711\t Acc: 0.00 \t Val Loss: 8.068\t Val Acc: 0.00\n",
      "[Epoch 405/500] \t Loss: 0.695\t Acc: 0.00 \t Val Loss: 7.900\t Val Acc: 0.00\n",
      "[Epoch 406/500] \t Loss: 0.955\t Acc: 0.00 \t Val Loss: 7.790\t Val Acc: 0.00\n",
      "[Epoch 407/500] \t Loss: 0.901\t Acc: 0.00 \t Val Loss: 7.735\t Val Acc: 0.00\n",
      "[Epoch 408/500] \t Loss: 0.613\t Acc: 0.00 \t Val Loss: 8.069\t Val Acc: 0.00\n",
      "[Epoch 409/500] \t Loss: 0.702\t Acc: 0.00 \t Val Loss: 8.174\t Val Acc: 0.00\n",
      "[Epoch 410/500] \t Loss: 0.660\t Acc: 0.00 \t Val Loss: 8.088\t Val Acc: 0.00\n",
      "[Epoch 411/500] \t Loss: 0.917\t Acc: 0.00 \t Val Loss: 8.047\t Val Acc: 0.00\n",
      "[Epoch 412/500] \t Loss: 1.049\t Acc: 0.00 \t Val Loss: 8.061\t Val Acc: 0.00\n",
      "[Epoch 413/500] \t Loss: 0.814\t Acc: 0.00 \t Val Loss: 8.094\t Val Acc: 0.00\n",
      "[Epoch 414/500] \t Loss: 1.066\t Acc: 0.00 \t Val Loss: 7.952\t Val Acc: 0.00\n",
      "[Epoch 415/500] \t Loss: 0.723\t Acc: 0.00 \t Val Loss: 8.182\t Val Acc: 0.00\n",
      "[Epoch 416/500] \t Loss: 0.779\t Acc: 0.00 \t Val Loss: 8.121\t Val Acc: 0.00\n",
      "[Epoch 417/500] \t Loss: 0.675\t Acc: 0.00 \t Val Loss: 7.930\t Val Acc: 0.00\n",
      "[Epoch 418/500] \t Loss: 0.678\t Acc: 0.00 \t Val Loss: 8.019\t Val Acc: 0.00\n",
      "[Epoch 419/500] \t Loss: 0.684\t Acc: 0.00 \t Val Loss: 7.968\t Val Acc: 0.00\n",
      "[Epoch 420/500] \t Loss: 0.664\t Acc: 0.00 \t Val Loss: 8.010\t Val Acc: 0.00\n",
      "[Epoch 421/500] \t Loss: 0.688\t Acc: 0.00 \t Val Loss: 8.056\t Val Acc: 0.00\n",
      "[Epoch 422/500] \t Loss: 0.622\t Acc: 0.00 \t Val Loss: 8.522\t Val Acc: 0.00\n",
      "[Epoch 423/500] \t Loss: 0.632\t Acc: 0.00 \t Val Loss: 8.160\t Val Acc: 0.00\n",
      "[Epoch 424/500] \t Loss: 1.098\t Acc: 0.00 \t Val Loss: 8.034\t Val Acc: 0.00\n",
      "[Epoch 425/500] \t Loss: 0.789\t Acc: 0.00 \t Val Loss: 8.384\t Val Acc: 0.00\n",
      "[Epoch 426/500] \t Loss: 0.761\t Acc: 0.00 \t Val Loss: 8.015\t Val Acc: 0.00\n",
      "[Epoch 427/500] \t Loss: 0.690\t Acc: 0.00 \t Val Loss: 8.743\t Val Acc: 0.00\n",
      "[Epoch 428/500] \t Loss: 0.629\t Acc: 0.00 \t Val Loss: 8.536\t Val Acc: 0.00\n",
      "[Epoch 429/500] \t Loss: 0.692\t Acc: 0.00 \t Val Loss: 8.717\t Val Acc: 0.00\n",
      "[Epoch 430/500] \t Loss: 0.637\t Acc: 0.00 \t Val Loss: 8.397\t Val Acc: 0.00\n",
      "[Epoch 431/500] \t Loss: 0.665\t Acc: 0.00 \t Val Loss: 8.516\t Val Acc: 0.00\n",
      "[Epoch 432/500] \t Loss: 0.637\t Acc: 0.00 \t Val Loss: 8.484\t Val Acc: 0.00\n",
      "[Epoch 433/500] \t Loss: 0.641\t Acc: 0.00 \t Val Loss: 8.138\t Val Acc: 0.00\n",
      "[Epoch 434/500] \t Loss: 0.598\t Acc: 0.00 \t Val Loss: 8.549\t Val Acc: 0.00\n",
      "[Epoch 435/500] \t Loss: 0.607\t Acc: 0.00 \t Val Loss: 8.411\t Val Acc: 0.00\n",
      "[Epoch 436/500] \t Loss: 0.591\t Acc: 0.00 \t Val Loss: 8.488\t Val Acc: 0.00\n",
      "[Epoch 437/500] \t Loss: 0.902\t Acc: 0.00 \t Val Loss: 8.448\t Val Acc: 0.00\n",
      "[Epoch 438/500] \t Loss: 0.586\t Acc: 0.00 \t Val Loss: 8.569\t Val Acc: 0.00\n",
      "[Epoch 439/500] \t Loss: 0.602\t Acc: 0.00 \t Val Loss: 8.496\t Val Acc: 0.00\n",
      "[Epoch 440/500] \t Loss: 0.594\t Acc: 0.00 \t Val Loss: 8.470\t Val Acc: 0.00\n",
      "[Epoch 441/500] \t Loss: 0.588\t Acc: 0.00 \t Val Loss: 8.097\t Val Acc: 0.00\n",
      "[Epoch 442/500] \t Loss: 0.603\t Acc: 0.00 \t Val Loss: 8.360\t Val Acc: 0.00\n",
      "[Epoch 443/500] \t Loss: 0.606\t Acc: 0.00 \t Val Loss: 8.187\t Val Acc: 0.00\n",
      "[Epoch 444/500] \t Loss: 0.580\t Acc: 0.00 \t Val Loss: 8.390\t Val Acc: 0.00\n",
      "[Epoch 445/500] \t Loss: 0.596\t Acc: 0.00 \t Val Loss: 8.448\t Val Acc: 0.00\n",
      "[Epoch 446/500] \t Loss: 0.596\t Acc: 0.00 \t Val Loss: 8.118\t Val Acc: 0.00\n",
      "[Epoch 447/500] \t Loss: 0.566\t Acc: 0.00 \t Val Loss: 8.481\t Val Acc: 0.00\n",
      "[Epoch 448/500] \t Loss: 0.601\t Acc: 0.00 \t Val Loss: 8.791\t Val Acc: 0.00\n",
      "[Epoch 449/500] \t Loss: 0.573\t Acc: 0.00 \t Val Loss: 8.496\t Val Acc: 0.00\n",
      "[Epoch 450/500] \t Loss: 0.556\t Acc: 0.00 \t Val Loss: 8.815\t Val Acc: 0.00\n",
      "[Epoch 451/500] \t Loss: 0.560\t Acc: 0.00 \t Val Loss: 8.851\t Val Acc: 0.00\n",
      "[Epoch 452/500] \t Loss: 0.552\t Acc: 0.00 \t Val Loss: 8.298\t Val Acc: 0.00\n",
      "[Epoch 453/500] \t Loss: 0.570\t Acc: 0.00 \t Val Loss: 8.789\t Val Acc: 0.00\n",
      "[Epoch 454/500] \t Loss: 0.550\t Acc: 0.00 \t Val Loss: 8.233\t Val Acc: 0.00\n",
      "[Epoch 455/500] \t Loss: 0.546\t Acc: 0.00 \t Val Loss: 8.659\t Val Acc: 0.00\n",
      "[Epoch 456/500] \t Loss: 0.551\t Acc: 0.00 \t Val Loss: 8.896\t Val Acc: 0.00\n",
      "[Epoch 457/500] \t Loss: 0.550\t Acc: 0.00 \t Val Loss: 8.279\t Val Acc: 0.00\n",
      "[Epoch 458/500] \t Loss: 0.541\t Acc: 0.00 \t Val Loss: 8.769\t Val Acc: 0.00\n",
      "[Epoch 459/500] \t Loss: 0.818\t Acc: 0.00 \t Val Loss: 8.759\t Val Acc: 0.00\n",
      "[Epoch 460/500] \t Loss: 0.734\t Acc: 0.00 \t Val Loss: 8.263\t Val Acc: 0.00\n",
      "[Epoch 461/500] \t Loss: 0.546\t Acc: 0.00 \t Val Loss: 8.679\t Val Acc: 0.00\n",
      "[Epoch 462/500] \t Loss: 0.540\t Acc: 0.00 \t Val Loss: 8.261\t Val Acc: 0.00\n",
      "[Epoch 463/500] \t Loss: 1.145\t Acc: 0.00 \t Val Loss: 8.756\t Val Acc: 0.00\n",
      "[Epoch 464/500] \t Loss: 0.860\t Acc: 0.00 \t Val Loss: 8.873\t Val Acc: 0.00\n",
      "[Epoch 465/500] \t Loss: 0.574\t Acc: 0.00 \t Val Loss: 8.337\t Val Acc: 0.00\n",
      "[Epoch 466/500] \t Loss: 0.565\t Acc: 0.00 \t Val Loss: 9.288\t Val Acc: 0.00\n",
      "[Epoch 467/500] \t Loss: 0.544\t Acc: 0.00 \t Val Loss: 9.126\t Val Acc: 0.00\n",
      "[Epoch 468/500] \t Loss: 0.517\t Acc: 0.00 \t Val Loss: 8.873\t Val Acc: 0.00\n",
      "[Epoch 469/500] \t Loss: 0.695\t Acc: 0.00 \t Val Loss: 8.957\t Val Acc: 0.00\n",
      "[Epoch 470/500] \t Loss: 0.572\t Acc: 0.00 \t Val Loss: 9.230\t Val Acc: 0.00\n",
      "[Epoch 471/500] \t Loss: 0.556\t Acc: 0.00 \t Val Loss: 8.378\t Val Acc: 0.00\n",
      "[Epoch 472/500] \t Loss: 0.547\t Acc: 0.00 \t Val Loss: 8.439\t Val Acc: 0.00\n",
      "[Epoch 473/500] \t Loss: 0.516\t Acc: 0.00 \t Val Loss: 8.964\t Val Acc: 0.00\n",
      "[Epoch 474/500] \t Loss: 0.531\t Acc: 0.00 \t Val Loss: 8.972\t Val Acc: 0.00\n",
      "[Epoch 475/500] \t Loss: 0.516\t Acc: 0.00 \t Val Loss: 8.968\t Val Acc: 0.00\n",
      "[Epoch 476/500] \t Loss: 0.524\t Acc: 0.00 \t Val Loss: 8.997\t Val Acc: 0.00\n",
      "[Epoch 477/500] \t Loss: 0.533\t Acc: 0.00 \t Val Loss: 8.485\t Val Acc: 0.00\n",
      "[Epoch 478/500] \t Loss: 0.522\t Acc: 0.00 \t Val Loss: 8.895\t Val Acc: 0.00\n",
      "[Epoch 479/500] \t Loss: 0.536\t Acc: 0.00 \t Val Loss: 9.103\t Val Acc: 0.00\n",
      "[Epoch 480/500] \t Loss: 0.522\t Acc: 0.00 \t Val Loss: 8.721\t Val Acc: 0.00\n",
      "[Epoch 481/500] \t Loss: 0.507\t Acc: 50.00 \t Val Loss: 8.500\t Val Acc: 0.00\n",
      "[Epoch 482/500] \t Loss: 0.519\t Acc: 50.00 \t Val Loss: 9.060\t Val Acc: 0.00\n",
      "[Epoch 483/500] \t Loss: 0.478\t Acc: 50.00 \t Val Loss: 8.938\t Val Acc: 0.00\n",
      "[Epoch 484/500] \t Loss: 0.490\t Acc: 50.00 \t Val Loss: 9.131\t Val Acc: 0.00\n",
      "[Epoch 485/500] \t Loss: 0.670\t Acc: 50.00 \t Val Loss: 9.029\t Val Acc: 0.00\n",
      "[Epoch 486/500] \t Loss: 0.506\t Acc: 0.00 \t Val Loss: 8.407\t Val Acc: 0.00\n",
      "[Epoch 487/500] \t Loss: 0.495\t Acc: 0.00 \t Val Loss: 8.014\t Val Acc: 0.00\n",
      "[Epoch 488/500] \t Loss: 0.475\t Acc: 0.00 \t Val Loss: 8.677\t Val Acc: 0.00\n",
      "[Epoch 489/500] \t Loss: 0.501\t Acc: 0.00 \t Val Loss: 8.721\t Val Acc: 0.00\n",
      "[Epoch 490/500] \t Loss: 0.480\t Acc: 50.00 \t Val Loss: 8.531\t Val Acc: 0.00\n",
      "[Epoch 491/500] \t Loss: 0.499\t Acc: 50.00 \t Val Loss: 9.169\t Val Acc: 0.00\n",
      "[Epoch 492/500] \t Loss: 0.456\t Acc: 50.00 \t Val Loss: 9.004\t Val Acc: 0.00\n",
      "[Epoch 493/500] \t Loss: 0.463\t Acc: 50.00 \t Val Loss: 9.209\t Val Acc: 0.00\n",
      "[Epoch 494/500] \t Loss: 0.485\t Acc: 0.00 \t Val Loss: 8.880\t Val Acc: 0.00\n",
      "[Epoch 495/500] \t Loss: 0.466\t Acc: 50.00 \t Val Loss: 8.804\t Val Acc: 0.00\n",
      "[Epoch 496/500] \t Loss: 1.770\t Acc: 0.00 \t Val Loss: 8.661\t Val Acc: 0.00\n",
      "[Epoch 497/500] \t Loss: 0.684\t Acc: 50.00 \t Val Loss: 8.825\t Val Acc: 0.00\n",
      "[Epoch 498/500] \t Loss: 0.468\t Acc: 50.00 \t Val Loss: 8.517\t Val Acc: 0.00\n",
      "[Epoch 499/500] \t Loss: 0.483\t Acc: 50.00 \t Val Loss: 8.939\t Val Acc: 0.00\n",
      "['<தொட்டாசாசார்்>ய>ய>ய>ய>ய>ய>ய>ய>ய>ய>ய>ய>ய>ய>ய>ய>ய>ய', '<மென்மைதான்>்>ய>ய>ய>ய>ய>ய>ய>ய>ய>ய>ய>ய>ய>ய>ய>ய>ய>ய>']\n",
      "['<தொட்டாச்சார்ய>                                   ', '<மென்மைதான்>                                      ']\n",
      "[Epoch 500/500] \t Loss: 0.550\t Acc: 50.00 \t Val Loss: 9.544\t Val Acc: 0.00\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAGzCAYAAABzfl4TAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABrP0lEQVR4nO3dd1hTd98G8DsECDOAbMoQ99a6qVpnta5qRevqW+2ytdhWbfu0dmhrh93V1tHxWLWP26rVqtVatxYXihW1uHAg08EmAZLz/hEJCSQhgZDF/bmuXCTn/M45XwI5fPlNkSAIAoiIiIjMwMnaARAREZHjYGJBREREZsPEgoiIiMyGiQURERGZDRMLIiIiMhsmFkRERGQ2TCyIiIjIbJhYEBERkdkwsSAiIiKzYWJBREREZsPEwkEtX74cIpFI7+Po0aPWDpGILGTx4sUQiUTo1q2btUOhesDZ2gFQ3Zo7dy6io6OrbG/SpIkVoiEia1i1ahUaNmyI48eP4/Lly/z8U51iYuHgBg8ejM6dOxtdvqysDEqlEq6urlX2FRYWwtPTs8axCIIAmUwGd3f3Gp+DiEyTkpKCv//+G5s2bcILL7yAVatWYc6cOdYOq4ra3l/IdrAppB67du0aRCIRvvzyS8yfPx+NGzeGRCLB+fPn8f7770MkEuH8+fOYMGEC/Pz80LNnTwCq5OPDDz9Ul2/YsCHefvttyOVyrfM3bNgQw4YNw65du9C5c2e4u7vjhx9+AADs3r0bPXv2hK+vL7y8vNC8eXO8/fbbFn8PiBzdqlWr4Ofnh6FDh2L06NFYtWpVlTI5OTmYMWMGGjZsCIlEgvDwcDz11FO4ffu2uoxMJsP777+PZs2awc3NDaGhoRg1ahSuXLkCANi/fz9EIhH279+vde7y+8zy5cvV2yZPngwvLy9cuXIFQ4YMgbe3NyZOnAgAOHToEMaMGYPIyEhIJBJERERgxowZKC4urhL3v//+iyeeeAKBgYFwd3dH8+bN8c477wAA9u3bB5FIhM2bN1c5bvXq1RCJRIiPjzf5/aTqscbCweXm5mrdHABAJBLB399f/XrZsmWQyWSYMmUKJBIJGjRooN43ZswYNG3aFJ988gkEQQAAPPfcc1ixYgVGjx6N1157DceOHcO8efNw4cKFKh/i5ORkjB8/Hi+88AKef/55NG/eHOfOncOwYcPQrl07zJ07FxKJBJcvX8aRI0fq8J0gqp9WrVqFUaNGwdXVFePHj8eSJUtw4sQJdOnSBQBQUFCAXr164cKFC3jmmWfQsWNH3L59G1u3bkVqaioCAgKgUCgwbNgw7NmzB+PGjcOrr76K/Px87N69G0lJSWjcuLHJcZWVlWHQoEHo2bMnvvzyS3h4eAAANmzYgKKiIkydOhX+/v44fvw4vvvuO6SmpmLDhg3q4//55x/06tULLi4umDJlCho2bIgrV67g999/x8cff4w+ffogIiICq1atwuOPP17lPWncuDFiYmJq8c6SXgI5pGXLlgkAdD4kEokgCIKQkpIiABCkUqmQlZWldfycOXMEAML48eO1ticmJgoAhOeee05r++uvvy4AEPbu3aveFhUVJQAQdu7cqVX2m2++EQAI2dnZ5vyWiaiSkydPCgCE3bt3C4IgCEqlUggPDxdeffVVdZnZs2cLAIRNmzZVOV6pVAqCIAg///yzAED4+uuv9ZbZt2+fAEDYt2+f1v7y+8yyZcvU2yZNmiQAEN56660q5ysqKqqybd68eYJIJBKuX7+u3vbwww8L3t7eWts04xEEQZg1a5YgkUiEnJwc9basrCzB2dlZmDNnTpXrkHmwKcTBLVq0CLt379Z6/PHHH1plYmNjERgYqPP4F198Uev1jh07AAAzZ87U2v7aa68BALZv3661PTo6GoMGDdLa5uvrCwDYsmULlEqlad8QERlt1apVCA4ORt++fQGoaivHjh2LtWvXQqFQAAA2btyI9u3bV/mvvrx8eZmAgAC8/PLLesvUxNSpU6ts0+yDVVhYiNu3b+Ohhx6CIAg4ffo0ACA7OxsHDx7EM888g8jISL3xPPXUU5DL5fj111/V29atW4eysjI8+eSTNY6bDGNi4eC6du2KAQMGaD3KbzLldI0a0bfv+vXrcHJyqtKrPCQkBL6+vrh+/Xq15x47dix69OiB5557DsHBwRg3bhzWr1/PJIPIjBQKBdauXYu+ffsiJSUFly9fxuXLl9GtWzdkZmZiz549AIArV66gTZs2Bs915coVNG/eHM7O5ms9d3Z2Rnh4eJXtN27cwOTJk9GgQQN4eXkhMDAQvXv3BqBq2gWAq1evAkC1cbdo0QJdunTR6leyatUqdO/enSNj6hD7WJDBURr69hn7X4qu493d3XHw4EHs27cP27dvx86dO7Fu3Tr069cPf/75J8RisXGBE5Fee/fuRXp6OtauXYu1a9dW2b9q1SoMHDjQbNfTd08orxmpTCKRwMnJqUrZRx55BHfv3sWbb76JFi1awNPTE7du3cLkyZNr9M/HU089hVdffRWpqamQy+U4evQoFi5caPJ5yHhMLMgkUVFRUCqVuHTpElq2bKnenpmZiZycHERFRRl1HicnJ/Tv3x/9+/fH119/jU8++QTvvPMO9u3bhwEDBtRV+ET1xqpVqxAUFIRFixZV2bdp0yZs3rwZ33//PRo3boykpCSD52rcuDGOHTuG0tJSuLi46Czj5+cHQDXCRFPlWkxDzp49i4sXL2LFihV46qmn1Nt3796tVa5Ro0YAUG3cADBu3DjMnDkTa9asQXFxMVxcXDB27FijYyLTsSmETDJkyBAAwPz587W2f/311wCAoUOHVnuOu3fvVtnWoUMHAKgyZJWITFdcXIxNmzZh2LBhGD16dJXHtGnTkJ+fj61btyI2NhZnzpzROSxTuD8SLDY2Frdv39b5n355maioKIjFYhw8eFBr/+LFi42Ou7y2svyc5c8XLFigVS4wMBAPP/wwfv75Z9y4cUNnPOUCAgIwePBgrFy5EqtWrcKjjz6KgIAAo2Mi07HGwsH98ccf+Pfff6tsf+ihh6pUQxqjffv2mDRpEn788Ufk5OSgd+/eOH78OFasWIGRI0dW6b+hy9y5c3Hw4EEMHToUUVFRyMrKwuLFixEeHq6eK4OIam7r1q3Iz8/HY489pnN/9+7dERgYiFWrVmH16tX49ddfMWbMGDzzzDPo1KkT7t69i61bt+L7779H+/bt8dRTT+GXX37BzJkzcfz4cfTq1QuFhYX466+/8NJLL2HEiBHw8fHBmDFj8N1330EkEqFx48bYtm0bsrKyjI67RYsWaNy4MV5//XXcunULUqkUGzduxL1796qU/fbbb9GzZ0907NgRU6ZMQXR0NK5du4bt27cjMTFRq+xTTz2F0aNHAwA+/PBD499IqhlrDkmhumNouCnuD/0qHwb2xRdfVDm+fLipriGhpaWlwgcffCBER0cLLi4uQkREhDBr1ixBJpNplYuKihKGDh1a5fg9e/YII0aMEMLCwgRXV1chLCxMGD9+vHDx4kXzvQFE9djw4cMFNzc3obCwUG+ZyZMnCy4uLsLt27eFO3fuCNOmTRMeeOABwdXVVQgPDxcmTZok3L59W12+qKhIeOedd9Sf+5CQEGH06NHClStX1GWys7OF2NhYwcPDQ/Dz8xNeeOEFISkpSedwU09PT51xnT9/XhgwYIDg5eUlBAQECM8//7xw5syZKucQBEFISkoSHn/8ccHX11dwc3MTmjdvLrz33ntVzimXywU/Pz/Bx8dHKC4uNvJdpJoSCUKleiMiIiIHUlZWhrCwMAwfPhxLly61djgOj30siIjIof3222/Izs7W6hBKdYc1FkRE5JCOHTuGf/75Bx9++CECAgJw6tQpa4dUL7DGgoiIHNKSJUswdepUBAUF4ZdffrF2OPUGayyIiIjIbFhjQURERGbDxIKIiIjMxuITZCmVSqSlpcHb27tWq+IRUc0IgoD8/HyEhYXVaJI0a+B9g8j6jL13WDyxSEtLQ0REhKUvS0SV3Lx5U+fqkraI9w0i21HdvcPiiYW3tzcAVWBSqdTSlyeq9/Ly8hAREaH+LNoD3jeIrM/Ye4fFE4vyakypVMobBJEV2VOTAu8bRLajunuHfTSwEhERkV3g6qYOSqlUoqSkxNphkBW5urraTedMMo5CoUBpaam1wyAH5uLiol6+vqaYWDigkpISpKSkQKlUWjsUsiInJydER0fD1dXV2qFQLQmCgIyMDOTk5Fg7FKoHfH19ERISUuPmUiYWDkYQBKSnp0MsFiMiIoL/sdZT5cMz09PTERkZaVf9Kaiq8qQiKCgIHh4e/HlSnRAEAUVFRcjKygIAhIaG1ug8TCwcTFlZGYqKihAWFgYPDw9rh0NWFBgYiLS0NJSVlcHFxcXa4VANKRQKdVLh7+9v7XDIwbm7uwMAsrKyEBQUVKNmEf4762AUCgUAsPqb1L8D5b8TZJ/K+1TwHwWylPLftZr252Fi4aBYVUr8HXAs/HmSpdT2d42JBREREZkNEwuyKy+++CI+//xzo8q2bt0a8fHxdRwREdUGP9OOh503ySK8vLzUzwsLC7V6tp8/fx6RkZFGnef77783+prnzp0zLUgjiUSiKj3z165di2HDhtXJ9YhskSN9pgFAJpMhODgY/fv3x6ZNm+rsOvUBayzIIgoKCtQPiUSCc+fOqV+X34AEQbCbuTeSk5O1viddSUVZWZlR2/Qxpaw1vf/++xCJRFqPFi1aqPfLZDLExcXB398fXl5eiI2NRWZmphUjJnNwtM/01q1bIRaLsWPHDovOF2Ivn3NTMLFwYIIgoKikzCIPQRBqFOPkyZMxbdo09OvXDx4eHrhy5Qp+/vlnNGvWDN7e3mjXrh3279+vVf6jjz4CACxfvhz9+vXD1KlTIZVK0apVK5w6dUpdtmHDhjh8+LD6uFdeeQX9+/eHt7c3Bg4ciLt376rL/vTTTwgPD0dISAh++ukniEQipKammvz9NGzYEJ9//jlatmyJJk2aYPny5ejbty9eeOEF+Pj4YNmyZbh58yaGDBkCPz8/tGrVClu2bFEf36dPH7z33nvo3LkzPD097WaWxdatWyM9PV39KH/fAWDGjBn4/fffsWHDBhw4cABpaWkYNWqUFaO1b5b6XNe3z/TKlSvx0ksvoXHjxli/fr3WvnXr1qFNmzbw9vZG27ZtkZycDABISUnB0KFD4e/vj9DQUHz77bdVvqfy72vAgAEAgP3796NJkyaYM2cOAgICMGfOHFy5cgUPP/wwfH19ERYWhrfffrva63/yySeYPHmyVrnevXtj9erVBn8+lsCmEAdWXKpAq9m7LHKt83MHwcO1Zr9Oa9euxa5du9C+fXsIgoBLly5hz549CAsLw88//4xx48bh+vXrkEgkVY49dOgQnn/+eSxcuBBz5szBjBkzcODAAZ3XWb9+Pf788080a9YMQ4cOxYIFC/DBBx/g7NmzeOONN/DXX3+hdevWeOmll2r0fZTbtGkT9u/fD6lUinXr1uHQoUOYNGkSlixZArlcjkceeQQxMTHYvHkz4uPjMXz4cJw+fRpNmjQBAKxZswY7d+5EeHg4nJ2NfE/L5EBhNuDkDHiH1Cr+mnB2dkZISNXr5ubmYunSpVi9ejX69esHAFi2bBlatmyJo0ePonv37jrPJ5fLIZfL1a/z8vLqJnA7ZKnPdX36TN+5cwc7d+7Exx9/DIlEgpUrV2LKlCkAgCNHjiAuLg5btmxBTEwMLl68CKlUirKyMgwdOhRPPPEENm7ciJKSEly6dMmo9+fatWsQi8VIT09HWVkZ0tLS8OGHH6JHjx5ISUlB//790bVrV4wcOVLv9SdOnIgOHTpAJpPBzc0NqampOH36NEaMGKF1rTKFErcLS6BUCnB1doKnqxj3ikpRqlCiVCHA3UUMkQjwcXeBp8Q8KQFrLMjqYmNj0alTJzg7O8PFxQVDhgxBREQExGIxnn/+eYhEIr0f2BYtWmD8+PEQi8WYMGECzpw5o/c6Y8aMQbt27eDm5obY2Fh12Y0bN2LUqFHo3Lkz3N3d8e6771Ybc+vWreHr66t+pKSkqPe9+uqrCA4OVk8007hxY0yePBlOTk64ffs2Tp48iblz50IikaBPnz4YNmwYNmzYoD7+2WefRZMmTeDm5mb8sC9FqSqxKLpbfdk6cOnSJYSFhaFRo0aYOHEibty4AQBISEhAaWmp+r81QPUzi4yMNNgJb968efDx8VE/IiIi6vx7IPOxt8/0unXr0KRJE7Rt2xbjxo3D4cOHcf36dQCq2oYXXngBPXr0gJOTE1q0aIHQ0FAcO3YM+fn5mD17Ntzc3CCVStGpUyej3h+JRIK3334bLi4ucHd3R+PGjdG7d284OzujadOmmDhxorpmRt/1o6Ki0LZtW+zYsUP9PQwbNgyenp5a17pXVIKsPBluF8iRllOMq9mFuF0gR25xKYpKynCnUI7bBXLIy8w33w1rLByYu4sY5+cOsti1aio8PFzr9W+//Ya5c+fi6tWrAID8/HzcuXNH57HBwcHq5x4eHigoKNB7HX1lMzIytGKoHI8u586d01uu8nbN12lpaQgMDFQnHQAQFRWFtLQ0k65f1f1qayvMddCtWzcsX74czZs3R3p6Oj744AP06tULSUlJyMjIgKurK3x9fbWOCQ4ORkZGht5zzpo1CzNnzlS/zsvLY3Jxn6U+1/XpM71y5UqMHTsWANC0aVM8+OCDWLVqFd5++22kpqaiW7duVY5JTU1FVFRUjZZNCAkJ0aqNvHXrFqZNm4b4+HgUFxejpKQE48aNU19H1/UB4Mknn8SaNWswatQorFmzBu+//36VMspKLVoKHU1cPu4ucKvFz7syJhYOTCQS1bgq05I0/yuXy+UYP348Nm3ahIEDB0IsFiM0NLTG7b3GCAkJwc2bN9Wva9K3QlPlWgbN12FhYcjOzlZXXwLAjRs30LZtW73HG0X9/lg+sRg8eLD6ebt27dCtWzdERUVh/fr1WgmUKSQSic5qcrKPz7U9faavXr2K+Ph4XLhwAUuWLAGgSmTLE4uIiAhcu3atynERERG4fv06BEGo8pn19PREcXGx+nXlzsqVy7/77rvw8/NTN3PMmjUL6enp6uvouj6gqrF58803kZCQgGvXrmHQoJolnMFSN7MmFmwKIZsil8tRUlKCoKAgAMCCBQuQnZ1dp9d8/PHHsXHjRpw6dQoymQyffPJJnV0rIiICHTt2xJw5c1BSUoKDBw/i999/x+jRo2t5ZuvVWFTm6+uLZs2a4fLlywgJCUFJSUmVXvaZmZk6+2SQ47H1z/TKlSvRtWtXXLhwAYmJiUhMTERCQgIuXbqEU6dOYdKkSfjhhx8QHx8PQRCQnJyM9PR0dO3aFd7e3vjwww8hk8mQl5eHhIQEAED79u2xfft25OXl4erVq1i6dKnBePPz8+Ht7Q0vLy8kJSVh5cqV6n36rg8Afn5+6Nu3LyZNmoQxY8boXBOo7tI3/ZhYkE2RSqX44osvMGjQIISEhODOnTvqTo11pX379vj0008xfPhwNGzYUN1Oaug/5ubNm8PLy0v9+PHHH42+3tq1a3HmzBkEBQXhhRdewIoVK9C0adPafRNWrLGorKCgAFeuXEFoaCg6deoEFxcX7NmzR70/OTkZN27cQExMjBWjJEux9c/0qlWrMHXqVISEhKgfLVu2xKhRo7By5Ur06NEDCxYswDPPPAOpVIoxY8YgLy8Pzs7O2LZtG/7++2+EhoaiefPm6n5D//d//4fGjRsjPDwc48ePx/jx4w3GO3v2bOzbtw9SqRSvvPIKYmNj1fv0Xb/ck08+iXPnzmHChAnmeOvMQiTUZX2UDnl5efDx8UFubi6kUqklL10vyGQypKSkIDo6Wl3VTqZJTk5Gu3btIJPJ7Gd9huJ7wL1rgKsXEKBKUvT9Lpj7M/j6669j+PDh6r4ic+bMQWJiIs6fP4/AwEBMnToVO3bswPLlyyGVSvHyyy8DAP7++2+jr1Gf7xv8TNeeXX6mjXTy5EnExsbi2rVrOr+3jDwZsvJkBs/RLNhbqymktvcO1lgQAdi2bRtkMhlyc3Mxa9YsPPbYY/Z1A7Ls/wdaUlNTMX78eDRv3hxPPPEE/P39cfToUQQGBgIAvvnmGwwbNgyxsbF4+OGHERISwpkNqc7Z/WfaCAqFAt999x2efvpp/d+bFW4Ntt0DiMhC1q1bh4kTJ8LJyQm9e/fG4sWLrR2SiazXx2Lt2rUG97u5uWHRokVYtGiRhSIicoTPtGF3795FZGQkWrZsiQULFtTqXOa+azCxIALwv//9z9ohVBAE0xMEG+pjQWQLbOozXQcaNGhgcCiuNbEphMiWKMuAzCTg7jUTD7SdUSFEVL8xsSCyJbJcVXIhu2facep2VCYWRGRdTCyIbIlI4yOZdQGQV1PVqVQASiVYY0FEtoKJBZEtEWnMflcmA+5e0V9WqQAy/gGyzrOPBRHZDCYWRLakco2DoNRfNu+W6quyFKyxICJbwcSCbN7kyZPx0UcfAVAtqdy+fXu9Zfv06aM1Ha4pBg8ejHXr1tXo2LpTKVFQKoDS+5PdFGks4qROQJhYkO2r359px8fEgixi4MCBmDdvXpXts2fPxqhRo4w+T69evQwuo2ys5cuXay3lDQB//PGHeoVDcymfDU9z+m8vLy+cPXtW9wGVJ7oqr4FQKgFFCXD7EpB9ASgprHScQrs8UR2rr5/pcqmpqRCLxVqr8JIKEwuyiCeffBKrV6+usn316tV48sknrRCR5YjFYhQUFGg9NFczLVdWVlZlm0JxvyYi+wKQeQ4ou79iYtFd7YJK5f3jmViQZdTnzzSg+j59fHywdu1aKBQKi11X133CMMtPvcnEwpEJguo/W0s8qplSetSoUbh69arWf+pHjx7FnTt3MHToUPz8889o1qwZvL290a5dO+zfv1/nefbv36+1gNGJEyfQrl07SKVSvPjii1AqK/okHDt2DF26dIFUKkVUVBS+++47AKplkl988UXs378fXl5eaN26NQDtKlelUok5c+YgIiICoaGheOWVVyCXywGo/jPq168fpk6dCqlUilatWuHUqVOm/3ygWj554cKFiI6ORt++ffH+R59g/EuzEPv86/Bq2gN7j5zAuXPn0OuxJ+Hb8mF0enQCjpxIVNdQNOw2FJ8vXo6W3fqjSY8RzCvqA0t9rvmZNvj9r1y5Eh999BFkMpnWInsA8O2336Jp06bw9vZG165dceeOqtkyMTERvXv3hq+vLyIjI7Fhw4YqcQLA+++/j+eee04dW9++ffHCCy/Ax8cHy5Yt0/s+6Lr+4H69kHPvLua+OR1Lvv5UXUYQBAx+qD0STx4z+32DM286stIi4JMwy1zr7TTA1VPvbi8vL4wYMQKrV69WV5+uXLkSY8aMgUQiQUhICPbs2YOwsDD8/PPPGDduHK5fv25whdGSkhKMGjUKb7/9Np577jl8//33+O9//4spU6YAAFxcXPDDDz+gQ4cOOHXqFPr374+ePXviwQcfxPfff4+VK1fir7/+0nnupUuX4tdff0V8fDzc3d3x2GOPYd68eXj//fcBqNqFn3/+eSxcuBBz5szBjBkzcODAgRq9dbt378aZM2fg4uKCzz76AJt37sPWZd9gww+fo0BWhg4Dh2P606Oxd/0P2LRjL4ZPno4rJ/fBz111N9i0Yy/2/7YSUokAZhb1gKU+1/xM6/1M//PPP/j3338xbtw4JCQkYOXKlRg4cCAA1WqpCxYswJYtW9CqVSskJibC1dUVubm5GDhwID755BP89ddfyMnJQWZmZrU/hvLYJk2ahCVLlkAul+PChQt634fK1//r0FG4uLhg6ONj8MGb0zF15lsAgDMJxwEAHTp3MyoGU7DGgizmySefxJo1ayAIAsrKyrB+/Xp1lemQIUMQEREBsViM559/HiKRCJcuXTJ4vvj4eDg7O2Pq1KlwcXHBtGnTEBoaqt7fsWNHdOzYEU5OTujcuTOGDBmCI0eOGBXr2rVr8frrryM8PBz+/v6YPXs21qxZo97fokULjB8/HmKxGBMmTDDYRqxQKODr66v10Kw6feuttyCVSuHu7g4A6N29Iwb2joGTkxPOnL8IpVKBV54dDxcXF4wdMQjNG0Vh55796uNffW4CggP94O7uxj4WZFH19TO9cuVKDBgwAA0aNMC4ceOwefNmFBUVAVDVMLz11lto06YNnJyc0LFjR3h7e2Pbtm1o2rQpnnvuObi4uCAwMBBt2rQxKvbGjRtj8uTJcHJygru7u8H3ofL123V4EJ5e3ujY7SGUlshx4azq+/pjy0YMfizW0GVrjDUWjszFQ/Vfh6WuVY2BAweiuLgYR44cQV5eHjw8PNCrVy8AwG+//Ya5c+fi6tWrAID8/Hx19aE+6enpCA8PV78WiURar8+dO4fp06cjMTERJSUlkMlkaNGihVHfTlpaGiIjI9Wvy5cELxccHKx+7uHhYXDOfrFYjJycHL37NWMGBISHVpw77dYtRAQ30CofFR6KtIysiuNDgzgqpD6x1Oean2md51EqlVizZo16VEu/fv3g4eGB3377DRMmTEBqaiqio6OrHKdvuzG07xGG3wd91xGJRBg8YjT+2PIrmrVqg93bt+CH1ZtrFE91mFg4MpHIYFWmpTk7O2Ps2LFYvXo1cnJyMGHCBIhEIsjlcowfPx6bNm3CwIEDIRaLERoaCqGaNt7Q0FCkpqZqbdN8PW3aNPTq1Qtbt26Fu7s7xo8frz5ndcsnh4WF4caNG+rXN27cQFhY3VQ/V45F83VYSCBupmlXl964lYERQwK1yys5KqTesKHPdX38TO/fvx+pqal47bXX8OabbwIAcnJysHLlSkyYMAERERG4du1aleMiIiLw+++/6zynp6cniouL1a8rN5FU/t4MvQ/6rg8AQ0c9gZf+bzS69ewD/4BANG3Rythv2yRsCiGLevLJJ7F+/Xps2bJFXWUql8tRUlKCoKAgAMCCBQuQnZ1d7bliYmJQWlqKH3/8EaWlpVi0aBHS09PV+/Pz8+Hr6ws3NzccOnQI27dvV+8LCgpCamqq3h7WY8eOxVdffYVbt27h7t27+PDDDzFu3LjafOs10u1B1eiRhcvWoqysDBt+340Ll1PwaN+e2gVZY0FWUt8+0ytXrkRsbCySkpKQmJiIxMRE7Nq1C7t370ZWVhYmT56Mzz77DOfPn4cgCDh16hTy8/MxdOhQXLx4EcuWLUNpaSmys7ORlJQEAGjfvj02bdoEuVyOxMRE/PrrrwZjMPQ+VL7+mcTTKCzIBwA0btYCvg388dWH72LwyNEmf+/GYmJBFtW1a1f4+/ujefPmaNVKlS1LpVJ88cUXGDRoEEJCQnDnzh2tXuL6uLq6YuPGjfjuu+/g7++Pf/75Bw899JB6/2effYZFixZBKpVi/vz5eOyxx9T7+vXrh4YNGyIwMBDt2rWrcu5nn30Wjz/+OLp27YpWrVqhffv2mDVrVo2+Z4VCUWUei507d1YUKMgGSor0fI8u2LrsG6z5bRf82/TDvIXLsHXZN/Dz8dYuyHksyErq02daJpNh48aNmDZtGkJCQtSPPn36oFOnTli7di0mTJiAuLg4DBs2DFKpFC+99BJKS0vh4+ODnTt3YunSpQgICEDnzp2RnJwMAJgxYwbkcjkCAgLwn//8p9qEx9D7UPn6b78+HWWlFcnW0MefwNVLyRg8oqJ/hbnvGiKhuropM8vLy4OPjw9yc3MhlUoteel6QSaTISUlBdHR0XBzc7N2OFSdgqyKqbnDHgQyzwMKefXHObncn8q7Ep8IwDMAgP7fBXv8DNpjzObCzzTVRnpuMbLzK+4pu7dvwaql32P5pj/U21qEeMPVuWKdotreO9jHgsgayuTA3auqhcbKKcuMSyoA6J30RsRKSCLSrUQux/r//YzHx/1fnV6HdyEia6icVACAwoQZ9fQtTsamECLS4d9zZ9GrbSO4uLhgyONj6vRarLEgsrTS4qpJBaCqsTCW3lVPmVgQUVUtWrfFsYu3LHIt1lg4KAt3nSFTZF/Uvb3AuFn4DNJoCuHvgGPhz5Mspba/a0wsHIxYrOqAU1JSYuVI6imlAshOBvLSDRXSvVmeV/vrazSFlP8OlP9OkH1ycXEBAPXMjkR1rfx3rfx3z1RsCnEwzs7O8PDwQHZ2NlxcXODkxNzRooruAcWFqocCgLtf1TJldfifp7wEEGRQKpXIzs6Gh4cHnJ35MbdnYrEYvr6+yMpSzbbq4eFR7WRQROVKS+QQygz/oymTyaB0FkMQBBQVFSErKwu+vr41/qeEdxwHIxKJEBoaipSUFFy/ft3a4dQ/JQUaS5pnA76RVcvkVD9RUI3ligFnVwCAk5MTIiMj+UfIAYSEhACAOrkgMlZucSnyZYb7bzkVSuCs8U+or6+v+neuJphYOCBXV1c0bdqUzSHW8O8O4MjsitfTTlYts7AOe2SPXQ0EqtYJcHV1ZY2Vgyj/hyEoKAilpTrmLyHS44cDV7D+pOG1ZVY91w0hPqpFEF1cXGrdfMrEwkE5OTlxMh1rcHUGCm5WvNb1M9Dcb27u7rqvSQ5BLBazzwyZpEjhhFv5CoNlXCRuZv17wX9niMxJ7Gr5az77l8b1a9bZiojIXJhYEFXHlKFXgo7/DHJuAqd+ARR1VIUt8ap4LpbUzTWIyC5ZY5AyEwsiQ5L/AD6LApJ3Vl8WqDrJ1fpJwPw2wNaXgSMLzB8foF1L4szEgohMY+7u3UwsiAxZMw6Q5QJrxhouV1oM/PoscHaD9vbzv1U8P2t4KeQa01wfhE0hRGRl7LxJZA7HfwSSqkkcZDnmv+7gL7TnymBTCBFpsMaMrUwsiMyh0Ii5KfLTAXl+7a7jFQIUZKieBzQHuk1RPf+/31S1Fc5W6DxKRHbN3FPdMLEgMgdjlyv/3+Omn7vbi6pFx3q/CTi7AfMeuL9D4z+Rxn1NPy8RUR1gHwtyTKUyYNlQYP9nhsvdSgD2fwqUyWt5QSNT/tQTpp+6YU9gyBeAZwDg4m768URUb1lj7TrWWJBjOrsBuH5Y9ejzpv5yP/VTfRW7Ar1m1vx61dUlOrsDZcU1O3dgy4rnTpwciYjMS2TmcSGssSDHpDCxBiLrfO2uZ6gpJKA5ENRS//7qNIjWvV1peDY9IiJrYGJBDsrUDLyWGXvlxCL64YrnEV1NXxLdzQfo9x4wZrn+Wgpdk3EREVkZm0LIMRnTzTlhhWnlDV6vUmIh0kwGatDIOfxboPVIw2WUStPPS0T1ijF3H3OPCmGNBTkmY0Zp/P6K5gHmvZ7mDJyCoBrVYayeM6tPKgDWWBCRTWJiQQ7KxEShtil75a7Xmv0fBEF3f4iuU3SfS3PCK0PYx4KIbBATC3JMJicKtUwsFCXar7VqE/TUWLQYVvH8kQ9VX8WuQKdJxl3TlFoQIqqXONyUyGw0EgVBMH8jYrnCO6q5JSonFpWbQiovTgYAUQ8BoR2AwOZAj1dUD1OwKYSIzMDcd0cmFuQYsi6oViLt8qxqFdF71yv2KcuqX5zL2JkzAeDSX8C+j4FBHwO/jAC8goHmQ7TLaCUSeppCxC7ACweMv25lupIVIiIrY2JB9k9RCizurnq+54Oq+41KLCq9LsgGdr2tu+yqWNXXZYNVX3NvVqzfob6mRjOFoKyb2gVr1HESkV0RajIqrZaYWJD9+66T4f2m/mdfUgh82cS0Y9JO679mRDfg0m7TzmcMdt4kInPgcFOiSnKuG96vKDXiJBqfrD/f010k+Q8DMdzQfq0sA6adBIbNBzo9DThVyuFbDjcipmqwjwUR2SCTEguFQoH33nsP0dHRcHd3R+PGjfHhhx9aZb13IgDGNQdU/s++VAb8MlJ72+n/ARlJqucXftd9njXjgK9bGxeXsgwIaAp0fhoQOwNj/we4NwBG/QS8dAwYvcy48xi8BhMLIjLM5keFfPbZZ1iyZAlWrFiB1q1b4+TJk3j66afh4+ODV14xsUc7kTkY88e1clPImdXA1X1Vyy0bDMy6Cchy9Z8rL7VmcUU9BPznqnlHp7DGgojMwKqLkP39998YMWIEhg4dioYNG2L06NEYOHAgjh8/btagiKpITQD+OwC4cUx7e+VhnrooKzWFyAt0lytfz8PUBcx00fVH39xDXjmPBRHZIJMSi4ceegh79uzBxYsXAQBnzpzB4cOHMXjwYL3HyOVy5OXlaT2ITLbsUSD1BLBimPb2ykmDLpVrLMzxB17sqnt7y8dUX2P/W/tr6DP0K9XXMcvr7hpERDVkUlPIW2+9hby8PLRo0QJisRgKhQIff/wxJk6cqPeYefPm4YMPdAwBJDJFec1E5RoKYzpm1kVfBGe3qrG4eqn6UigV+lckNYcuzwEdJqom5iIiqiWrLkK2fv16rFq1CqtXr8apU6ewYsUKfPnll1ixYoXeY2bNmoXc3Fz14+bNm7UOmkjNqMTChOGmyTuNK+fsBrQaob1t8nbV17pMKsoxqSAiG2VSjcUbb7yBt956C+PGjQMAtG3bFtevX8e8efMwaZLu9Q0kEgkkEkntIyUCAKdKE10Z08fCqOGm960Za1w5ZzdgxGLAvwlw6CvVzJthHYy/DhGRgzIpsSgqKoKTk3Ylh1gshlLJTmRkIS4e2q+NqY34oZdqTY7/2wyUyfTPU1GdoNZA1rn7cbgBEi+g/2xV04RnUM3OSURUh4yZDsKqa4UMHz4cH3/8MSIjI9G6dWucPn0aX3/9NZ555hkzh0Wkh4tbxXNBUK3ZYYz0RNV032c3AjWd4tbTv+K5s0YtnDSsZucjInJAJiUW3333Hd577z289NJLyMrKQlhYGF544QXMnj27ruIj0laQCcjyADepai6KpI3GH5udDJTk1/zaHgEVzxVcAIyISBeTOm96e3tj/vz5uH79OoqLi3HlyhV89NFHcHXVM/SOqC7s/Uj1tSDLtONuxFfd5mxCJ0jNIabF90y7dj3y6aefQiQSYfr06eptMpkMcXFx8Pf3h5eXF2JjY5GZmWm9IInqCWPqZ0VmHhbCtULIMhSlqsW9zCHzfj+Hyutv1IS++Sh00Zwzo/JqpgQAOHHiBH744Qe0a9dOa/uMGTPw+++/Y8OGDThw4ADS0tIwatQoK0VJRHWJiQVZxqJuwLxw/bNeGqJvHgpzJBbOJiQWxoxAqccKCgowceJE/PTTT/Dz81Nvz83NxdKlS/H111+jX79+6NSpE5YtW4a///4bR48etWLERFQXmFhQ3dg5C9jzYcXru1dUU1BXXl7cGGWVpti+lQCUlZgnsYjoZnxZRRnQ41XV8+ZDan9tBxMXF4ehQ4diwIABWtsTEhJQWlqqtb1FixaIjIxEfLyO5ilwxl4ic7HGImRMLMj88tKAo4uBQ18CpcWmz3yZfgb4rnPFKqOV1+4oKwZ2vW1cYuFdzYiNBtHA83uBdmOB8C7A8G+BsI66y0q8gX7vAU/8Dxi5uPpr1yNr167FqVOnMG/evCr7MjIy4OrqCl9fX63twcHByMjQ3aQ0b948+Pj4qB8RERF1ETYRwfzDTZlYkPlpJhJlcu0aB2M6CW14GrhzCVj35P1z6GiCOPGTcbFU19TR9gnggU7AqB+B5/4COk0Cnqk0+2azwaqkY8D7gNgFaPUY4O6n83T10c2bN/Hqq69i1apVcHNzq/4AI3DGXiL7ZYa6ZKrX5AXA8R9Vi2+JREDRHUD6QMV+RYn2Kpz66uU019coqdQPo0ym5xgjhnyKDEyv3WcWENqu6vbKHTo7TFAlE6RTQkICsrKy0LFjRU2PQqHAwYMHsXDhQuzatQslJSXIycnRqrXIzMxESEiIznNyxl4i8xBqOm9PLbDGgmpn70eqiacWdQG+6wgsfQS4e7Vif+UaC13JQNImYF5ExTodmk0c8YuAP9/Vfe11+he/UzO0bkdoe93bRSLg/36reO3uW/116rH+/fvj7NmzSExMVD86d+6MiRMnqp+7uLhgz5496mOSk5Nx48YNxMTEWDFyIgLMvwgZayyodm7e79WvWSuRdb7ieZkccCrTfl3Zr0+rvq4ZC7yfq51Y7Hq7dvE5V6qaD3uwogOp5oRXlWku8qVZA0NVeHt7o02bNlrbPD094e/vr97+7LPPYubMmWjQoAGkUilefvllxMTEoHv37tYImYjqEBMLqh1dHSg1F/0qzAJEGhVjms0agqB7kitzjPYo16S/an2RgCZA00FASFvg2w6qfR4N9B+nOSyWiUWtffPNN3ByckJsbCzkcjkGDRqExYvZAZaorlljVAgTC6qdyquNAtrNHcsGa+/TrLH4633gyHzt/YpS8y47LnICnt1V8VozYfDwr1q+nOaaJC7m6ZBYn+zfv1/rtZubGxYtWoRFixZZJyAi0ktk5nEhTCyodnQlAX/N0V++TAacXgns/wzIvVF1/5fNAFmO2cJDTqVrSLyAYfNVcRvqOxHVA+j9pqqGg4iIjMbEgrTlpQE3j6lGeVROGpI2AT7hQETXim1iHTUWhpTJgT/e0L+/+K5p56us09OAmw9w6hfVuXRNgNX56erPIxIBfWvZv4OIqB5iYkHaFseoagwGfwF0m1KxPeNsRSfL93MrtpvaH6LyZFfmNny+6muvmcCFbUAbrkdBRPWXUV0szDwqhMNNSVt5M8SlXdrb71zRXV5XHwtD9M1JYaxOk40r5+YDPDhRe3QHERHVOSYWpPLvdlWtRLmiSk0SmsNJAdW02/9sML2jpa7hpsZqOggYvgCYsr/m5yAiojrFphACruwF1k7Q3pZ2CijIBrwCVa8rJxY/PKz66hlk2rVKi2sWI1CRxIQ9CAS3BTLPGi5PRFTPGTPc1NwTZLHGgoDj/9W9/creiuf6puUu1DEPhSHxC00rr0Xjt79J/1qch4iI6goTi/pOqQSSt+vZpzEfhWZiYepqpeaimVb3ftM6MRARkUFMLOo7hY6VQ8sJGgmEVmJhxOJfNTVujf59momFq0fdxUBE5DCqbwvhsulUO6UyoDhH9VxRCmybrr+sUl9iUVq1rDkEtQaaPmKgQDW//g+9YtZwiIjIdOy8Wd8s7KKa8fLNa0DWv8AZAzUEmUmqKbAlXtqJhaKOEovxa7Qn3JJIAXlexWt9PYwkPsDTO4CgVnUTFxERGY01FvVN+TTaKQeB0kLDZU/8F9j4rOq5Zu1FSTXH1VT5omA9XlWN+ph5ARg0T6OAnsRCUAAhbQAn/joTEWmyxiJkvBPXB2fWAusnaScE658Cygz0ryh3cSdw7xpwblPFtrpKLCTeqq+PzAWmHlbVlMS8VLFfVOnXtfH9kSHGTppFRERViMw83pRNIfXB5hdUX/0aam831HFT04L22q+rq+kwRUQ31dokD3SqvmzlX/4nVgDX/wYa9TFfPEREVCtMLOqT1JPar41NLCozZ41Fq5GqoaNhD1ZftvKCYhJvoNkg88VCRORgrNEUwsSiPrlbab2PmiYIJUW1j0VTdZNdvXQMuHZItXIpERGZlbmHmzKxqE/y07Vf1zSx2Pth7WMBAGk40H5c9eWCWqgeRERk85hY1Gc1TSwyk8xz/RlJ5p+knoiI1ATjFk43K44Kqc9KCnRvD+9at9d19QZG/ZdJBRGRDTD3rZg1Fo4s+yLw+6v69+ursWg5HEg9XvPrvpsN5N4EfuwLyHMBN1+g6xTANxKQhgKN+nHOCSIiB8XEwlEoFUDKAcA3Ctg5C2g/Ftj6iv5aCUB/YhHcuuZxBLUCnF0B/8bAK6eA3FQgrEPNz0dERHaFiYWjOLMG2BJX8frSruqP0Zd0uPsBb90ANk/Vv/JpZT2mq2bqHLO8YptngOpBRERWYcxwU5GZx4UwsbBnijLg0JdAVA8gcbXpx+ursRAEwM0H8PAzcLBItbbH3wuBJv2AXq8B/d4DxPyVIiKqz/hXwJ4l/Qrsn1d9OX30JRbla3Y4u+veH9YRGL8W8A4Gmg+u2M6kgoio3uNfAnuWe9O08i2GAYHNgX82qBYjq5xYTNoGFGYBDaJVr130JBYSL1VSQURENs2YwaYcFUIVXL2qbhNLgFE/ABsmV903bpXq67nNqq9Z57T3R/fSfq0vsdBXk0FERPUex/zZM1fPqtt6vAI0GWD4OLGrced3dtO9vccrxh1PRET1Dmss7Jmu7r5lMkAkruZAHfVez+6uui2kbdVtr11kMwgRkZ2wxiJkrLGwZ7pWJw1qBTjpyBef3FjxXJarvW/8OiBCx2ybTR/Rfv3oZ0wqiIjIINZY2LMymfbrgR8B7cbqLusdVvG8cmLh7lv9tQKaA91fNCk8IiKqf1hjYc8qJxYPvQw4iQGRjh+rm0/F89JKo0HcfPVfI6CZ6mt1/TaIiMjmWGMRMtZY2LMyue7tusYOuUn1n8dQjcWkbUDyDqDtGJNCIyIi+2Du4aassbBnlWss9BmxGJB4V7xu+4T2fkM1Ft7BQOenVXNXEBERVYM1FvasTEfnzcrCuwIPTtTeNuwbVbKQckjVROKiZ1gpERHZNyuMCmFiYc80ayye32f8cRIvIOoh1YOIiOo1LkJGQNFdYOOzwJW9qtePfAg80NG6MREREYGJhe0qlQF3LgEQAWIX1Rof5VIOVCQVAOAssXh4REREujCxsFVbXgKSNCa1eu9Oxeqh8nztsvrW9CAionqNi5BRBc2kAgBKClQJxS+PAXevau9z8bBcXERERAZwuKm9KCkAjiyomlQAulc5JSIisgImFrao8pTbgKq2Qt+qpK6ssSAioqoEI1YhM3NLCBMLm6FUqEZ7AEBBVtX98nzAK1D3sS46lk8nIiKyAiYWtuKXEcDn0cC/24GFnavul+erRorowhoLIiKyEey8aS37PgEykoCQNsC1I8D1w6rtayfoLi/PB0qLdO9zNVRjYYVp14iIyCYYNyqEE2Q5hgOfqb4mbzeuvDxf/9ogbAohIiIbwaYQW+MVont7emJFjUWHSmt/sCmEiIhsBBMLW6MvSbi4q6KPRXAbIKJ7xT5nLiJGRERVGTEohKNCHJ6uoaYAkHsTyM9QPXdxB9qOVj2P6mH+adOIiIhqiH0sbI2+xAKo6ODp4g60fhzwjQQa9bFIWERERMZgYmENSqWBfWXVH+/irlp4rNkg88VEREQOxxrjAtkUYg2ConbHBzSvvgwREZERzN2azsTCUv5eqFrrAzCuVkIf6QNAUAvzxERERGRmbAqxBHk+8Oc7qucdntS9kJixGjQyrpyrl2rhskZ9a34tIiIiE5lcY3Hr1i08+eST8Pf3h7u7O9q2bYuTJ0/WRWyOQ1Fa8fyPN4ClA2p+Licjc8GX4oFh84GHX6/5tYiIyK4ZtQiZNWfevHfvHnr06IG+ffvijz/+QGBgIC5dugQ/Pz+zBuVwlBp9KpI21u5cuhYo08U3Euj8dO2uRUREZCKTEovPPvsMERERWLZsmXpbdHS02YNyOEW3zXcufeuFEBER2QCTmkK2bt2Kzp07Y8yYMQgKCsKDDz6In376yeAxcrkceXl5Wo965fZlYHH36ssZS5ZjvnMRmcGSJUvQrl07SKVSSKVSxMTE4I8//lDvl8lkiIuLg7+/P7y8vBAbG4vMzEwrRkxUf9j8cNOrV69iyZIlaNq0KXbt2oWpU6filVdewYoVK/QeM2/ePPj4+KgfERERtQ7arpzS/97UyKBPzHs+oloKDw/Hp59+ioSEBJw8eRL9+vXDiBEjcO7cOQDAjBkz8Pvvv2PDhg04cOAA0tLSMGrUKCtHTUR1RSQY07PjPldXV3Tu3Bl///23etsrr7yCEydOID4+Xucxcrkccrlc/TovLw8RERHIzc2FVCqtReh24s/3gL+/Ne2YRn2AkLbA399V3fe+gZk5iYyQl5cHHx+fOv0MNmjQAF988QVGjx6NwMBArF69GqNHq6ah//fff9GyZUvEx8eje3fjavMsETORI4pbfQrb/0k3WObap0ONOpexn0OTaixCQ0PRqlUrrW0tW7bEjRs39B4jkUjUVaTlj/qlBhVR/k2BgR8BD3RWvfbwN29IRHVEoVBg7dq1KCwsRExMDBISElBaWooBAypGQrVo0QKRkZF6/xkB2IRKZDZWaAsxKbHo0aMHkpOTtbZdvHgRUVFRZg3KoRhfIVTB5f5qpc/+Ccy6BTSpxfBUIgs4e/YsvLy8IJFI8OKLL2Lz5s1o1aoVMjIy4OrqCl9fX63ywcHByMjI0Hu+et+ESmTHTEosZsyYgaNHj+KTTz7B5cuXsXr1avz444+Ii4urq/jqJ5f7S6c7iQGJF/Dop0D3l4AXD1s3LiI9mjdvjsTERBw7dgxTp07FpEmTcP78+Rqfb9asWcjNzVU/bt68acZoiagumTTctEuXLti8eTNmzZqFuXPnIjo6GvPnz8fEiRPrKr76ycVd+7VHA+DRedaJhcgIrq6uaNKkCQCgU6dOOHHiBBYsWICxY8eipKQEOTk5WrUWmZmZCAkJ0Xs+iUQCiURS12ETOTzBCm0hJk/pPWzYMAwbNqwuYnEMggDcPAYEtQLcatifpLzGgshOKZVKyOVydOrUCS4uLtizZw9iY2MBAMnJybhx4wZiYmKsHCUR1QWuFWJuZ9YCv70IhD0IPL8PiF9ouHz/2cCeudrbKtdYENmwWbNmYfDgwYiMjER+fj5Wr16N/fv3Y9euXfDx8cGzzz6LmTNnokGDBpBKpXj55ZcRExNj9IgQIrIvTCzMJeUg4BcNJK5SvU47DWQnGz7GvykQqGOlUtZYkB3JysrCU089hfT0dPj4+KBdu3bYtWsXHnnkEQDAN998AycnJ8TGxkIul2PQoEFYvHixlaMmqh9qMn6gtphYmEP6GWDF8KrbFSWGj3NyBqRhVbc7u5knLiILWLp0qcH9bm5uWLRoERYtWmShiIjImkxe3ZR0yPpX93ZBoXt7OScxIH2g6nY2hRARkZ1iYqGLokz3dqUCKLpbdbtYT8VPabHh6ziJAc/AqtvZFEJERHaKiUVl/+4APosCDn9Tdd+a8cDn0cDdq9rbSwp1n2vZYMPXcnIGRCJg4kbAR2MCINZYEBGRGVijjwUTi8rWjgdKCnQnFpd2qb6eXqW9XV5Qs2vl31/hsekAYMqBiu3OHL9PRET2iZ039ZFVWuxLs3lEng+876N6/tQW/TUW1clLrXjOWgoiInIArLHQVJyj/XrbTCDz/rTEGWcqth//oeL5+qeAknzVc1cv067XfkLFc83EwivYtPMQERHpYBczbzq03ErrEZxcCiSuBv5zFfipn+5jZLkVNRbeIcCdy8Zd68UjgG9kxWuRCJj6N1AqU03hTUREZIdYY6EpR8dCR2XFVWfGrKw8sfAIqNgWWc10xSFtqk75HdwaCO9UfZxEREQ2iomFpuQdqq/NhwAP/l/F9lMrDB93Zo3qq0SjKUTX/BTlhn5ds/iIiIhMwFEh1lRSCJz+n+q5RwNgxEJgxjnV69Ii3ce4+2m/7j8b8PAHWgwD+r4NuHoDgS21yzy/F+jyrHljJyIishHsY5GRBPz+CuDXsGJbkwGqr7pqHYZ8Cex4/X65R4Cz61XP248HQtsDMy8AYldVn4k3r6kmzyofQTJuDfAAmzqIiMhxMbH44z/ArQTVAwACmgOtRqqei0RAoz7A1f0V5TVHfrhorOnx+Peqr5pzUJTPyPnsX0B6ItC8mgmziIiIzMgKLSH1vCnk4i7g+hHtbUO/VCUU5frP1t7fdCAglgAR3as2hegT0QXo+rz2eYmIiBxQ/a6xWP1ExfPeb6n++HsGaJd5oJNqtdEymeq1pz/wxmXVeh6yHCDlkKoZhIiIiOpxjUVuqvbrHq9WTSrKDfhA9bXT06qvblJVM4dnADBlH9BtSt3FSUREVEPWGBVSf2ssNmkkA4/MBVwNrCja7QUg+mEgoFndx0VERGTH6mdikZem6lshcgJeTgAaNDJcXiQCgltZJjYiIiI7Vv+aQspKgG9aq56HtK0+qSAiIrJblm8LqX+JRfIOQFCqnjd71LqxEBEROZj6lViUyoBDX1a87vqC9WIhIiJyQPWjj0XxPeDoEuDAZxXbXj6lGjpKREREZlM/EovtrwNJv1a8fuhlwL+x9eIhIiKyAC5CVhcyz2snFa1GAo98aLVwiIiIHJnj11hcO6T6Kn0AaDMKePgNTq1NRERURxw/sbi8R/W1w0Sg3zvWjYWIiMiCuAiZuf29ELi0S/W8UW/rxkJERFQPOG5iUZwD7Lm/xseAD4CGPa0aDhERUX3guInF+S2AogQIagX0nG7taIiIiCxOsMKwEMdMLOQFwJH5qudtR1s1FCIiovrEMROLo0uAu1cB71DgwaesHQ0REVG94ZiJxbnNqq/93gO8Aq0bCxERkZVwVIg53DwOZJ0DnFyA5oOtHQ0REVG94niJxZEFqq/txwIeDawbCxERUT3jWInFnSvAv9sBiICHXrV2NERERFbFtUJq69/tAASgUR8gsJm1oyEiIqp3HCuxuHh/ls3mQ6wbBxERUT3lOIlF8T3gRrzqebNB1o2FiIionnKcxOLyHkBQqGba9IuydjRERERWx+GmtXFxp+orayuIiIisxjESC0UZcGm36nmzR60bCxERUT3mGInFzWOALAdw9wPCu1g7GiIiIpvARchqqrwZpOlAwEls3ViIiIhsjEhkuWs5RmKRclD1tckj1o2DiIjIBlkwr3CAxEJeAGScVT2PirFuLERERPWc/ScWtxJUw0yl4YBPuLWjISIisjkiC7aF2H9iUT4pVkRX68ZBRERko9gUYop/t6m+Nu5n3TiIiIhsDBchM9XdFFX/CpGY64MQERHpwVEhxrqyR/U16iHA09+6sRAREdkokQUbQ+w7sbh5XPU1qod14yAiIrJBghVWC7HfxEIQgGtHVM8ju1k3FiIiIlvGphAj3L0K5KUCYlcgoru1oyEiIrJZHBVijKv7VV/DuwKuHlYNhYiIiFTsN7FIOaD62qi3deMgIiKyURxuaiylEkg5pHoezcSCiIjIEA43rU7mWaD4LuDqBTzQ0drREBER2TQON61Oef+Khj0BsYtVQyGq7+bNm4cuXbrA29sbQUFBGDlyJJKTk7XKyGQyxMXFwd/fH15eXoiNjUVmZqaVIiaqP9gUYqzyxKJRH2tGQUQADhw4gLi4OBw9ehS7d+9GaWkpBg4ciMLCQnWZGTNm4Pfff8eGDRtw4MABpKWlYdSoUVaMmqh+sWRTiLPlLmUmZXLg+v2Fx9i/gsjqdu7cqfV6+fLlCAoKQkJCAh5++GHk5uZi6dKlWL16Nfr1U63ps2zZMrRs2RJHjx5F9+4cLk5U1+r9cFNZqQKXswqQnJFfdee1Q0BZMeAVDAS1tHxwRGRQbm4uAKBBgwYAgISEBJSWlmLAgAHqMi1atEBkZCTi4+N1nkMulyMvL0/rQUSms7uZNz/99FOIRCJMnz7dTOGonEvLw4CvD+C5X07o2LlZ9bXFMMvW7RBRtZRKJaZPn44ePXqgTZs2AICMjAy4urrC19dXq2xwcDAyMjJ0nmfevHnw8fFRPyIiIuo6dCKHJrLg38saJxYnTpzADz/8gHbt2pkzHgCAt5uqhaZAVqa9Q6kAku9Xu7YaYfbrElHtxMXFISkpCWvXrq3VeWbNmoXc3Fz14+bNm2aKkKh+svmmkIKCAkycOBE//fQT/Pz8DJatSZWml8QZYijwcMkhCLm3KnYkrgaKbgNuPqoVTYnIZkybNg3btm3Dvn37EB4ert4eEhKCkpIS5OTkaJXPzMxESEiIznNJJBJIpVKtBxGZzm5GhcTFxWHo0KFabab61KRK08vNGW86r8UC52+h3DZTtTEvDfjzHdXzViM4zJTIRgiCgGnTpmHz5s3Yu3cvoqOjtfZ36tQJLi4u2LNnj3pbcnIybty4gZiYGEuHS1Q/2fKokLVr1+LUqVM4cUJH/wcdZs2ahZkzZ6pf5+XlVZtceLk6Y4rzdgCA+NJO1UiQDZMBWS7gHQoMmmdq2ERUR+Li4rB69Wps2bIF3t7e6n4TPj4+cHd3h4+PD5599lnMnDkTDRo0gFQqxcsvv4yYmBiOCCGyEEs2hZiUWNy8eROvvvoqdu/eDTc3N6OOkUgkkEgkJgXlBCVKBTFcRArVhv/2BzLOAi6ewFNbAYmXSecjorqzZMkSAECfPn20ti9btgyTJ08GAHzzzTdwcnJCbGws5HI5Bg0ahMWLF1s4UqL6xwotIaYlFgkJCcjKykLHjhXTaCsUChw8eBALFy6EXC6HWCyufVS5NyuSCkCVVADAIx8Agc1qf34iMhvBiEZcNzc3LFq0CIsWLbJARERUmSVHhZiUWPTv3x9nz57V2vb000+jRYsWePPNN82TVADA3RT1U7lHKCROSmDw50DrkeY5PxERUT1iszNvent7q8eml/P09IS/v3+V7bXSuC/+L2AdUm+l4s3RQ/Fo6xDOWUFERGQHbHZKb1cvf6QICtwrKmVSQUREVBNW6GRR68Ri//79ZgijKj9PVwDA3cKSOjk/ERFRfWHzE2RZQoP7icU9JhZERES1YhdTetc1P4/7iUVRqZUjISIisk92twhZXfK/X2ORXSC3ciRERET2jU0hACL9PQAA124XWjkSIiIi+2bJMRA2m1g0DlTNrnnzXhFkpYpqShMREVFldrMImSUEeLlC6uYMQQCu3WGtBRERUc2x8yZEIhEaB6lqLa5mM7EgIiKqKTaF3FfeHHIlq8DKkRAREdkfayxCZtOJRVQDVQfOm/eKrBwJERGR/eKokPvCfN0BALdyiq0cCRERkf1iU8h9D/jdTyzuMbEgIiIylWCFYSG2nVjcr7FIy5FBqbRGSxEREZH9E3FUiEqIjxucRECJQonbhZyBk4iIqCbYFHKfi9gJwVI3AGwOISIiMhVHhehQ3hxy4y5HhhAREdk6m08sWodJAQCnb+RYNxAiIiI7xeGmGjpG+QEAzqTmWDcQIiIiOyWyYCcLm08sWoaqaiwuZxZYZdgMERGRveIiZDo09PeE2EmEfHkZMvJk1g6HiIiIDLD5xMLV2Uk9tXcKFyMjIiIyGYebVtIwwBMAkMLl04mIiIzG4aZ6RPmraiyu3WZiQUREZCrWWFQSXV5jcZtzWRAREZmKU3pXEuWvSiyusymEiIjIeFyETLfo8sTibhEUXIyMiIjIJGwKqeQBP3e4OjuhpEyJ1HtsDiEiIjIFZ96sROwkQqP7/SwuZxVYORoiIiL7wFEhBjQO8gLAxIKIiMhUnNJbh6ZMLIiIiGqETSE6NLmfWFxiYkFERGQUrhViQKv7i5GdT89DSZnSytEQERHZEY4KqSo6wBN+Hi4oKVPiXFqutcMhIiKyG2wK0UEkEqFjpB8A4NSNHOsGQ0RERDrZTWIBAB2jyhOLe1aOhIiIyPYJVhhwaleJxYORvgCAU9eZWBARERmLw031aB/uCycRkJ4rQ3pusbXDISIisgvsY6GHp8QZLUJUo0NOXc+xbjBEREQ2jsNNjdAxyhcA+1kQEREZi4uQGVA+MiSB/SyIiIiMIrJgY4jdJRadoxoAAM6l5aK4RGHlaIiIiGwXm0KMENHAHSFSN5QqBBy8lG3tcIiIiGwem0IMEIlEGPFgGABg9bEbVo6GiIiINNldYgEAjz/4AADgeMpdNocQERHpYYWWEPtMLJoHeyPczx3FpQqsO8FaCyIiIkM4QVY1RCIRnoqJAgAcvHTbytEQERHZNk6QZYSYRgEAgAMXs7naKRERkQ6CFYaF2G1i0eYBKQa0DIZCKeCDreet8uYRERHZA44KMYJIJMLcEa3h5uKE49fu4qdDV60dEhERkU1iYmGkMF93vDO0FQDgs53JOMR5LYiIiKzKrhMLAHiyWyRGPfgAFEoBTy87gZ1J6VXKJN3KxbDvDuHARSYeREREdcnuEwuRSISPH2+LmEb+KFMKiFt9Gkm3tDtzvvC/BCTdysOkn49bKUoiIiLr4VohJnJ3FePHpzqhQ4QvFEoBz644gaWHU7D+xE0olQJu5RRbO0QiIiKrYR+LGvB2c8HSSZ0R2cADmXlyfLjtPP6z8R/8dSHT2qERObSDBw9i+PDhCAsLg0gkwm+//aa1XxAEzJ49G6GhoXB3d8eAAQNw6dIl6wRLVM9wEbJa8veSYPHEjnDSyMxeWJmgfi52EnFYKpGZFRYWon379li0aJHO/Z9//jm+/fZbfP/99zh27Bg8PT0xaNAgyGQyC0dKVH9ZcoIsZwteyyLaPOCDpZO6YOnhFBy+fFsrW1MoBaTcLkSjQC/rBUjkYAYPHozBgwfr3CcIAubPn493330XI0aMAAD88ssvCA4Oxm+//YZx48ZZMlSi+suCbSEOl1gAQN8WQWj9gBRdP95TZV+/rw6gZ5MARDRwh6erM6Y83AhBUjcrREnk+FJSUpCRkYEBAwaot/n4+KBbt26Ij4/Xm1jI5XLI5XL167y8vDqPlcgRCVZYhsyhmkI0BXm74efJnXXuO3z5NtYcv4n/Hk7B/y09DoVSwLm0XOQWlVo4SiLHlpGRAQAIDg7W2h4cHKzep8u8efPg4+OjfkRERNRpnESOjmuFmEm/FsH4YnQ7tAv3wYpnuuosk5yZj8Zv78DQbw9jyLeHcDkrH70+34vVx7hqKpG1zJo1C7m5uerHzZs3rR0SkV3jqBAzGtM5Alun9UTvZoEI8HI1WPZWTjE+/eNf3LxbjLc3n7VQhESOKyQkBACQmak9OiszM1O9TxeJRAKpVKr1ICLTcVRIHXukVXC1ZUoUHDVCZC7R0dEICQnBnj0V/Z3y8vJw7NgxxMTEWDEyovqFo0LqyCePt0UDT1cs2ndFb5mDGtN+K5QCxE6W/HEQ2Z+CggJcvnxZ/TolJQWJiYlo0KABIiMjMX36dHz00Udo2rQpoqOj8d577yEsLAwjR460XtBE9YyIo0LqhkgkwhuDWmBYuzAcu3oH7/9+3mD59Nxi+Hq4wkuiepsEQbDoD4fIHpw8eRJ9+/ZVv545cyYAYNKkSVi+fDn+85//oLCwEFOmTEFOTg569uyJnTt3ws2No7GI6po16uDrVWJRrmWoFC1Dpbhxtxg/H0nRW673F/uhUAqIn9UPf13IwofbzuN/z3RFt0b+FoyWyLb16dPH4MRzIpEIc+fOxdy5cy0YFRFpstlRIfPmzUOXLl3g7e2NoKAgjBw5EsnJyXUVW517d2hLDGsXqne/Qqm6WcbM24v3fktCSZkSY388aqnwiIiIzMJmR4UcOHAAcXFxOHr0KHbv3o3S0lIMHDgQhYWFdRVfnXJyEuGbsR0wvmsEIht4YMaAZtYOiYiIyK6Z1BSyc+dOrdfLly9HUFAQEhIS8PDDD+s8xtZn0HMRO2HeqHYAVH0ovvnrYrXHFJco4O4qrrJdEARkF8gR5M22YyIisj5rrI9Vq+Gmubm5AIAGDRroLWNPM+iJRCLsea13teUOXMzCxoRUdVNJuS92JaPrx3uw4SQn8yEiItshsmAvixonFkqlEtOnT0ePHj3Qpk0bveXsbQa9xoFe+N+zumfpLPfiylN4bcMZ/JGUjtyiUvxw4ArSc4uxeL9qGOsH1Yw2ISIisigL9rGo8aiQuLg4JCUl4fDhwwbLSSQSSCSSml7GKno1DcTVT4ag0ds7DJbb+28WZq4/g5IyJdadqEiYCuRleptLiIiILMUaw01rVGMxbdo0bNu2Dfv27UN4eLi5Y7IJTk4ibInrAQ8DycGmU7dQUqYEAFy9rd2B9ceDV+s0PiIiImPZ7HBTQRAwbdo0bN68GXv37kV0dHRdxWUT2kf44vzcRzGyQxicRMDkhxoafWxaTnHdBUZERGQCSw43NakpJC4uDqtXr8aWLVvg7e2tXvbYx8cH7u7udRKgLZg/7kF8GtsORSUKLP/7mlHHKK2x8gsREZEmW1+EbMmSJcjNzUWfPn0QGhqqfqxbt66u4rMZbi5i+Hm4oHGgp1HllcwriIjIRlhyVIhJNRbWGA9rS0QiETZOfQizt5zD1jNpBsvW9/eKLEepFFCiUMLNhZ2FiUg3m515kwBfD1d8Gtu22nKbTt/C7C1JFoiI6rsJ/z2KFu/txO0CefWFiahesZtRIfWdh6sz5o2qPrn4Jf66BaKh+u7o1bsAgF3nMqwcCRHZKtZY2IFxXSJw7O3++Gik/snBAKDhW9vx/C8nLRQV1WdsfSMifexi5s36TiQSIVjqhie7R+GrMe0Nlt19PhN3WE1NdYx5BRFVZndrhZBKTGP/astcybbPFWCJiMj+sSnEzoT5uuP83EH4+HH9zSJXswuw+3wmFu27jFKFUmufrFQBeZmirsMkIiKqczVeK4S0ebg6I7KBh979x1PuYtPpWwCAByN88VCTAACAQingkW8OQFaqRPxb/eAsZq5HRETmwVEhdq5nkwC8Mag5fp7cGe6V5hQ4eytX/TynuLTieVEJbt4tRna+HLc4DTjVBntvEpENYGJhRiKRCHF9m6Bfi+AqU3pfu1PRxyItpxgJ11VDBOVlFc0iN+8ysaCaY1pBRPqILNjJgk0hdcTdVayVNJQqKm77H22/AAB4pX9TjOwQpt5+/W4heiLAckESEVG9YLOrm5LxRrQPq7bMt3suIS1Hpn6dU1RqoDSRYWwJIaLKrHFfYGJRR94a3BIBXq7VlvvzfMVsicUlHBlCRETmx+GmDsDdVYyT7z6C83MHwUWs/yeqOe33wn2Xcf0O57sgIiLzYlOIA/FwdcZXT3Qwuvzw7w7XXTDk0LiiLhFVJlihWzcTCwt4rH0YUuYNwVMxUdWWzZOVoUBeZoGoiIiovrDkqBAmFhYiEonQNNjbqLL/PXQVgGpGzs2nU3E85a7e/0YvZuZj2z9pZouT7ItSWfF7wfoKItKHTSEOqn+LIKPKrTyq6ncxb8cFzFh3Bk/8EI91J27qLDvwm4OYtvo0Dl+6bbY4yX5Uni+FiEgTR4U4uDBfd/zyTNdqy90uKMGNO0VYq5FMfLX7olYZhVJA4s0c9euktFxQ/aNkXkFERuCoEAf2cLNA/BbXA5tfeshguYHzD2i9zs6XI/Vekfr1gj2XMHLRkTqJkeyHZo0FKy+ISD/OvOnQOkT4AgCGtgvF9n/S0T7cB2dSczG8fRh+P6PqLyErVULirJ33DfzmIB5tE4K84lL8dSFLa9/tfLlFYq9PBEHAP6m5aBLkBU+JbX5UmEwQkSHWuEfY5t2ynlgwtgPeHNQCkf4ekJcp4Cp2UicWQNVfiKISBTaduqXzXP89nII3Hm0OibMYOUUlKJCXIdxPe7VVQRBw9lYuGgV6wauaP5RXsguw4590PN0zutqyjmpLYhqmr0tEu3AfbJ3W09rh6KQQ2HmTiKrHppB6wlnshEh/1R9/ibMYIpEI7wxpqd5folDqO1Snu4UlKClTImbeXvT8bB/uFpZo7d+fnI3HFh7B1JUJOo/PzperRxkMWXAIX+2+iHk7LpgUgyNZf1LVx+WfVNvtv6LdFMLUgoh046iQeuz5hxuhSZBXjY69U1CCZu/+geJS1dTgKbe1Z/H8Jf4aAODQpdv4+XCK1r7Dl26jy8d/4fVfzwCoWHU1/sqdGsXiCKwx4kJpYm9MwbTck4iozjGxsEE//F+nGh2XdEv7P+vK/8FKnMXq53O3ncflrAL162/3XAKAKk0tptaaOBJLj7g4deMe2s/9E6uP3TD6GA43JSJjsCmknmsc6IWh7UJNPi49V6b1+nZBCab8chL9vtyPo1fvVGkaeXvTWfVzfQlE2f3l3m8XyPHz4RTkFGmfo7hEgaw8ma5D7Z7JtQeCgAvpeSitYTL22vozyJeV4e3NZ6svfB8TCyIyhsiCjSFMLGzU/LEd8Mag5iYdk1EpsXhn81n8eT4TV28XYtyPR5F/f6rw9vdHpRy/dhfHU+4CAMqUuv8Ylv+RnPLLSczddh59v9yP305X1Gq8svY0Hvp0L65kF+g83p4Z80dbc0Xa5X9fw+AFh/Da+jM1ul5N/qPQ7LzJJIOIbAETCxvlInZCXN8muPrJEEzsFmnUMRmVag7uVKqhyL1f2zBcozbkiR/i8dr6MyjSs2R7eU3GqRs5AIB7RaWYvi4RSbdyoVAK2H0+E2VKwaTq+8p2n89El4//wqFL2TU+h6Y8WSkWGVgpVlaqwDPLT1TpZ1KZoQqLUoUST3wfj5azd2Lr/ZE8i/dfAQD1a1O5OJn+cdTMJepxqxUR6WGNTt1MLGyck5MIHz/eFpuqmVALAA5cNPyHOe1+jUagt0Rr+8ZTqbiaXfFHeM6WJPXzUoUSp2/cq3KuS1n5SLldoFUOAIpKyjD39/PYn5xV5Rh9nv/lJLLz5fi/pceNPqZMocT+5CzkFpeqtxWXKCAIAhbvu4IvdiVj4DcHdR67+fQt7P03C3O3nTd4DUMfyB8OXMHxa6ranlfWnAYAiGvZiOksNv14JWssiMgIluxjUT8nKLBDHSP9cPb9gfh8ZzIaeLqi7QM+eO6XkzU6V4CXxOD+FfHX1c9LFQLe/73qH+AZ686gb/NA9evzaXlIupWLRfsu44+kDPx8JAXLn+6CPs1V66MIgqBeXU9WqoCTSARX55rntWtP3MS7vyWhfbgPtkzrifUnb+LtTWcxoVskbtxVzVAqL1NqXbdcgcy41WMN1VhsP5tRZZvYqXafXBex6e+HZoym9gkhovqDiQXp5O3mgg9HtgGg+kMtdhJBUYM/JpVrLAxRKAW9f7D2JVfUkJy8fg/DvjustX9rYhr6NA9C0q1cTPjpKBoHeeHbcQ+i1+f7EOXvga/GtK9R/ADwR1I6AOBMai6USgH/+fUfAMAv8dfR9gEfdbnd5zMxsHWI1rHGfsAM1QDISqs2HdWgJUOLS01qLDTePwVrLIioEmvcFZhY2CmRSIQrnwzBn+cyUKJQItzPA2O+/xuliup/jQKrqbGorKZV7AcvZUMQBPx48CryZGU4fSMHvT7fBwC4fqcIo7+Pr3LMyqPX8WT3qCrbk27l4naBXF0DEh3giSOXVXNsrD6u3b/jrMaw2/e3nkO+rAyxncJNjl9XzvPRtvP460JmlRE4AOBU26aQGmQm2k0htbo8ETkwS44KYWJh5zT/G9/3eh+sPnYDKbcL8UdS1ar6clJ3F5OucS4tr0ax3S4owf+OXse/GcYf/+5vSegQ4Ys2GrUOANS1ITun90KLECm83Sq+h62J+jtLpuXK8NqGM+jSsIF6llPNBEChFPQ2Yeiqqfmvjg6f5YfrO49CKcBJhCpNMpXVrI+FxnNmFkSkD+exoJoI9/PAfx5tgdiOhv87r21fgMraR/jqXU9k9pZzuFNQonOfPtkF2guqaXai/Pt+LYXmMM/L2QWo7lvSPKfm33ddTRrljK2pKX8/dXXeLClT4pGvD+Cpn6vvmOpaoz4WbAohIv2scVtgYuGABrQKxqWPB+Pap0Mxa3CLWnWS1OUBX3fMHdFa/XruY63x0f2+HwAQ4OWqVb7ysNfqyCoNfdUcClvRMbNiW4G8rNpmBM0/wJp//o1NLAz1BSmvidCVsJ1JzcHV24U4dOl2tcO+NGssjJ1kS+CoECIygiXXCmFTiIMqH2HwQu/GmNg9Cj8dvIqrtwsxuI2q6eTIW/1wPOUObt0rxr7kbCRc1x5S2rd5oFbnTE1uLk54tE0IZm85BwBoEuSFyAYVK6nmGTnqQp+E6/eQeq8Yw9qHItTHHQXyivPlFJVAEAStGouSMmW1HTLTcooxZMEhhPm6oVu0v3q7rEz/H3DN/iqlCiVEevLw8iaI6vpYlCoEuDrrL+OsUWMhK1UYNUqETSFEZIzqmmLNiYlFPeAlccaMR5ppbXvA1x2PP6hqMnmiSwS2JqbhSnYhbhfI8f2TnSB2EqHhW9t1nu9KdiGCvN2w4cUYiAB4SpzhqdEftKyWMzWV92P48s9kDGgVjO6NKhKB3xLTcOTKHTT0r7wkvOFzLt53BcmZ+TifnofIBp7q7YZqLEo0ko7LWQVoFOips1zZ/T/o1TUxycoUOmuPBEHAM8tPaCVyxaUKrX4k+mjWpDCvIKLKBCuMC2FiQQjydsNzvRpV2T6q4wNVFiXT1KVhA63XiyZ0xIx1ifjqifZwEolw/W4hdiZlID1XhtcHNsPV24X44cBVAMBLfRojI1eGTaf1n19epsT2f9Kx/Z90re3Z+XJk58v1HKVbcma++rnmYm2GEgvN5pZh3x3GyXcH6C1bplBqJRZlCiWcxU5af/hlpQpIdSQLabmyKrVDshLjkjNjm2uIqH5jUwjZhK/GtMdHI9vg9Q1n0DmqAf5ISseJa/ew7OkuOssPbReKQa2Dtar0X+rTRKvMrMEttV63C/fBL/HXcfW27um368I/t3LUz2Wluv+AC4Kg1QQDAEVy/UnIxlOpkGjURiRcv4dujfy1Ehe5nmvpquEpNpDwaMdZ8Zx9LIhIH06QRTZBJBLBw9UZiyeqlnF/pmd0tcc4mziyYXKPaEzuEY31J29i1dHr+Gx0O3y75xLO3MzFrZziGsVdHc1kYtWx6/BwFUMQgFZhUgCqpGLkoiNV5gR5+It9es954to9rT/yhy/fvp9YVFxLX+2IrnVajE0sOKU3ERlijdsCEwuyCU90jsATnSMAAIsndkKpQolb94rh5eaMWZvOYvf5TDzfKxqPPxiOId8e0nmOAS2DcLugBIk3cwAADzcLxMFq1k/ZdOqWurlnxyu9EOXvgedWnMSZ1FyDx1Xm6SpGTnHF6JedSRmI69tEqzlFX+2IriQi6VYuRKhYiVYfzdYPLkJGRPpYsimEw03JJrmIndAwwBMBXhL89FRnnPtgEN4e0hKtwqQ4/GZfNA70REwjf0zt0xgA0NDfA1P7NMELD1f0FXmlXxMcf6c/PFzFCPVxw7op3Q1ec8i3h9B6zi7EX72jtb1yR1FNYT5uAFTrq1zMrFiU7VJWAWKX/K01ekVWprsWovLwWkA1UdiIRUeQVWnFWnmZAr8mpCLj/syfmv0qrLGKIRHZB44KIarEU2MCrnA/D+x5rY/69X8GNdda4Cy2YzhEIqBDhC+cxU7Y93ofuLuK4a1nEq/q+HtJcO1Okc59X45pjwn/PaZz37m0PLy16az6tb6mEEPNHpezCxAkdVO//n7/VXzz10WE+bjh71n9tZIJQx1RiYgshYkF2T3NTNzNRYyvnmivtT9Y4w/zlrgeuJJdgEdaBeOrPy8iSCrBS32aIP7KHYz/6WiVczcO9MQDvu7qeT42Tn0IsUv+BqBqpohp7I+lkzrj2RXVrzQrK1VCXqaAxFmstV1XH4uKY7T3/XleNVV72v0aC82mkAIDnUuJqH7iImREdax9hK+638L7j1XMHhrT2B//vD8QhfIylCkEjPvxKLzdnLHyuW64fqcQ2/5RrdTa+n4HTwBYMrEjRCIR+rcMViccbi5O6N8iGNvPple+NJ6vtMz9ppceQsdIP4M1FpWnQ688CZdmh81Cee0mJiMix8XhpkRWIHVzUc8zceStfurtAV4SHPxPXwR4SeDmIsY7Q1qiTCkgzNddXaZTlB/+92xXNAnyQrC3G74Z2wEnr93V20wCAKMW/422D/igaZCX3jJpOdp9LCo3k2omFtfvFEIQBIu2pRKRneBwUyLbEu5X0YHz+YerTiYGAL2aBqqfuzqJ8FCTAGycGoOfj1zD1exChPq4Ye+/WVrHnL2Vq7XMe2Xf/HUR4X7uSM8tRqiPe5V7g1JjJEhargy9Pt+HQ//py+SCiABwuCmRw+kU1QCdorRnKD1zMwcr4q/h9zNpVebK0OW1DWd0br+SXVClGSXAS2LTScWiRYvwxRdfICMjA+3bt8d3332Hrl27WjssIocnsmCVBRMLIgtrH+GLryM64OsnOkBWqkBaTjGu3SmEn4crcotLca+oBDPWnUGTIC9czirQe57+Xx2osq1ZsP5mFWtbt24dZs6cie+//x7dunXD/PnzMWjQICQnJyMoKKj2FxAEoFT36B2i+spNkMEdckiEYrhDprtQSSHg4mG26TlFgoUHv+fl5cHHxwe5ubmQSqXVH0BUD928W4RgqRsupOehuFSB9uG+iFt9qkpTSmXvDm2pc90XTdb6DHbr1g1dunTBwoULAQBKpRIRERF4+eWX8dZbb2mVlcvlkMsr1oPJy8tDRESE4ZhLCoFPwuosfiKH9nYa4Kp7ocVyxt47OEEWkQ2KaOABV2cntI/wRfdG/nB3FePH/+uEp3s0xEcj22BAy+Aqx7QI8caYThFWiLZ6JSUlSEhIwIABFQu5OTk5YcCAAYiPj69Sft68efDx8VE/IiJs8/sioqrYFEJkJ5zFTpgzXDVE9snuUSgqKcPm07fQMdIPzYO9IaD6pdut5fbt21AoFAgO1k6IgoOD8e+//1YpP2vWLMycOVP9urzGwiAXD+wYnoCLGivZEhHQJMgL/VoEYf3JVOQWlaJ/y2Ck3C5A/NU7mPJwY9Xswi76Zxg2FRMLIjvl4eqMid2irB1GnZBIJJBIJKYdJBJhSKcmGFI3IRHZvcl9fNTP20QDw7s0rZPrsCmEiOpcQEAAxGIxMjMztbZnZmYiJCTESlERUV1gYkFEdc7V1RWdOnXCnj171NuUSiX27NmDmJgYK0ZGRObGphAisoiZM2di0qRJ6Ny5M7p27Yr58+ejsLAQTz/9tLVDIyIzYmJBRBYxduxYZGdnY/bs2cjIyECHDh2wc+fOKh06ici+MbEgIouZNm0apk2bZu0wiKgOsY8FERERmQ0TCyIiIjIbJhZERERkNkwsiIiIyGyYWBAREZHZMLEgIiIis2FiQURERGbDxIKIiIjMxuITZAmCAEC1DDIRWV75Z6/8s2gPeN8gsj5j7x0WTyzy8/MBABEREZa+NBFpyM/Ph4+PT/UFbQDvG0S2o7p7h0iw8L8tSqUSaWlp8Pb2hkgk0lsuLy8PERERuHnzJqRSqQUjdCx8H83Dkd5HQRCQn5+PsLAwODnZR2so7xuWx/fSPBzpfTT23mHxGgsnJyeEh4cbXV4qldr9D8MW8H00D0d5H+2lpqIc7xvWw/fSPBzlfTTm3mEf/64QERGRXWBiQURERGZjs4mFRCLBnDlzIJFIrB2KXeP7aB58H+0Df07mw/fSPOrj+2jxzptERETkuGy2xoKIiIjsDxMLIiIiMhsmFkRERGQ2TCyIiIjIbJhYEBERkdnYZGKxaNEiNGzYEG5ubujWrRuOHz9u7ZBsyrx589ClSxd4e3sjKCgII0eORHJyslYZmUyGuLg4+Pv7w8vLC7GxscjMzNQqc+PGDQwdOhQeHh4ICgrCG2+8gbKyMkt+Kzbl008/hUgkwvTp09Xb+D7aF9479ON9o27wvqGDYGPWrl0ruLq6Cj///LNw7tw54fnnnxd8fX2FzMxMa4dmMwYNGiQsW7ZMSEpKEhITE4UhQ4YIkZGRQkFBgbrMiy++KERERAh79uwRTp48KXTv3l146KGH1PvLysqENm3aCAMGDBBOnz4t7NixQwgICBBmzZpljW/J6o4fPy40bNhQaNeunfDqq6+qt/N9tB+8dxjG+4b58b6hm80lFl27dhXi4uLUrxUKhRAWFibMmzfPilHZtqysLAGAcODAAUEQBCEnJ0dwcXERNmzYoC5z4cIFAYAQHx8vCIIg7NixQ3BychIyMjLUZZYsWSJIpVJBLpdb9huwsvz8fKFp06bC7t27hd69e6tvEHwf7QvvHabhfaN2eN/Qz6aaQkpKSpCQkIABAwaotzk5OWHAgAGIj4+3YmS2LTc3FwDQoEEDAEBCQgJKS0u13scWLVogMjJS/T7Gx8ejbdu2CA4OVpcZNGgQ8vLycO7cOQtGb31xcXEYOnSo1vsF8H20J7x3mI73jdrhfUM/i69uasjt27ehUCi03mwACA4Oxr///mulqGybUqnE9OnT0aNHD7Rp0wYAkJGRAVdXV/j6+mqVDQ4ORkZGhrqMrve5fF99sXbtWpw6dQonTpyoso/vo/3gvcM0vG/UDu8bhtlUYkGmi4uLQ1JSEg4fPmztUOzOzZs38eqrr2L37t1wc3OzdjhEFsP7Rs3xvlE9m2oKCQgIgFgsrtJ7NjMzEyEhIVaKynZNmzYN27Ztw759+xAeHq7eHhISgpKSEuTk5GiV13wfQ0JCdL7P5fvqg4SEBGRlZaFjx45wdnaGs7MzDhw4gG+//RbOzs4IDg7m+2gneO8wHu8btcP7RvVsKrFwdXVFp06dsGfPHvU2pVKJPXv2ICYmxoqR2RZBEDBt2jRs3rwZe/fuRXR0tNb+Tp06wcXFRet9TE5Oxo0bN9TvY0xMDM6ePYusrCx1md27d0MqlaJVq1aW+UasrH///jh79iwSExPVj86dO2PixInq53wf7QPvHdXjfcM8eN8wgrV7j1a2du1aQSKRCMuXLxfOnz8vTJkyRfD19dXqPVvfTZ06VfDx8RH2798vpKenqx9FRUXqMi+++KIQGRkp7N27Vzh58qQQExMjxMTEqPeXD3caOHCgkJiYKOzcuVMIDAx0mOFONaXZu1sQ+D7aE947DON9o+7wvqHN5hILQRCE7777ToiMjBRcXV2Frl27CkePHrV2SDYFgM7HsmXL1GWKi4uFl156SfDz8xM8PDyExx9/XEhPT9c6z7Vr14TBgwcL7u7uQkBAgPDaa68JpaWlFv5ubEvlGwTfR/vCe4d+vG/UHd43tIkEQRCsU1dCREREjsam+lgQERGRfWNiQURERGbDxIKIiIjMhokFERERmQ0TCyIiIjIbJhZERERkNkwsiIiIyGyYWBAREZHZMLEgIiIis2FiQURERGbDxIKIiIjM5v8Bq2G28XwMZzoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_list, acc_list, val_loss_list, val_acc_list = [], [], [], []\n",
    "teacher_forcing_ratio = 0.5\n",
    "print_batches = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    running_accuracy = 0\n",
    "    total_batches = len(train)\n",
    "\n",
    "    # for inp_data, target in tqdm(train, desc=f\"[Epoch {epoch+1:3d}/{num_epochs}] \", leave = False):        \n",
    "    for inp_data, target in train:        \n",
    "        inp_data = inp_data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        teacher_forcing = False\n",
    "        if(epoch < 0.5*num_epochs): #for inital epochs, batch wise tfr\n",
    "            teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "        \n",
    "        output = mod(inp_data, target, teacher_forcing).to(device)\n",
    "        \n",
    "        if(epoch == num_epochs-1 and print_batches != 0):\n",
    "            print(data.tensor_to_string(output.argmax(2)))\n",
    "            print(data.tensor_to_string(target))\n",
    "            print_batches -= 1\n",
    "\n",
    "        running_accuracy += mod.calc_accuracy(output.argmax(2), target)\n",
    "\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        target = target.reshape(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(mod.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    val_loss, val_accuracy= mod.calc_evaluation_metrics(valid)\n",
    "\n",
    "    print(f\"[Epoch {epoch+1:3d}/{num_epochs}] \\t Loss: {(running_loss/total_batches):.3f}\\t Acc: {(running_accuracy/total_batches):2.2f} \\t Val Loss: {val_loss:2.3f}\\t Val Acc: {val_accuracy:2.2f}\")\n",
    "    loss_list.append(running_loss/total_batches)\n",
    "    acc_list.append(running_accuracy/total_batches)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_acc_list.append(val_accuracy)\n",
    "\n",
    "fig = plot_graphs(loss_list, val_loss_list, acc_list, val_acc_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
