{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "from Utils import plot_graphs\n",
    "from AkshrantarDataset import AksharantarDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'tam'\n",
    "start_token = '<'\n",
    "end_token = '>'\n",
    "pad_token = ' '\n",
    "unk_token = '~'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = AksharantarDataset(language, start_token, end_token, pad_token, unk_token)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=data.tr_l2i[pad_token])\n",
    "\n",
    "target_dict_count = len(data.tr_l2i)\n",
    "english_dict_count = len(data.en_l2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers = 1, p = 0, bi_dir = False, rnn_class = nn.GRU):\n",
    "        \"\"\"\n",
    "        Init Parameters:\n",
    "        input_size : english_dict_count\n",
    "        embedding_size : size of each embedding vector\n",
    "        hidden_size : size of hidden state vector\n",
    "        num_layers : number of recurrent layers of RNN\n",
    "        p : dropout probability\n",
    "        rnn_class: type of RNN to be used in the encoder\n",
    "\n",
    "        Input:\n",
    "        x : torch.Tensor of shape (seq_length, N)\n",
    "            where seq_length - len of longest string in the batch\n",
    "            N - batch size\n",
    "        \n",
    "        Outpus:\n",
    "        outputs: torch.Tensor of shape (seq_len, N, hidden_size * D), where D = 2 if bi_dir = True else 1\n",
    "        hidden: torch.Tensor of shape (num_layers * D, N, hidden_size)\n",
    "        \n",
    "        cell: torch.Tensor of shape (num_layers * D, N, hidden_size) if(rnn_class == \"LSTM\")\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn_class= rnn_class\n",
    "        self.rnn = rnn_class(embedding_size, hidden_size, num_layers, dropout=p, bidirectional = bi_dir)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_length, N) where N is batch size\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (seq_length, N, embedding_size)\n",
    "\n",
    "        if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "            outputs, (hidden, cell) = self.rnn(embedding)\n",
    "            # outputs shape: (seq_length, N, hidden_size)\n",
    "        else:\n",
    "            outputs, hidden = self.rnn(embedding)\n",
    "        \n",
    "        if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "            return outputs, hidden, cell\n",
    "        else:\n",
    "            return outputs, hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers = 1, p = 0, bi_dir = False, rnn_class = nn.GRU):\n",
    "        \"\"\"input size = output size = target language charecters\n",
    "        Init Parameters:\n",
    "        input_size: target_dict_count\n",
    "        embedding_size: size of each embedding vector\n",
    "        hidden_size: size of hidden state vector\n",
    "        output_size: number of output features in fully connected layer\n",
    "        num_layers : number of recurrent layers of RNN\n",
    "        p : dropout probability\n",
    "        rnn_class: type of RNN to be used in the encoder\n",
    "\n",
    "        Input:\n",
    "        x: torch.Tensor of shape (N)\n",
    "        hidden: torch.Tensor of shape (num_layers * D, N, hidden_size)\n",
    "        cell: torch.Tensor of shape (num_layers * D, N, hidden_size)\n",
    "\n",
    "        Outputs:\n",
    "        predications: torch.Tensor of shape (N, target_dict_count), where D = 2 if bi_dir = True else 1\n",
    "        hidden: torch.Tensor of shape (num_layers * D, N, hidden_size)\n",
    "        \n",
    "        cell: torch.Tensor of shape (num_layers * D, N, hidden_size) if(rnn_class == \"LSTM\")\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn_class = rnn_class\n",
    "        self.rnn = rnn_class(embedding_size, hidden_size, num_layers, dropout=p, bidirectional = bi_dir)\n",
    "\n",
    "        self.D = 1\n",
    "        if(bi_dir == True):\n",
    "            self.D = 2\n",
    "        self.fc = nn.Linear(hidden_size * self.D, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden, cell = None):\n",
    "        #cell is set to none, for GRU and RNN\n",
    "\n",
    "        # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n",
    "        # is 1 here because we are sending in a single word and not a sentence\n",
    "        # print(x.shape, hidden.shape, cell.shape)\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (1, N, embedding_size)\n",
    "        \n",
    "        if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "            outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
    "            # outputs shape: (1, N, hidden_size * D)\n",
    "        else:\n",
    "            outputs, hidden = self.rnn(embedding, hidden)\n",
    "            \n",
    "\n",
    "        predictions = self.fc(outputs)\n",
    "\n",
    "        # predictions shape: (1, N, length_target_vocabulary) to send it to\n",
    "        # loss function we want it to be (N, length_target_vocabulary) so we're\n",
    "        # just gonna remove the first dim\n",
    "        predictions = predictions.squeeze(0)\n",
    "\n",
    "        if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "            return predictions, hidden, cell\n",
    "        else:\n",
    "            return predictions, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        encoder_layers = encoder.num_layers\n",
    "        decoder_layers = decoder.num_layers\n",
    "        D = decoder.D #we set bidiretion as common in both encoder and decoder, so no need to check for D value seperately\n",
    "        self.enc_to_dec = nn.Linear(encoder_layers*D, decoder_layers*D)\n",
    "        self.rnn_class = decoder.rnn_class #we use same rnn in both encoder and decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_forcing = False):\n",
    "        \"\"\"source : (source_len, N) - not sure\n",
    "        teacher_forching_ratio : probability in which original values is favored over predicted values\n",
    "                                if 0 : predicted values is passed for all chars in target\n",
    "                                if 1 : true values is passed for all chars in target\n",
    "\n",
    "        \"\"\"\n",
    "        batch_size = source.shape[1] \n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = target_dict_count\n",
    "\n",
    "        # print(\"source shape \", source.shape)\n",
    "        # print(\"target shape \", target.shape)\n",
    "        # print(\"N : \", batch_size)\n",
    "        # print(\"tar len : \", target_len)\n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size)\n",
    "        # print(\"outputs shape : \", outputs.shape)\n",
    "\n",
    "        \n",
    "        if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "            _, hidden, cell = self.encoder(source)\n",
    "        else:\n",
    "            _, hidden = self.encoder(source)\n",
    "\n",
    "        N = hidden.shape[1]\n",
    "        hidden_size= hidden.shape[2]\n",
    "        # hidden, cell shape: (D*encoder_layers, N, hidden_size)\n",
    "\n",
    "        #UNCOMMENT THIS TO HANDLE dinstinct encoder and decoder layers\n",
    "        # hidden = hidden.transpose(0, 2) # hidden shape: (hidden_size, N, D*encoder_layers)\n",
    "        # hidden = hidden.reshape(-1, hidden.shape[2]) # hidden shape: (hidden_size * N, D*encoder_layers)\n",
    "        # hidden = self.enc_to_dec(hidden) # hidden shape: (hidden_size * N, D*decoder_layers)\n",
    "        # hidden = hidden.reshape(hidden_size, N, hidden.shape[1]) # hidden shape: (hidden_size, N, D*decoder_layers)\n",
    "        # hidden = hidden.transpose(0,2) # hidden shape: (D*decoder_layers, N, hidden_size)\n",
    "\n",
    "        # if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "        #     #at all the below steps, cell will have the shape of hidden\n",
    "        #     cell = cell.transpose(0,2)\n",
    "        #     cell = cell.reshape(-1, cell.shape[2])\n",
    "        #     cell = self.enc_to_dec(cell)\n",
    "        #     cell = cell.reshape(hidden_size, N, cell.shape[1])\n",
    "        #     cell = cell.transpose(0,2)\n",
    "\n",
    "\n",
    "        # Grab the first input to the Decoder\n",
    "        x = target[0]\n",
    "        outputs[:, :, data.tr_l2i[start_token]] = 1 #setting prob = 1 for starting token \n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            # Use previous hidden, cell as context from encoder at start\n",
    "            if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "                output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "            else:\n",
    "                output, hidden = self.decoder(x, hidden)\n",
    "\n",
    "            # Store next output prediction\n",
    "            outputs[t] = output\n",
    "\n",
    "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
    "            best_guess = output.argmax(1)\n",
    "\n",
    "            # With probability of teacher_force_ratio we take the actual next word\n",
    "            # otherwise we take the word that the Decoder predicted it to be.\n",
    "            # Teacher Forcing is used so that the model gets used to seeing\n",
    "            # similar inputs at training and testing time, if teacher forcing is 1\n",
    "            # then inputs at test time might be completely different than what the\n",
    "            # network is used to. This was a long comment.\n",
    "            x = target[t] if teacher_forcing == True else best_guess\n",
    "        # print(\"OUTPUTS: \", outputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def calc_accuracy(self, output, target):\n",
    "        \"\"\"\n",
    "        output: torch.Tensor of shape (seq_len, N)\n",
    "        target: torch.Tensor of shape (seq_len, N)\n",
    "        \"\"\"\n",
    "        # batch_size = 32\n",
    "        running_acc = 0\n",
    "        seq_len = output.shape[0]\n",
    "        N = output.shape[1]\n",
    "        matched_strings = 0\n",
    "        with torch.no_grad():\n",
    "            for j in range(N):\n",
    "                current_word_matched = True\n",
    "                for i in range(seq_len):\n",
    "                    if(target[i][j] == data.tr_l2i[pad_token]): #we dont care whatever prediction in the pad_token place\n",
    "                        break\n",
    "                    if(output[i][j] != target[i][j]): #compare the predictions of charecters, start and end_token places\n",
    "                        current_word_matched = False\n",
    "                        break\n",
    "                if(current_word_matched == True):\n",
    "                    matched_strings += 1\n",
    "        return matched_strings*100 / N \n",
    "    \n",
    "\n",
    "    def calc_evaluation_metrics(self, src_tar_pair):\n",
    "        \"\"\"\n",
    "        src_tar_pair: a list of tuples, where each tuple consists of two tensors (source, target)\n",
    "                      source:tensors of shape (seq_len * N), note: seq_len might not be same across all the tensors in the list\n",
    "                      target:tensors of shape (seq_len * N) \n",
    "        \"\"\"\n",
    "        num_batches = len(src_tar_pair)\n",
    "        with torch.no_grad():\n",
    "            acc = 0\n",
    "            running_loss = 0\n",
    "            for source, target in src_tar_pair:\n",
    "                source = source.to(device)\n",
    "                target = target.to(device)\n",
    "                \n",
    "                output = self(source, target).to(device)\n",
    "                acc += self.calc_accuracy(output.argmax(2), target)\n",
    "\n",
    "                output = output.reshape(-1, output.shape[2])\n",
    "                target = target.reshape(-1)\n",
    "\n",
    "                loss = criterion(output, target)\n",
    "                running_loss += loss.item()\n",
    "        return running_loss, acc/num_batches\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "# Model hyperparameters\n",
    "input_size_encoder = english_dict_count\n",
    "input_size_decoder = target_dict_count\n",
    "output_size = target_dict_count\n",
    "embedding_size = 256\n",
    "encoder_layers = 3\n",
    "decoder_layers = 3\n",
    "enc_dropout = 0.3\n",
    "dec_dropout = 0.3\n",
    "hidden_size = 128\n",
    "bi_directional = True\n",
    "rnn = nn.LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(english_dict_count, embedding_size, hidden_size, \n",
    "              num_layers=encoder_layers, \n",
    "              bi_dir=bi_directional,\n",
    "              p=enc_dropout,\n",
    "              rnn_class=rnn).to(device)\n",
    "dec = Decoder(target_dict_count, embedding_size, hidden_size, target_dict_count, \n",
    "              num_layers=decoder_layers, \n",
    "              bi_dir=bi_directional, \n",
    "              p = dec_dropout,\n",
    "              rnn_class=rnn).to(device)\n",
    "\n",
    "mod = Seq2Seq(enc, dec).to(device)\n",
    "\n",
    "optimizer = optim.Adam(mod.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.load_data(\"train\", batch_size)\n",
    "valid = data.load_data(\"valid\", batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list, acc_list, val_loss_list, val_acc_list = [], [], [], []\n",
    "teacher_forcing_ratio = 0.5\n",
    "print_batches = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    running_accuracy = 0\n",
    "    total_batches = len(train)\n",
    "\n",
    "    for inp_data, target in tqdm(train, desc=f\"Epoch {epoch}: \", leave = False):\n",
    "        # teacher_forcing_ratio = max(0, init_teacher_forcing_ratio * (1 - (epoch*2/num_epochs)))\n",
    "        # teacher_forcing = True if epoch < teacher_forcing_ratio*num_epochs else False\n",
    "        \n",
    "        inp_data = inp_data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        teacher_forcing = False\n",
    "        if(epoch < 0.5*num_epochs): #for inital epochs, batch wise tfr\n",
    "            teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "        output = mod(inp_data, target, teacher_forcing).to(device)\n",
    "        \n",
    "        # if(epoch == num_epochs-1 and print_batches != 0):\n",
    "        #     print(data.tensor_to_string(output.argmax(2)))\n",
    "        #     print(data.tensor_to_string(target))\n",
    "        #     print_batches -= 1\n",
    "\n",
    "        running_accuracy += mod.calc_accuracy(output.argmax(2), target)\n",
    "\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        target = target.reshape(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(mod.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    val_loss, val_accuracy= mod.calc_evaluation_metrics(valid)\n",
    "\n",
    "    print(f\"[Epoch {epoch+1:3d}/{num_epochs}] \\t Loss: {(running_loss/total_batches):.3f}\\t Acc: {(running_accuracy/total_batches):2.2f} \\t Val Loss: {val_loss:2.3f}\\t Val Acc: {val_accuracy:2.2f}\")\n",
    "    loss_list.append(running_loss/total_batches)\n",
    "    acc_list.append(running_accuracy/total_batches)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_acc_list.append(val_accuracy)\n",
    "\n",
    "fig = plot_graphs(loss_list, val_loss_list, acc_list, val_acc_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
