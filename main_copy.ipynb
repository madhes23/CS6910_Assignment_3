{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import preprocess, string_to_tensor\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'tam'\n",
    "start_token = '<'\n",
    "end_token = '>'\n",
    "pad_token = ' '\n",
    "unk_token = '~'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'aksharantar_sampled/' + language\n",
    "\n",
    "train_df = pd.read_csv(path+'/'+language+'_train.csv', header=None)\n",
    "test_df = pd.read_csv(path+'/'+language+'_test.csv', header=None)\n",
    "val_df = pd.read_csv(path+'/'+language+'_valid.csv', header=None)\n",
    "\n",
    "train_source, train_target = train_df[0].tolist(), train_df[1].tolist()\n",
    "test_source, test_target = test_df[0].tolist(), test_df[1].tolist()\n",
    "val_source, val_target = val_df[0].tolist(), val_df[1].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thottacharya  -->  தொட்டாச்சார்ய\n",
      "menmaithaan  -->  மென்மைதான்\n",
      "avarantri  -->  அவரன்றி\n",
      "mudiyarathu  -->  முடியறது\n",
      "aadaiyanigalaal  -->  ஆடையணிகளால்\n"
     ]
    }
   ],
   "source": [
    "num_sample = 5\n",
    "for i in range(num_sample):\n",
    "    print(f'{train_source[i]}  -->  {train_target[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Charecters :  50\n",
      "Target Charecters: \n",
      "0  \t\n",
      "1 ழ\t2 ச\t3 ு\t4 ஷ\t5 ஓ\t\n",
      "6 ெ\t7 ~\t8 ள\t9 ூ\t10 உ\t\n",
      "11 ர\t12 ஞ\t13 ஈ\t14 >\t15 <\t\n",
      "16 ய\t17 இ\t18 ா\t19 ந\t20 ஐ\t\n",
      "21 ொ\t22 ஃ\t23 ோ\t24 எ\t25 ங\t\n",
      "26 ஏ\t27 ஸ\t28 ஆ\t29 ல\t30 ண\t\n",
      "31 வ\t32 ன\t33 ை\t34 க\t35 ஒ\t\n",
      "36 ஜ\t37 ௌ\t38 ஊ\t39 த\t40 ற\t\n",
      "41 ்\t42 ி\t43 ீ\t44 ஹ\t45 ப\t\n",
      "46 ம\t47 ட\t48 அ\t49 ே\t"
     ]
    }
   ],
   "source": [
    "english_chars = list(set(''.join(train_source) + start_token + end_token + pad_token + unk_token))\n",
    "target_chars = list(set(''.join(train_target) + start_token + end_token + pad_token + unk_token))\n",
    "\n",
    "english_dict_count = len(english_chars)\n",
    "target_dict_count = len(target_chars)\n",
    "\n",
    "print(\"Number of Charecters : \", target_dict_count)\n",
    "\n",
    "print(\"Target Charecters: \")\n",
    "for i, c in enumerate(target_chars):\n",
    "    print(i, c, end='\\t')\n",
    "    if i % 5 == 0:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_i2l, en_l2i = {}, {}\n",
    "tr_i2l, tr_l2i = {}, {}\n",
    "\n",
    "for i, x in enumerate(english_chars):\n",
    "    en_l2i[x] = i\n",
    "    en_i2l[i] = x\n",
    "\n",
    "for i, x in enumerate(target_chars):\n",
    "    tr_l2i[x] = i\n",
    "    tr_i2l[i] = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing test and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string_to_tensor() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Education\\IITM\\Second Sem\\DL\\3\\main_copy.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Education/IITM/Second%20Sem/DL/3/main_copy.ipynb#X64sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m val_source_tensor \u001b[39m=\u001b[39m string_to_tensor(preprocess(val_source, start_token, end_token, pad_token), en_l2i, unk_token)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mrequires_grad_(\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Education/IITM/Second%20Sem/DL/3/main_copy.ipynb#X64sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m val_target_tensor \u001b[39m=\u001b[39m string_to_tensor(preprocess(val_target, start_token, end_token, pad_token), tr_l2i, unk_token)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mrequires_grad_(\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Education/IITM/Second%20Sem/DL/3/main_copy.ipynb#X64sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m test_source_tensor \u001b[39m=\u001b[39m string_to_tensor(preprocess(test_source, start_token, end_token, pad_token), en_l2i, unk_token)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mrequires_grad_(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: string_to_tensor() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "val_source_tensor = string_to_tensor(preprocess(val_source, start_token, end_token, pad_token), en_l2i, unk_token).transpose(0,1).requires_grad_(False)\n",
    "val_target_tensor = string_to_tensor(preprocess(val_target, start_token, end_token, pad_token), tr_l2i, unk_token).transpose(0,1).requires_grad_(False)\n",
    "\n",
    "test_source_tensor = string_to_tensor(preprocess(test_source, start_token, end_token, pad_token), en_l2i, unk_token).transpose(0,1).requires_grad_(False)\n",
    "test_target_tensor = string_to_tensor(preprocess(test_target, start_token, end_token, pad_token), tr_l2i, unk_token).transpose(0,1).requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers = 1, p = 0, bi_dir = False, rnn_class = nn.GRU):\n",
    "        \"\"\"\n",
    "        Init Parameters:\n",
    "        input_size : english_dict_count\n",
    "        embedding_size : size of each embedding vector\n",
    "        hidden_size : size of hidden state vector\n",
    "        num_layers : number of recurrent layers of RNN\n",
    "        p : dropout probability\n",
    "        rnn_class: type of RNN to be used in the encoder\n",
    "\n",
    "        Input:\n",
    "        x : torch.Tensor of shape (seq_length, N)\n",
    "            where seq_length - len of longest string in the batch\n",
    "            N - batch size\n",
    "        \n",
    "        Outpus:\n",
    "        outputs: torch.Tensor of shape (seq_len, N, hidden_size * D), where D = 2 if bi_dir = True else 1\n",
    "        hidden: torch.Tensor of shape (num_layers * D, N, hidden_size)\n",
    "        \n",
    "        cell: torch.Tensor of shape (num_layers * D, N, hidden_size) if(rnn_class == \"LSTM\")\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn_class= rnn_class\n",
    "        self.rnn = rnn_class(embedding_size, hidden_size, num_layers, dropout=p, bidirectional = bi_dir)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_length, N) where N is batch size\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (seq_length, N, embedding_size)\n",
    "\n",
    "        if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "            outputs, (hidden, cell) = self.rnn(embedding)\n",
    "            # outputs shape: (seq_length, N, hidden_size)\n",
    "        else:\n",
    "            outputs, hidden = self.rnn(embedding)\n",
    "        \n",
    "        if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "            return outputs, hidden, cell\n",
    "        else:\n",
    "            return outputs, hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers = 1, p = 0, bi_dir = False, rnn_class = nn.GRU):\n",
    "        \"\"\"input size = output size = target language charecters\n",
    "        Init Parameters:\n",
    "        input_size: target_dict_count\n",
    "        embedding_size: size of each embedding vector\n",
    "        hidden_size: size of hidden state vector\n",
    "        output_size: number of output features in fully connected layer\n",
    "        num_layers : number of recurrent layers of RNN\n",
    "        p : dropout probability\n",
    "        rnn_class: type of RNN to be used in the encoder\n",
    "\n",
    "        Input:\n",
    "        x: torch.Tensor of shape (N)\n",
    "        hidden: torch.Tensor of shape (num_layers * D, N, hidden_size)\n",
    "        cell: torch.Tensor of shape (num_layers * D, N, hidden_size)\n",
    "\n",
    "        Outputs:\n",
    "        predications: torch.Tensor of shape (N, target_dict_count), where D = 2 if bi_dir = True else 1\n",
    "        hidden: torch.Tensor of shape (num_layers * D, N, hidden_size)\n",
    "        \n",
    "        cell: torch.Tensor of shape (num_layers * D, N, hidden_size) if(rnn_class == \"LSTM\")\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn_class = rnn_class\n",
    "        self.rnn = rnn_class(embedding_size, hidden_size, num_layers, dropout=p, bidirectional = bi_dir)\n",
    "\n",
    "        self.D = 1\n",
    "        if(bi_dir == True):\n",
    "            self.D = 2\n",
    "        self.fc = nn.Linear(hidden_size * self.D, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden, cell = None):\n",
    "        #cell is set to none, for GRU and RNN\n",
    "\n",
    "        # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n",
    "        # is 1 here because we are sending in a single word and not a sentence\n",
    "        # print(x.shape, hidden.shape, cell.shape)\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (1, N, embedding_size)\n",
    "        \n",
    "        if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "            outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
    "            # outputs shape: (1, N, hidden_size * D)\n",
    "        else:\n",
    "            outputs, hidden = self.rnn(embedding, hidden)\n",
    "            \n",
    "\n",
    "        predictions = self.fc(outputs)\n",
    "\n",
    "        # predictions shape: (1, N, length_target_vocabulary) to send it to\n",
    "        # loss function we want it to be (N, length_target_vocabulary) so we're\n",
    "        # just gonna remove the first dim\n",
    "        predictions = predictions.squeeze(0)\n",
    "\n",
    "        if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "            return predictions, hidden, cell\n",
    "        else:\n",
    "            return predictions, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        encoder_layers = encoder.num_layers\n",
    "        decoder_layers = decoder.num_layers\n",
    "        D = decoder.D #we set bidiretion as common in both encoder and decoder, so no need to check for D value seperately\n",
    "        self.enc_to_dec = nn.Linear(encoder_layers*D, decoder_layers*D)\n",
    "        self.rnn_class = decoder.rnn_class #we use same rnn in both encoder and decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        \"\"\"source : (source_len, N) - not sure\n",
    "        teacher_forching_ratio : probability in which original values is favored over predicted values\n",
    "                                if 0 : predicted values is passed for all chars in target\n",
    "                                if 1 : true values is passed for all chars in target\n",
    "\n",
    "        \"\"\"\n",
    "        batch_size = source.shape[1] \n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = target_dict_count\n",
    "\n",
    "        # print(\"source shape \", source.shape)\n",
    "        # print(\"target shape \", target.shape)\n",
    "        # print(\"N : \", batch_size)\n",
    "        # print(\"tar len : \", target_len)\n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size)\n",
    "        # print(\"outputs shape : \", outputs.shape)\n",
    "\n",
    "        \n",
    "        if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "            _, hidden, cell = self.encoder(source)\n",
    "        else:\n",
    "            _, hidden = self.encoder(source)\n",
    "\n",
    "        N = hidden.shape[1]\n",
    "        hidden_size= hidden.shape[2]\n",
    "        # hidden, cell shape: (D*encoder_layers, N, hidden_size)\n",
    "\n",
    "        hidden = hidden.transpose(0, 2) # hidden shape: (hidden_size, N, D*encoder_layers)\n",
    "        hidden = hidden.reshape(-1, hidden.shape[2]) # hidden shape: (hidden_size * N, D*encoder_layers)\n",
    "        hidden = self.enc_to_dec(hidden) # hidden shape: (hidden_size * N, D*decoder_layers)\n",
    "        hidden = hidden.reshape(hidden_size, N, hidden.shape[1]) # hidden shape: (hidden_size, N, D*decoder_layers)\n",
    "        hidden = hidden.transpose(0,2) # hidden shape: (D*decoder_layers, N, hidden_size)\n",
    "\n",
    "        if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "            #at all the below steps, cell will have the shape of hidden\n",
    "            cell = cell.transpose(0,2)\n",
    "            cell = cell.reshape(-1, cell.shape[2])\n",
    "            cell = self.enc_to_dec(cell)\n",
    "            cell = cell.reshape(hidden_size, N, cell.shape[1])\n",
    "            cell = cell.transpose(0,2)\n",
    "\n",
    "        # Grab the first input to the Decoder which will be <SOS> token\n",
    "        x = target[0]\n",
    "        outputs[:, :, tr_l2i[start_token]] = 1 #setting prob = 1 for starting token \n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            # Use previous hidden, cell as context from encoder at start\n",
    "            if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "                output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "            else:\n",
    "                output, hidden = self.decoder(x, hidden)\n",
    "\n",
    "            # Store next output prediction\n",
    "            outputs[t] = output\n",
    "\n",
    "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
    "            best_guess = output.argmax(1)\n",
    "\n",
    "            # With probability of teacher_force_ratio we take the actual next word\n",
    "            # otherwise we take the word that the Decoder predicted it to be.\n",
    "            # Teacher Forcing is used so that the model gets used to seeing\n",
    "            # similar inputs at training and testing time, if teacher forcing is 1\n",
    "            # then inputs at test time might be completely different than what the\n",
    "            # network is used to. This was a long comment.\n",
    "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "        # print(\"OUTPUTS: \", outputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def calc_accuracy(self, output, target):\n",
    "        \"\"\"\n",
    "        output: torch.Tensor of shape (seq_len, N)\n",
    "        target: torch.Tensor of shape (seq_len, N)\n",
    "        \"\"\"\n",
    "        # batch_size = 32\n",
    "        running_acc = 0\n",
    "        seq_len = output.shape[0]\n",
    "        N = output.shape[1]\n",
    "        matched_strings = 0\n",
    "        with torch.no_grad():\n",
    "            for j in range(N):\n",
    "                current_word_matched = True\n",
    "                for i in range(seq_len):\n",
    "                    if(target[i][j] in {tr_l2i[pad_token], tr_l2i[start_token], tr_l2i[end_token]}):\n",
    "                        continue\n",
    "                    if(output[i][j] != target[i][j]):\n",
    "                        current_word_matched = False\n",
    "                        break\n",
    "                if(current_word_matched == True):\n",
    "                    matched_strings += 1\n",
    "        return matched_strings*100 / N \n",
    "\n",
    "\n",
    "    def calc_evaluation_metrics(self, soruce_strings, target_strings):\n",
    "        \"\"\"\n",
    "\n",
    "        Returns:\n",
    "        loss: loss value for the current batch of strings\n",
    "        accuracy: accuracy value for the current batch of strings\n",
    "        \"\"\"\n",
    "        batch_size = 32\n",
    "        loss  = 0\n",
    "        with torch.no_grad():\n",
    "            no_of_batch = 0\n",
    "            running_accuracy = 0\n",
    "            for i in range(0, len(soruce_strings), batch_size):\n",
    "                inp_data = string_to_tensor(preprocess(soruce_strings[i:i+batch_size], start_token, end_token, pad_token), en_l2i).transpose(0,1)\n",
    "                target = string_to_tensor(preprocess(target_strings[i:i+batch_size], start_token, end_token, pad_token), tr_l2i).transpose(0,1)\n",
    "\n",
    "                output = self(inp_data, target)\n",
    "                running_accuracy += self.calc_accuracy(output.argmax(2), target)\n",
    "\n",
    "                output = output.reshape(-1, output.shape[2])\n",
    "                target = target.reshape(-1)\n",
    "\n",
    "                loss = criterion(output, target)\n",
    "                loss += loss.item()\n",
    "                no_of_batch += 1\n",
    "        return loss, running_accuracy*100/no_of_batch\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "\n",
    "# Model hyperparameters\n",
    "load_model = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_size_encoder = english_dict_count\n",
    "input_size_decoder = target_dict_count\n",
    "output_size = target_dict_count\n",
    "embedding_size = 32\n",
    "encoder_layers = 2\n",
    "decoder_layers = 2\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n",
    "hidden_size = 32\n",
    "bi_directional = True\n",
    "rnn = nn.LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(english_dict_count, embedding_size, hidden_size, \n",
    "              num_layers=encoder_layers, \n",
    "              bi_dir=bi_directional,\n",
    "              p=enc_dropout,\n",
    "              rnn_class=rnn)\n",
    "dec = Decoder(target_dict_count, embedding_size, hidden_size, target_dict_count, \n",
    "              num_layers=decoder_layers, \n",
    "              bi_dir=bi_directional, \n",
    "              p = dec_dropout,\n",
    "              rnn_class=rnn)\n",
    "\n",
    "mod = Seq2Seq(enc, dec)\n",
    "\n",
    "optimizer = optim.Adam(mod.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_print(output):\n",
    "    \"\"\"output shape: target_seq_length * N\"\"\"\n",
    "    res = []\n",
    "    for j in range(output.shape[1]):\n",
    "        temp = \"\"\n",
    "        for i in range(output.shape[0]):\n",
    "            temp += tr_i2l[output[i,j].item()]\n",
    "        \n",
    "        res.append(temp)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch   1/1000] \t Loss: 3.8328\t Acc: 0.0000 \t Val Loss: 7.6642\t Val Acc: 0.0000\n",
      "[Epoch   2/1000] \t Loss: 3.8281\t Acc: 0.0000 \t Val Loss: 7.6468\t Val Acc: 0.0000\n",
      "[Epoch   3/1000] \t Loss: 3.8090\t Acc: 0.0000 \t Val Loss: 7.6084\t Val Acc: 0.0000\n",
      "[Epoch   4/1000] \t Loss: 3.8045\t Acc: 0.0000 \t Val Loss: 7.6169\t Val Acc: 0.0000\n",
      "[Epoch   5/1000] \t Loss: 3.7752\t Acc: 0.0000 \t Val Loss: 7.5459\t Val Acc: 0.0000\n",
      "[Epoch   6/1000] \t Loss: 3.7659\t Acc: 0.0000 \t Val Loss: 7.5402\t Val Acc: 0.0000\n",
      "[Epoch   7/1000] \t Loss: 3.7515\t Acc: 0.0000 \t Val Loss: 7.5167\t Val Acc: 0.0000\n",
      "[Epoch   8/1000] \t Loss: 3.7333\t Acc: 0.0000 \t Val Loss: 7.4697\t Val Acc: 0.0000\n",
      "[Epoch   9/1000] \t Loss: 3.7025\t Acc: 0.0000 \t Val Loss: 7.3800\t Val Acc: 0.0000\n",
      "[Epoch  10/1000] \t Loss: 3.6910\t Acc: 0.0000 \t Val Loss: 7.3744\t Val Acc: 0.0000\n",
      "[Epoch  11/1000] \t Loss: 3.6825\t Acc: 0.0000 \t Val Loss: 7.3449\t Val Acc: 0.0000\n",
      "[Epoch  12/1000] \t Loss: 3.6506\t Acc: 0.0000 \t Val Loss: 7.2546\t Val Acc: 0.0000\n",
      "[Epoch  13/1000] \t Loss: 3.6353\t Acc: 0.0000 \t Val Loss: 7.2245\t Val Acc: 0.0000\n",
      "[Epoch  14/1000] \t Loss: 3.5995\t Acc: 0.0000 \t Val Loss: 7.1896\t Val Acc: 0.0000\n",
      "[Epoch  15/1000] \t Loss: 3.5914\t Acc: 0.0000 \t Val Loss: 7.1438\t Val Acc: 0.0000\n",
      "[Epoch  16/1000] \t Loss: 3.5329\t Acc: 0.0000 \t Val Loss: 7.1139\t Val Acc: 0.0000\n",
      "[Epoch  17/1000] \t Loss: 3.5490\t Acc: 0.0000 \t Val Loss: 7.0189\t Val Acc: 0.0000\n",
      "[Epoch  18/1000] \t Loss: 3.4892\t Acc: 0.0000 \t Val Loss: 6.9184\t Val Acc: 0.0000\n",
      "[Epoch  19/1000] \t Loss: 3.4612\t Acc: 0.0000 \t Val Loss: 6.8555\t Val Acc: 0.0000\n",
      "[Epoch  20/1000] \t Loss: 3.4019\t Acc: 0.0000 \t Val Loss: 6.7102\t Val Acc: 0.0000\n",
      "[Epoch  21/1000] \t Loss: 3.3885\t Acc: 0.0000 \t Val Loss: 6.6256\t Val Acc: 0.0000\n",
      "[Epoch  22/1000] \t Loss: 3.3481\t Acc: 0.0000 \t Val Loss: 6.5526\t Val Acc: 0.0000\n",
      "[Epoch  23/1000] \t Loss: 3.2799\t Acc: 0.0000 \t Val Loss: 6.5630\t Val Acc: 0.0000\n",
      "[Epoch  24/1000] \t Loss: 3.2083\t Acc: 0.0000 \t Val Loss: 6.3979\t Val Acc: 0.0000\n",
      "[Epoch  25/1000] \t Loss: 3.1902\t Acc: 0.0000 \t Val Loss: 6.3777\t Val Acc: 0.0000\n",
      "[Epoch  26/1000] \t Loss: 3.1222\t Acc: 0.0000 \t Val Loss: 6.2986\t Val Acc: 0.0000\n",
      "[Epoch  27/1000] \t Loss: 3.0583\t Acc: 0.0000 \t Val Loss: 6.2120\t Val Acc: 0.0000\n",
      "[Epoch  28/1000] \t Loss: 3.0578\t Acc: 0.0000 \t Val Loss: 6.1842\t Val Acc: 0.0000\n",
      "[Epoch  29/1000] \t Loss: 2.9797\t Acc: 0.0000 \t Val Loss: 6.0329\t Val Acc: 0.0000\n",
      "[Epoch  30/1000] \t Loss: 2.9193\t Acc: 0.0000 \t Val Loss: 6.0651\t Val Acc: 0.0000\n",
      "[Epoch  31/1000] \t Loss: 2.8653\t Acc: 0.0000 \t Val Loss: 6.0447\t Val Acc: 0.0000\n",
      "[Epoch  32/1000] \t Loss: 2.8269\t Acc: 0.0000 \t Val Loss: 6.0307\t Val Acc: 0.0000\n",
      "[Epoch  33/1000] \t Loss: 2.7796\t Acc: 0.0000 \t Val Loss: 5.9913\t Val Acc: 0.0000\n",
      "[Epoch  34/1000] \t Loss: 2.7465\t Acc: 0.0000 \t Val Loss: 5.9980\t Val Acc: 0.0000\n",
      "[Epoch  35/1000] \t Loss: 2.7227\t Acc: 0.0000 \t Val Loss: 6.0130\t Val Acc: 0.0000\n",
      "[Epoch  36/1000] \t Loss: 2.7086\t Acc: 0.0000 \t Val Loss: 6.0851\t Val Acc: 0.0000\n",
      "[Epoch  37/1000] \t Loss: 2.6731\t Acc: 0.0000 \t Val Loss: 6.1011\t Val Acc: 0.0000\n",
      "[Epoch  38/1000] \t Loss: 2.6358\t Acc: 0.0000 \t Val Loss: 6.1645\t Val Acc: 0.0000\n",
      "[Epoch  39/1000] \t Loss: 2.6198\t Acc: 0.0000 \t Val Loss: 6.1593\t Val Acc: 0.0000\n",
      "[Epoch  40/1000] \t Loss: 2.5976\t Acc: 0.0000 \t Val Loss: 6.2732\t Val Acc: 0.0000\n",
      "[Epoch  41/1000] \t Loss: 2.6146\t Acc: 0.0000 \t Val Loss: 6.1595\t Val Acc: 0.0000\n",
      "[Epoch  42/1000] \t Loss: 2.5843\t Acc: 0.0000 \t Val Loss: 6.1905\t Val Acc: 0.0000\n",
      "[Epoch  43/1000] \t Loss: 2.5584\t Acc: 0.0000 \t Val Loss: 6.3186\t Val Acc: 0.0000\n",
      "[Epoch  44/1000] \t Loss: 2.5914\t Acc: 0.0000 \t Val Loss: 6.4087\t Val Acc: 0.0000\n",
      "[Epoch  45/1000] \t Loss: 2.5324\t Acc: 0.0000 \t Val Loss: 6.2410\t Val Acc: 0.0000\n",
      "[Epoch  46/1000] \t Loss: 2.5251\t Acc: 0.0000 \t Val Loss: 6.3101\t Val Acc: 0.0000\n",
      "[Epoch  47/1000] \t Loss: 2.5014\t Acc: 0.0000 \t Val Loss: 6.3454\t Val Acc: 0.0000\n",
      "[Epoch  48/1000] \t Loss: 2.4850\t Acc: 0.0000 \t Val Loss: 6.2785\t Val Acc: 0.0000\n",
      "[Epoch  49/1000] \t Loss: 2.4536\t Acc: 0.0000 \t Val Loss: 6.5119\t Val Acc: 0.0000\n",
      "[Epoch  50/1000] \t Loss: 2.4668\t Acc: 0.0000 \t Val Loss: 6.5084\t Val Acc: 0.0000\n",
      "[Epoch  51/1000] \t Loss: 2.4695\t Acc: 0.0000 \t Val Loss: 6.5970\t Val Acc: 0.0000\n",
      "[Epoch  52/1000] \t Loss: 2.4296\t Acc: 0.0000 \t Val Loss: 6.4071\t Val Acc: 0.0000\n",
      "[Epoch  53/1000] \t Loss: 2.4272\t Acc: 0.0000 \t Val Loss: 6.3954\t Val Acc: 0.0000\n",
      "[Epoch  54/1000] \t Loss: 2.3928\t Acc: 0.0000 \t Val Loss: 6.3588\t Val Acc: 0.0000\n",
      "[Epoch  55/1000] \t Loss: 2.3768\t Acc: 0.0000 \t Val Loss: 6.5425\t Val Acc: 0.0000\n",
      "[Epoch  56/1000] \t Loss: 2.3602\t Acc: 0.0000 \t Val Loss: 6.5906\t Val Acc: 0.0000\n",
      "[Epoch  57/1000] \t Loss: 2.3803\t Acc: 0.0000 \t Val Loss: 6.4991\t Val Acc: 0.0000\n",
      "[Epoch  58/1000] \t Loss: 2.3395\t Acc: 0.0000 \t Val Loss: 6.4362\t Val Acc: 0.0000\n",
      "[Epoch  59/1000] \t Loss: 2.3296\t Acc: 0.0000 \t Val Loss: 6.5172\t Val Acc: 0.0000\n",
      "[Epoch  60/1000] \t Loss: 2.3060\t Acc: 0.0000 \t Val Loss: 6.7398\t Val Acc: 0.0000\n",
      "[Epoch  61/1000] \t Loss: 2.3076\t Acc: 0.0000 \t Val Loss: 6.7313\t Val Acc: 0.0000\n",
      "[Epoch  62/1000] \t Loss: 2.3363\t Acc: 0.0000 \t Val Loss: 6.5699\t Val Acc: 0.0000\n",
      "[Epoch  63/1000] \t Loss: 2.2893\t Acc: 0.0000 \t Val Loss: 6.5985\t Val Acc: 0.0000\n",
      "[Epoch  64/1000] \t Loss: 2.2273\t Acc: 0.0000 \t Val Loss: 6.4855\t Val Acc: 0.0000\n",
      "[Epoch  65/1000] \t Loss: 2.2245\t Acc: 0.0000 \t Val Loss: 6.6404\t Val Acc: 0.0000\n",
      "[Epoch  66/1000] \t Loss: 2.2423\t Acc: 0.0000 \t Val Loss: 6.7743\t Val Acc: 0.0000\n",
      "[Epoch  67/1000] \t Loss: 2.1868\t Acc: 0.0000 \t Val Loss: 6.5721\t Val Acc: 0.0000\n",
      "[Epoch  68/1000] \t Loss: 2.2033\t Acc: 0.0000 \t Val Loss: 6.7473\t Val Acc: 0.0000\n",
      "[Epoch  69/1000] \t Loss: 2.1431\t Acc: 0.0000 \t Val Loss: 6.7422\t Val Acc: 0.0000\n",
      "[Epoch  70/1000] \t Loss: 2.1419\t Acc: 0.0000 \t Val Loss: 6.8285\t Val Acc: 0.0000\n",
      "[Epoch  71/1000] \t Loss: 2.1854\t Acc: 0.0000 \t Val Loss: 6.8555\t Val Acc: 0.0000\n",
      "[Epoch  72/1000] \t Loss: 2.1302\t Acc: 0.0000 \t Val Loss: 6.6281\t Val Acc: 0.0000\n",
      "[Epoch  73/1000] \t Loss: 2.1483\t Acc: 0.0000 \t Val Loss: 6.8015\t Val Acc: 0.0000\n",
      "[Epoch  74/1000] \t Loss: 2.1553\t Acc: 0.0000 \t Val Loss: 6.9547\t Val Acc: 0.0000\n",
      "[Epoch  75/1000] \t Loss: 2.1502\t Acc: 0.0000 \t Val Loss: 6.9103\t Val Acc: 0.0000\n",
      "[Epoch  76/1000] \t Loss: 2.0784\t Acc: 0.0000 \t Val Loss: 6.8595\t Val Acc: 0.0000\n",
      "[Epoch  77/1000] \t Loss: 2.0740\t Acc: 0.0000 \t Val Loss: 6.9958\t Val Acc: 0.0000\n",
      "[Epoch  78/1000] \t Loss: 2.0750\t Acc: 0.0000 \t Val Loss: 6.8933\t Val Acc: 0.0000\n",
      "[Epoch  79/1000] \t Loss: 2.1535\t Acc: 0.0000 \t Val Loss: 7.1204\t Val Acc: 0.0000\n",
      "[Epoch  80/1000] \t Loss: 2.0778\t Acc: 0.0000 \t Val Loss: 6.8086\t Val Acc: 0.0000\n",
      "[Epoch  81/1000] \t Loss: 2.0399\t Acc: 0.0000 \t Val Loss: 6.7013\t Val Acc: 0.0000\n",
      "[Epoch  82/1000] \t Loss: 1.9948\t Acc: 0.0000 \t Val Loss: 7.0873\t Val Acc: 0.0000\n",
      "[Epoch  83/1000] \t Loss: 2.0374\t Acc: 0.0000 \t Val Loss: 7.1929\t Val Acc: 0.0000\n",
      "[Epoch  84/1000] \t Loss: 2.0087\t Acc: 0.0000 \t Val Loss: 6.9508\t Val Acc: 0.0000\n",
      "[Epoch  85/1000] \t Loss: 2.0542\t Acc: 0.0000 \t Val Loss: 6.9196\t Val Acc: 0.0000\n",
      "[Epoch  86/1000] \t Loss: 1.9466\t Acc: 0.0000 \t Val Loss: 7.0749\t Val Acc: 0.0000\n",
      "[Epoch  87/1000] \t Loss: 2.0094\t Acc: 0.0000 \t Val Loss: 6.9520\t Val Acc: 0.0000\n",
      "[Epoch  88/1000] \t Loss: 1.9368\t Acc: 0.0000 \t Val Loss: 7.3834\t Val Acc: 0.0000\n",
      "[Epoch  89/1000] \t Loss: 1.9228\t Acc: 0.0000 \t Val Loss: 6.9174\t Val Acc: 0.0000\n",
      "[Epoch  90/1000] \t Loss: 2.0185\t Acc: 0.0000 \t Val Loss: 7.3872\t Val Acc: 0.0000\n",
      "[Epoch  91/1000] \t Loss: 1.9764\t Acc: 0.0000 \t Val Loss: 7.1422\t Val Acc: 0.0000\n",
      "[Epoch  92/1000] \t Loss: 1.9059\t Acc: 0.0000 \t Val Loss: 7.2504\t Val Acc: 0.0000\n",
      "[Epoch  93/1000] \t Loss: 1.9588\t Acc: 0.0000 \t Val Loss: 7.2735\t Val Acc: 0.0000\n",
      "[Epoch  94/1000] \t Loss: 1.8937\t Acc: 0.0000 \t Val Loss: 6.9627\t Val Acc: 0.0000\n",
      "[Epoch  95/1000] \t Loss: 1.9424\t Acc: 0.0000 \t Val Loss: 7.3178\t Val Acc: 0.0000\n",
      "[Epoch  96/1000] \t Loss: 1.8904\t Acc: 0.0000 \t Val Loss: 7.0108\t Val Acc: 0.0000\n",
      "[Epoch  97/1000] \t Loss: 1.8778\t Acc: 0.0000 \t Val Loss: 7.1384\t Val Acc: 0.0000\n",
      "[Epoch  98/1000] \t Loss: 1.8531\t Acc: 0.0000 \t Val Loss: 7.1383\t Val Acc: 0.0000\n",
      "[Epoch  99/1000] \t Loss: 1.9148\t Acc: 0.0000 \t Val Loss: 7.3911\t Val Acc: 0.0000\n",
      "[Epoch 100/1000] \t Loss: 1.8813\t Acc: 0.0000 \t Val Loss: 7.1719\t Val Acc: 0.0000\n",
      "[Epoch 101/1000] \t Loss: 1.8025\t Acc: 0.0000 \t Val Loss: 7.1047\t Val Acc: 0.0000\n",
      "[Epoch 102/1000] \t Loss: 1.8201\t Acc: 0.0000 \t Val Loss: 7.2636\t Val Acc: 0.0000\n",
      "[Epoch 103/1000] \t Loss: 1.7848\t Acc: 0.0000 \t Val Loss: 7.2188\t Val Acc: 0.0000\n",
      "[Epoch 104/1000] \t Loss: 1.7764\t Acc: 0.0000 \t Val Loss: 7.4068\t Val Acc: 0.0000\n",
      "[Epoch 105/1000] \t Loss: 1.8254\t Acc: 0.0000 \t Val Loss: 7.4384\t Val Acc: 0.0000\n",
      "[Epoch 106/1000] \t Loss: 1.7649\t Acc: 0.0000 \t Val Loss: 7.0492\t Val Acc: 0.0000\n",
      "[Epoch 107/1000] \t Loss: 1.7667\t Acc: 0.0000 \t Val Loss: 7.3062\t Val Acc: 0.0000\n",
      "[Epoch 108/1000] \t Loss: 1.7491\t Acc: 0.0000 \t Val Loss: 7.2209\t Val Acc: 0.0000\n",
      "[Epoch 109/1000] \t Loss: 1.8298\t Acc: 0.0000 \t Val Loss: 7.4154\t Val Acc: 0.0000\n",
      "[Epoch 110/1000] \t Loss: 1.8540\t Acc: 0.0000 \t Val Loss: 7.5919\t Val Acc: 0.0000\n",
      "[Epoch 111/1000] \t Loss: 1.7284\t Acc: 0.0000 \t Val Loss: 7.3714\t Val Acc: 0.0000\n",
      "[Epoch 112/1000] \t Loss: 1.7622\t Acc: 0.0000 \t Val Loss: 7.2636\t Val Acc: 0.0000\n",
      "[Epoch 113/1000] \t Loss: 1.7231\t Acc: 0.0000 \t Val Loss: 7.3066\t Val Acc: 0.0000\n",
      "[Epoch 114/1000] \t Loss: 1.7317\t Acc: 0.0000 \t Val Loss: 7.5828\t Val Acc: 0.0000\n",
      "[Epoch 115/1000] \t Loss: 1.7022\t Acc: 0.0000 \t Val Loss: 7.2293\t Val Acc: 0.0000\n",
      "[Epoch 116/1000] \t Loss: 1.7173\t Acc: 0.0000 \t Val Loss: 7.2177\t Val Acc: 0.0000\n",
      "[Epoch 117/1000] \t Loss: 1.6516\t Acc: 0.0000 \t Val Loss: 7.3089\t Val Acc: 0.0000\n",
      "[Epoch 118/1000] \t Loss: 1.6438\t Acc: 0.0000 \t Val Loss: 7.4641\t Val Acc: 0.0000\n",
      "[Epoch 119/1000] \t Loss: 1.6689\t Acc: 0.0000 \t Val Loss: 7.3490\t Val Acc: 0.0000\n",
      "[Epoch 120/1000] \t Loss: 1.6349\t Acc: 0.0000 \t Val Loss: 7.6304\t Val Acc: 0.0000\n",
      "[Epoch 121/1000] \t Loss: 1.7240\t Acc: 0.0000 \t Val Loss: 7.3145\t Val Acc: 0.0000\n",
      "[Epoch 122/1000] \t Loss: 1.6420\t Acc: 0.0000 \t Val Loss: 7.4198\t Val Acc: 0.0000\n",
      "[Epoch 123/1000] \t Loss: 1.6229\t Acc: 0.0000 \t Val Loss: 7.8457\t Val Acc: 0.0000\n",
      "[Epoch 124/1000] \t Loss: 1.5887\t Acc: 0.0000 \t Val Loss: 7.5740\t Val Acc: 0.0000\n",
      "[Epoch 125/1000] \t Loss: 1.6064\t Acc: 0.0000 \t Val Loss: 7.3750\t Val Acc: 0.0000\n",
      "[Epoch 126/1000] \t Loss: 1.6651\t Acc: 0.0000 \t Val Loss: 7.4769\t Val Acc: 0.0000\n",
      "[Epoch 127/1000] \t Loss: 1.5888\t Acc: 0.0000 \t Val Loss: 7.3512\t Val Acc: 0.0000\n",
      "[Epoch 128/1000] \t Loss: 1.5748\t Acc: 0.0000 \t Val Loss: 7.5115\t Val Acc: 0.0000\n",
      "[Epoch 129/1000] \t Loss: 1.6300\t Acc: 0.0000 \t Val Loss: 7.3260\t Val Acc: 0.0000\n",
      "[Epoch 130/1000] \t Loss: 1.5861\t Acc: 0.0000 \t Val Loss: 8.0539\t Val Acc: 0.0000\n",
      "[Epoch 131/1000] \t Loss: 1.5526\t Acc: 0.0000 \t Val Loss: 7.8499\t Val Acc: 0.0000\n",
      "[Epoch 132/1000] \t Loss: 1.5499\t Acc: 0.0000 \t Val Loss: 7.9000\t Val Acc: 0.0000\n",
      "[Epoch 133/1000] \t Loss: 1.5298\t Acc: 0.0000 \t Val Loss: 7.6685\t Val Acc: 0.0000\n",
      "[Epoch 134/1000] \t Loss: 1.5497\t Acc: 0.0000 \t Val Loss: 7.9733\t Val Acc: 0.0000\n",
      "[Epoch 135/1000] \t Loss: 1.4983\t Acc: 0.0000 \t Val Loss: 7.8175\t Val Acc: 0.0000\n",
      "[Epoch 136/1000] \t Loss: 1.4992\t Acc: 0.0000 \t Val Loss: 7.5816\t Val Acc: 0.0000\n",
      "[Epoch 137/1000] \t Loss: 1.4883\t Acc: 0.0000 \t Val Loss: 7.7753\t Val Acc: 0.0000\n",
      "[Epoch 138/1000] \t Loss: 1.5235\t Acc: 0.0000 \t Val Loss: 7.7421\t Val Acc: 0.0000\n",
      "[Epoch 139/1000] \t Loss: 1.5328\t Acc: 0.0000 \t Val Loss: 7.8085\t Val Acc: 0.0000\n",
      "[Epoch 140/1000] \t Loss: 1.4939\t Acc: 0.0000 \t Val Loss: 7.7403\t Val Acc: 0.0000\n",
      "[Epoch 141/1000] \t Loss: 1.4699\t Acc: 0.0000 \t Val Loss: 7.8170\t Val Acc: 0.0000\n",
      "[Epoch 142/1000] \t Loss: 1.5102\t Acc: 0.0000 \t Val Loss: 8.3008\t Val Acc: 0.0000\n",
      "[Epoch 143/1000] \t Loss: 1.4763\t Acc: 0.0000 \t Val Loss: 7.8076\t Val Acc: 0.0000\n",
      "[Epoch 144/1000] \t Loss: 1.4635\t Acc: 0.0000 \t Val Loss: 7.6317\t Val Acc: 0.0000\n",
      "[Epoch 145/1000] \t Loss: 1.4477\t Acc: 0.0000 \t Val Loss: 7.6975\t Val Acc: 0.0000\n",
      "[Epoch 146/1000] \t Loss: 1.4338\t Acc: 0.0000 \t Val Loss: 7.7226\t Val Acc: 0.0000\n",
      "[Epoch 147/1000] \t Loss: 1.4343\t Acc: 0.0000 \t Val Loss: 8.1073\t Val Acc: 0.0000\n",
      "[Epoch 148/1000] \t Loss: 1.4368\t Acc: 0.0000 \t Val Loss: 7.9378\t Val Acc: 0.0000\n",
      "[Epoch 149/1000] \t Loss: 1.4128\t Acc: 0.0000 \t Val Loss: 7.7470\t Val Acc: 0.0000\n",
      "[Epoch 150/1000] \t Loss: 1.4266\t Acc: 0.0000 \t Val Loss: 8.0538\t Val Acc: 0.0000\n",
      "[Epoch 151/1000] \t Loss: 1.3997\t Acc: 0.0000 \t Val Loss: 8.1938\t Val Acc: 0.0000\n",
      "[Epoch 152/1000] \t Loss: 1.3854\t Acc: 0.0000 \t Val Loss: 7.8733\t Val Acc: 0.0000\n",
      "[Epoch 153/1000] \t Loss: 1.4441\t Acc: 0.0000 \t Val Loss: 7.9251\t Val Acc: 0.0000\n",
      "[Epoch 154/1000] \t Loss: 1.3762\t Acc: 0.0000 \t Val Loss: 8.0847\t Val Acc: 0.0000\n",
      "[Epoch 155/1000] \t Loss: 1.4747\t Acc: 0.0000 \t Val Loss: 8.1327\t Val Acc: 0.0000\n",
      "[Epoch 156/1000] \t Loss: 1.3617\t Acc: 0.0000 \t Val Loss: 7.7620\t Val Acc: 0.0000\n",
      "[Epoch 157/1000] \t Loss: 1.3391\t Acc: 0.0000 \t Val Loss: 7.8920\t Val Acc: 0.0000\n",
      "[Epoch 158/1000] \t Loss: 1.3461\t Acc: 0.0000 \t Val Loss: 8.1563\t Val Acc: 0.0000\n",
      "[Epoch 159/1000] \t Loss: 1.3426\t Acc: 0.0000 \t Val Loss: 7.8953\t Val Acc: 0.0000\n",
      "[Epoch 160/1000] \t Loss: 1.3278\t Acc: 0.0000 \t Val Loss: 7.8509\t Val Acc: 0.0000\n",
      "[Epoch 161/1000] \t Loss: 1.3440\t Acc: 0.0000 \t Val Loss: 7.9154\t Val Acc: 0.0000\n",
      "[Epoch 162/1000] \t Loss: 1.3571\t Acc: 0.0000 \t Val Loss: 8.0348\t Val Acc: 0.0000\n",
      "[Epoch 163/1000] \t Loss: 1.2995\t Acc: 0.0000 \t Val Loss: 7.9417\t Val Acc: 0.0000\n",
      "[Epoch 164/1000] \t Loss: 1.3009\t Acc: 0.0000 \t Val Loss: 7.9386\t Val Acc: 0.0000\n",
      "[Epoch 165/1000] \t Loss: 1.2970\t Acc: 0.0000 \t Val Loss: 8.2537\t Val Acc: 0.0000\n",
      "[Epoch 166/1000] \t Loss: 1.3378\t Acc: 0.0000 \t Val Loss: 7.8858\t Val Acc: 0.0000\n",
      "[Epoch 167/1000] \t Loss: 1.2735\t Acc: 0.0000 \t Val Loss: 8.1495\t Val Acc: 0.0000\n",
      "[Epoch 168/1000] \t Loss: 1.2869\t Acc: 0.0000 \t Val Loss: 8.1159\t Val Acc: 0.0000\n",
      "[Epoch 169/1000] \t Loss: 1.3033\t Acc: 0.0000 \t Val Loss: 8.1676\t Val Acc: 0.0000\n",
      "[Epoch 170/1000] \t Loss: 1.2521\t Acc: 0.0000 \t Val Loss: 7.9746\t Val Acc: 0.0000\n",
      "[Epoch 171/1000] \t Loss: 1.2422\t Acc: 0.0000 \t Val Loss: 8.0680\t Val Acc: 0.0000\n",
      "[Epoch 172/1000] \t Loss: 1.2788\t Acc: 0.0000 \t Val Loss: 8.2770\t Val Acc: 0.0000\n",
      "[Epoch 173/1000] \t Loss: 1.2773\t Acc: 0.0000 \t Val Loss: 8.2712\t Val Acc: 0.0000\n",
      "[Epoch 174/1000] \t Loss: 1.2511\t Acc: 0.0000 \t Val Loss: 8.2423\t Val Acc: 0.0000\n",
      "[Epoch 175/1000] \t Loss: 1.2525\t Acc: 0.0000 \t Val Loss: 8.0992\t Val Acc: 0.0000\n",
      "[Epoch 176/1000] \t Loss: 1.2799\t Acc: 0.0000 \t Val Loss: 7.8707\t Val Acc: 0.0000\n",
      "[Epoch 177/1000] \t Loss: 1.2592\t Acc: 0.0000 \t Val Loss: 8.0498\t Val Acc: 0.0000\n",
      "[Epoch 178/1000] \t Loss: 1.3216\t Acc: 0.0000 \t Val Loss: 7.9377\t Val Acc: 0.0000\n",
      "[Epoch 179/1000] \t Loss: 1.2736\t Acc: 0.0000 \t Val Loss: 8.4621\t Val Acc: 0.0000\n",
      "[Epoch 180/1000] \t Loss: 1.2239\t Acc: 0.0000 \t Val Loss: 8.7125\t Val Acc: 0.0000\n",
      "[Epoch 181/1000] \t Loss: 1.2812\t Acc: 0.0000 \t Val Loss: 8.0306\t Val Acc: 0.0000\n",
      "[Epoch 182/1000] \t Loss: 1.1898\t Acc: 0.0000 \t Val Loss: 8.3880\t Val Acc: 0.0000\n",
      "[Epoch 183/1000] \t Loss: 1.2455\t Acc: 0.0000 \t Val Loss: 8.5128\t Val Acc: 0.0000\n",
      "[Epoch 184/1000] \t Loss: 1.1873\t Acc: 0.0000 \t Val Loss: 7.8747\t Val Acc: 0.0000\n",
      "[Epoch 185/1000] \t Loss: 1.1739\t Acc: 0.0000 \t Val Loss: 8.5230\t Val Acc: 0.0000\n",
      "[Epoch 186/1000] \t Loss: 1.2219\t Acc: 0.0000 \t Val Loss: 8.4624\t Val Acc: 0.0000\n",
      "[Epoch 187/1000] \t Loss: 1.1811\t Acc: 0.0000 \t Val Loss: 8.3921\t Val Acc: 0.0000\n",
      "[Epoch 188/1000] \t Loss: 1.1772\t Acc: 0.0000 \t Val Loss: 8.2831\t Val Acc: 0.0000\n",
      "[Epoch 189/1000] \t Loss: 1.1823\t Acc: 0.0000 \t Val Loss: 8.3841\t Val Acc: 0.0000\n",
      "[Epoch 190/1000] \t Loss: 1.2044\t Acc: 0.0000 \t Val Loss: 8.2499\t Val Acc: 0.0000\n",
      "[Epoch 191/1000] \t Loss: 1.1918\t Acc: 0.0000 \t Val Loss: 8.4009\t Val Acc: 0.0000\n",
      "[Epoch 192/1000] \t Loss: 1.1518\t Acc: 0.0000 \t Val Loss: 8.5988\t Val Acc: 0.0000\n",
      "[Epoch 193/1000] \t Loss: 1.1451\t Acc: 0.0000 \t Val Loss: 8.3351\t Val Acc: 0.0000\n",
      "[Epoch 194/1000] \t Loss: 1.1521\t Acc: 0.0000 \t Val Loss: 8.4902\t Val Acc: 0.0000\n",
      "[Epoch 195/1000] \t Loss: 1.1582\t Acc: 0.0000 \t Val Loss: 8.4779\t Val Acc: 0.0000\n",
      "[Epoch 196/1000] \t Loss: 1.1248\t Acc: 33.3333 \t Val Loss: 8.3610\t Val Acc: 0.0000\n",
      "[Epoch 197/1000] \t Loss: 1.1181\t Acc: 0.0000 \t Val Loss: 8.4902\t Val Acc: 0.0000\n",
      "[Epoch 198/1000] \t Loss: 1.1030\t Acc: 0.0000 \t Val Loss: 8.3883\t Val Acc: 0.0000\n",
      "[Epoch 199/1000] \t Loss: 1.1259\t Acc: 0.0000 \t Val Loss: 8.5962\t Val Acc: 0.0000\n",
      "[Epoch 200/1000] \t Loss: 1.1238\t Acc: 0.0000 \t Val Loss: 8.4582\t Val Acc: 0.0000\n",
      "[Epoch 201/1000] \t Loss: 1.1215\t Acc: 0.0000 \t Val Loss: 8.6501\t Val Acc: 0.0000\n",
      "[Epoch 202/1000] \t Loss: 1.1314\t Acc: 33.3333 \t Val Loss: 8.6354\t Val Acc: 0.0000\n",
      "[Epoch 203/1000] \t Loss: 1.2183\t Acc: 0.0000 \t Val Loss: 8.9628\t Val Acc: 0.0000\n",
      "[Epoch 204/1000] \t Loss: 1.1137\t Acc: 0.0000 \t Val Loss: 8.4204\t Val Acc: 0.0000\n",
      "[Epoch 205/1000] \t Loss: 1.1752\t Acc: 0.0000 \t Val Loss: 8.5507\t Val Acc: 0.0000\n",
      "[Epoch 206/1000] \t Loss: 1.0906\t Acc: 0.0000 \t Val Loss: 8.4462\t Val Acc: 0.0000\n",
      "[Epoch 207/1000] \t Loss: 1.0875\t Acc: 0.0000 \t Val Loss: 8.7048\t Val Acc: 0.0000\n",
      "[Epoch 208/1000] \t Loss: 1.0867\t Acc: 0.0000 \t Val Loss: 8.5378\t Val Acc: 0.0000\n",
      "[Epoch 209/1000] \t Loss: 1.1236\t Acc: 0.0000 \t Val Loss: 8.5719\t Val Acc: 0.0000\n",
      "[Epoch 210/1000] \t Loss: 1.0978\t Acc: 0.0000 \t Val Loss: 8.9161\t Val Acc: 0.0000\n",
      "[Epoch 211/1000] \t Loss: 1.1693\t Acc: 0.0000 \t Val Loss: 9.0187\t Val Acc: 0.0000\n",
      "[Epoch 212/1000] \t Loss: 1.0827\t Acc: 0.0000 \t Val Loss: 8.3821\t Val Acc: 0.0000\n",
      "[Epoch 213/1000] \t Loss: 1.0854\t Acc: 33.3333 \t Val Loss: 8.6718\t Val Acc: 0.0000\n",
      "[Epoch 214/1000] \t Loss: 1.0621\t Acc: 0.0000 \t Val Loss: 8.7417\t Val Acc: 0.0000\n",
      "[Epoch 215/1000] \t Loss: 1.0448\t Acc: 33.3333 \t Val Loss: 8.4323\t Val Acc: 0.0000\n",
      "[Epoch 216/1000] \t Loss: 1.0359\t Acc: 0.0000 \t Val Loss: 8.2072\t Val Acc: 0.0000\n",
      "[Epoch 217/1000] \t Loss: 1.0890\t Acc: 0.0000 \t Val Loss: 9.0370\t Val Acc: 0.0000\n",
      "[Epoch 218/1000] \t Loss: 1.0881\t Acc: 0.0000 \t Val Loss: 8.7482\t Val Acc: 0.0000\n",
      "[Epoch 219/1000] \t Loss: 1.0549\t Acc: 33.3333 \t Val Loss: 8.4923\t Val Acc: 0.0000\n",
      "[Epoch 220/1000] \t Loss: 1.0891\t Acc: 0.0000 \t Val Loss: 8.3209\t Val Acc: 0.0000\n",
      "[Epoch 221/1000] \t Loss: 1.0353\t Acc: 0.0000 \t Val Loss: 8.7433\t Val Acc: 0.0000\n",
      "[Epoch 222/1000] \t Loss: 1.0781\t Acc: 0.0000 \t Val Loss: 8.8606\t Val Acc: 0.0000\n",
      "[Epoch 223/1000] \t Loss: 1.0241\t Acc: 0.0000 \t Val Loss: 8.8600\t Val Acc: 0.0000\n",
      "[Epoch 224/1000] \t Loss: 1.0073\t Acc: 33.3333 \t Val Loss: 8.4652\t Val Acc: 0.0000\n",
      "[Epoch 225/1000] \t Loss: 0.9927\t Acc: 33.3333 \t Val Loss: 8.6340\t Val Acc: 0.0000\n",
      "[Epoch 226/1000] \t Loss: 0.9906\t Acc: 0.0000 \t Val Loss: 8.4329\t Val Acc: 0.0000\n",
      "[Epoch 227/1000] \t Loss: 1.0032\t Acc: 33.3333 \t Val Loss: 8.3527\t Val Acc: 0.0000\n",
      "[Epoch 228/1000] \t Loss: 1.0068\t Acc: 0.0000 \t Val Loss: 8.5872\t Val Acc: 0.0000\n",
      "[Epoch 229/1000] \t Loss: 1.0246\t Acc: 33.3333 \t Val Loss: 8.4554\t Val Acc: 0.0000\n",
      "[Epoch 230/1000] \t Loss: 1.0006\t Acc: 66.6667 \t Val Loss: 8.6551\t Val Acc: 0.0000\n",
      "[Epoch 231/1000] \t Loss: 0.9912\t Acc: 33.3333 \t Val Loss: 8.3569\t Val Acc: 0.0000\n",
      "[Epoch 232/1000] \t Loss: 0.9863\t Acc: 33.3333 \t Val Loss: 9.2206\t Val Acc: 0.0000\n",
      "[Epoch 233/1000] \t Loss: 0.9791\t Acc: 0.0000 \t Val Loss: 8.8102\t Val Acc: 0.0000\n",
      "[Epoch 234/1000] \t Loss: 0.9801\t Acc: 0.0000 \t Val Loss: 8.5043\t Val Acc: 0.0000\n",
      "[Epoch 235/1000] \t Loss: 0.9754\t Acc: 33.3333 \t Val Loss: 8.8468\t Val Acc: 0.0000\n",
      "[Epoch 236/1000] \t Loss: 1.0150\t Acc: 0.0000 \t Val Loss: 8.3879\t Val Acc: 0.0000\n",
      "[Epoch 237/1000] \t Loss: 0.9830\t Acc: 0.0000 \t Val Loss: 8.7379\t Val Acc: 0.0000\n",
      "[Epoch 238/1000] \t Loss: 0.9223\t Acc: 33.3333 \t Val Loss: 8.5719\t Val Acc: 0.0000\n",
      "[Epoch 239/1000] \t Loss: 0.9599\t Acc: 33.3333 \t Val Loss: 8.3676\t Val Acc: 0.0000\n",
      "[Epoch 240/1000] \t Loss: 0.9694\t Acc: 33.3333 \t Val Loss: 8.4279\t Val Acc: 0.0000\n",
      "[Epoch 241/1000] \t Loss: 0.9765\t Acc: 0.0000 \t Val Loss: 8.5907\t Val Acc: 0.0000\n",
      "[Epoch 242/1000] \t Loss: 0.9924\t Acc: 33.3333 \t Val Loss: 8.7371\t Val Acc: 0.0000\n",
      "[Epoch 243/1000] \t Loss: 0.9136\t Acc: 66.6667 \t Val Loss: 8.8602\t Val Acc: 0.0000\n",
      "[Epoch 244/1000] \t Loss: 1.0004\t Acc: 0.0000 \t Val Loss: 8.4743\t Val Acc: 0.0000\n",
      "[Epoch 245/1000] \t Loss: 0.9664\t Acc: 33.3333 \t Val Loss: 8.6982\t Val Acc: 0.0000\n",
      "[Epoch 246/1000] \t Loss: 0.9425\t Acc: 0.0000 \t Val Loss: 9.1245\t Val Acc: 0.0000\n",
      "[Epoch 247/1000] \t Loss: 0.9788\t Acc: 0.0000 \t Val Loss: 8.6618\t Val Acc: 0.0000\n",
      "[Epoch 248/1000] \t Loss: 0.9396\t Acc: 33.3333 \t Val Loss: 8.5947\t Val Acc: 0.0000\n",
      "[Epoch 249/1000] \t Loss: 0.9315\t Acc: 0.0000 \t Val Loss: 9.0657\t Val Acc: 0.0000\n",
      "[Epoch 250/1000] \t Loss: 0.9347\t Acc: 33.3333 \t Val Loss: 8.9533\t Val Acc: 0.0000\n",
      "[Epoch 251/1000] \t Loss: 0.9223\t Acc: 33.3333 \t Val Loss: 8.3659\t Val Acc: 0.0000\n",
      "[Epoch 252/1000] \t Loss: 0.9654\t Acc: 33.3333 \t Val Loss: 8.8071\t Val Acc: 0.0000\n",
      "[Epoch 253/1000] \t Loss: 0.9403\t Acc: 33.3333 \t Val Loss: 8.9823\t Val Acc: 0.0000\n",
      "[Epoch 254/1000] \t Loss: 0.9406\t Acc: 33.3333 \t Val Loss: 8.9739\t Val Acc: 0.0000\n",
      "[Epoch 255/1000] \t Loss: 0.9646\t Acc: 0.0000 \t Val Loss: 8.9087\t Val Acc: 0.0000\n",
      "[Epoch 256/1000] \t Loss: 0.9128\t Acc: 33.3333 \t Val Loss: 8.5358\t Val Acc: 0.0000\n",
      "[Epoch 257/1000] \t Loss: 0.8988\t Acc: 0.0000 \t Val Loss: 8.9222\t Val Acc: 0.0000\n",
      "[Epoch 258/1000] \t Loss: 0.8702\t Acc: 66.6667 \t Val Loss: 8.9146\t Val Acc: 0.0000\n",
      "[Epoch 259/1000] \t Loss: 0.8904\t Acc: 33.3333 \t Val Loss: 8.7382\t Val Acc: 0.0000\n",
      "[Epoch 260/1000] \t Loss: 0.9018\t Acc: 66.6667 \t Val Loss: 8.8524\t Val Acc: 0.0000\n",
      "[Epoch 261/1000] \t Loss: 0.8671\t Acc: 0.0000 \t Val Loss: 8.9482\t Val Acc: 0.0000\n",
      "[Epoch 262/1000] \t Loss: 0.9357\t Acc: 33.3333 \t Val Loss: 9.2333\t Val Acc: 0.0000\n",
      "[Epoch 263/1000] \t Loss: 0.8683\t Acc: 33.3333 \t Val Loss: 8.8606\t Val Acc: 0.0000\n",
      "[Epoch 264/1000] \t Loss: 0.8797\t Acc: 66.6667 \t Val Loss: 8.9906\t Val Acc: 0.0000\n",
      "[Epoch 265/1000] \t Loss: 0.8464\t Acc: 33.3333 \t Val Loss: 9.6681\t Val Acc: 0.0000\n",
      "[Epoch 266/1000] \t Loss: 0.8723\t Acc: 33.3333 \t Val Loss: 8.9692\t Val Acc: 0.0000\n",
      "[Epoch 267/1000] \t Loss: 0.8465\t Acc: 66.6667 \t Val Loss: 8.8378\t Val Acc: 0.0000\n",
      "[Epoch 268/1000] \t Loss: 0.8495\t Acc: 0.0000 \t Val Loss: 9.0212\t Val Acc: 0.0000\n",
      "[Epoch 269/1000] \t Loss: 0.9084\t Acc: 0.0000 \t Val Loss: 8.8443\t Val Acc: 0.0000\n",
      "[Epoch 270/1000] \t Loss: 0.8486\t Acc: 0.0000 \t Val Loss: 9.2714\t Val Acc: 0.0000\n",
      "[Epoch 271/1000] \t Loss: 0.8699\t Acc: 33.3333 \t Val Loss: 9.4897\t Val Acc: 0.0000\n",
      "[Epoch 272/1000] \t Loss: 0.8504\t Acc: 66.6667 \t Val Loss: 8.7749\t Val Acc: 0.0000\n",
      "[Epoch 273/1000] \t Loss: 0.8461\t Acc: 33.3333 \t Val Loss: 8.8480\t Val Acc: 0.0000\n",
      "[Epoch 274/1000] \t Loss: 0.8210\t Acc: 66.6667 \t Val Loss: 8.8775\t Val Acc: 0.0000\n",
      "[Epoch 275/1000] \t Loss: 0.8971\t Acc: 33.3333 \t Val Loss: 9.2411\t Val Acc: 0.0000\n",
      "[Epoch 276/1000] \t Loss: 0.8299\t Acc: 33.3333 \t Val Loss: 8.8369\t Val Acc: 0.0000\n",
      "[Epoch 277/1000] \t Loss: 0.8530\t Acc: 33.3333 \t Val Loss: 9.0575\t Val Acc: 0.0000\n",
      "[Epoch 278/1000] \t Loss: 0.8333\t Acc: 33.3333 \t Val Loss: 8.6890\t Val Acc: 0.0000\n",
      "[Epoch 279/1000] \t Loss: 0.7851\t Acc: 66.6667 \t Val Loss: 8.9829\t Val Acc: 0.0000\n",
      "[Epoch 280/1000] \t Loss: 0.8076\t Acc: 33.3333 \t Val Loss: 9.2825\t Val Acc: 0.0000\n",
      "[Epoch 281/1000] \t Loss: 0.8253\t Acc: 66.6667 \t Val Loss: 9.0677\t Val Acc: 0.0000\n",
      "[Epoch 282/1000] \t Loss: 0.8562\t Acc: 33.3333 \t Val Loss: 9.2700\t Val Acc: 0.0000\n",
      "[Epoch 283/1000] \t Loss: 0.8499\t Acc: 33.3333 \t Val Loss: 9.2064\t Val Acc: 0.0000\n",
      "[Epoch 284/1000] \t Loss: 0.8973\t Acc: 33.3333 \t Val Loss: 9.1458\t Val Acc: 0.0000\n",
      "[Epoch 285/1000] \t Loss: 0.8794\t Acc: 33.3333 \t Val Loss: 9.6109\t Val Acc: 0.0000\n",
      "[Epoch 286/1000] \t Loss: 0.7799\t Acc: 66.6667 \t Val Loss: 8.9015\t Val Acc: 0.0000\n",
      "[Epoch 287/1000] \t Loss: 0.8285\t Acc: 33.3333 \t Val Loss: 8.6314\t Val Acc: 0.0000\n",
      "[Epoch 288/1000] \t Loss: 0.8140\t Acc: 66.6667 \t Val Loss: 8.9623\t Val Acc: 0.0000\n",
      "[Epoch 289/1000] \t Loss: 0.7915\t Acc: 33.3333 \t Val Loss: 8.9815\t Val Acc: 0.0000\n",
      "[Epoch 290/1000] \t Loss: 0.7723\t Acc: 33.3333 \t Val Loss: 9.0032\t Val Acc: 0.0000\n",
      "[Epoch 291/1000] \t Loss: 0.8845\t Acc: 0.0000 \t Val Loss: 9.5384\t Val Acc: 0.0000\n",
      "[Epoch 292/1000] \t Loss: 0.8240\t Acc: 0.0000 \t Val Loss: 9.3208\t Val Acc: 0.0000\n",
      "[Epoch 293/1000] \t Loss: 0.7558\t Acc: 33.3333 \t Val Loss: 9.0489\t Val Acc: 0.0000\n",
      "[Epoch 294/1000] \t Loss: 0.8421\t Acc: 33.3333 \t Val Loss: 8.9537\t Val Acc: 0.0000\n",
      "[Epoch 295/1000] \t Loss: 0.8030\t Acc: 33.3333 \t Val Loss: 9.1037\t Val Acc: 0.0000\n",
      "[Epoch 296/1000] \t Loss: 0.8194\t Acc: 0.0000 \t Val Loss: 9.3050\t Val Acc: 0.0000\n",
      "[Epoch 297/1000] \t Loss: 0.8298\t Acc: 33.3333 \t Val Loss: 9.0169\t Val Acc: 0.0000\n",
      "[Epoch 298/1000] \t Loss: 0.7963\t Acc: 0.0000 \t Val Loss: 9.4012\t Val Acc: 0.0000\n",
      "[Epoch 299/1000] \t Loss: 0.7618\t Acc: 33.3333 \t Val Loss: 8.9337\t Val Acc: 0.0000\n",
      "[Epoch 300/1000] \t Loss: 0.8632\t Acc: 33.3333 \t Val Loss: 9.2807\t Val Acc: 0.0000\n",
      "[Epoch 301/1000] \t Loss: 0.7686\t Acc: 66.6667 \t Val Loss: 9.4721\t Val Acc: 0.0000\n",
      "[Epoch 302/1000] \t Loss: 0.7843\t Acc: 66.6667 \t Val Loss: 9.0426\t Val Acc: 0.0000\n",
      "[Epoch 303/1000] \t Loss: 0.7595\t Acc: 33.3333 \t Val Loss: 9.3603\t Val Acc: 0.0000\n",
      "[Epoch 304/1000] \t Loss: 0.7681\t Acc: 33.3333 \t Val Loss: 9.1279\t Val Acc: 0.0000\n",
      "[Epoch 305/1000] \t Loss: 0.7953\t Acc: 0.0000 \t Val Loss: 9.3985\t Val Acc: 0.0000\n",
      "[Epoch 306/1000] \t Loss: 0.8183\t Acc: 33.3333 \t Val Loss: 9.4437\t Val Acc: 0.0000\n",
      "[Epoch 307/1000] \t Loss: 0.7264\t Acc: 66.6667 \t Val Loss: 9.2906\t Val Acc: 0.0000\n",
      "[Epoch 308/1000] \t Loss: 0.7020\t Acc: 66.6667 \t Val Loss: 8.9929\t Val Acc: 0.0000\n",
      "[Epoch 309/1000] \t Loss: 0.7374\t Acc: 33.3333 \t Val Loss: 9.6330\t Val Acc: 0.0000\n",
      "[Epoch 310/1000] \t Loss: 0.8547\t Acc: 33.3333 \t Val Loss: 9.4382\t Val Acc: 0.0000\n",
      "[Epoch 311/1000] \t Loss: 0.8256\t Acc: 33.3333 \t Val Loss: 9.6488\t Val Acc: 0.0000\n",
      "[Epoch 312/1000] \t Loss: 0.7365\t Acc: 66.6667 \t Val Loss: 8.9785\t Val Acc: 0.0000\n",
      "[Epoch 313/1000] \t Loss: 0.7332\t Acc: 66.6667 \t Val Loss: 9.0017\t Val Acc: 0.0000\n",
      "[Epoch 314/1000] \t Loss: 0.7321\t Acc: 66.6667 \t Val Loss: 9.3121\t Val Acc: 0.0000\n",
      "[Epoch 315/1000] \t Loss: 0.7152\t Acc: 66.6667 \t Val Loss: 9.1214\t Val Acc: 0.0000\n",
      "[Epoch 316/1000] \t Loss: 0.7274\t Acc: 66.6667 \t Val Loss: 9.4195\t Val Acc: 0.0000\n",
      "[Epoch 317/1000] \t Loss: 0.6721\t Acc: 66.6667 \t Val Loss: 9.0712\t Val Acc: 0.0000\n",
      "[Epoch 318/1000] \t Loss: 0.7732\t Acc: 66.6667 \t Val Loss: 9.5049\t Val Acc: 0.0000\n",
      "[Epoch 319/1000] \t Loss: 0.6952\t Acc: 66.6667 \t Val Loss: 9.2352\t Val Acc: 0.0000\n",
      "[Epoch 320/1000] \t Loss: 0.7076\t Acc: 33.3333 \t Val Loss: 8.8989\t Val Acc: 0.0000\n",
      "[Epoch 321/1000] \t Loss: 0.7006\t Acc: 0.0000 \t Val Loss: 9.4973\t Val Acc: 0.0000\n",
      "[Epoch 322/1000] \t Loss: 0.8116\t Acc: 0.0000 \t Val Loss: 9.1294\t Val Acc: 0.0000\n",
      "[Epoch 323/1000] \t Loss: 0.7903\t Acc: 33.3333 \t Val Loss: 9.3618\t Val Acc: 0.0000\n",
      "[Epoch 324/1000] \t Loss: 0.7055\t Acc: 66.6667 \t Val Loss: 9.0463\t Val Acc: 0.0000\n",
      "[Epoch 325/1000] \t Loss: 0.7805\t Acc: 0.0000 \t Val Loss: 9.6323\t Val Acc: 0.0000\n",
      "[Epoch 326/1000] \t Loss: 0.6820\t Acc: 33.3333 \t Val Loss: 9.3434\t Val Acc: 0.0000\n",
      "[Epoch 327/1000] \t Loss: 0.7157\t Acc: 33.3333 \t Val Loss: 9.1431\t Val Acc: 0.0000\n",
      "[Epoch 328/1000] \t Loss: 0.7044\t Acc: 66.6667 \t Val Loss: 8.9350\t Val Acc: 0.0000\n",
      "[Epoch 329/1000] \t Loss: 0.7395\t Acc: 33.3333 \t Val Loss: 9.4002\t Val Acc: 0.0000\n",
      "[Epoch 330/1000] \t Loss: 0.8001\t Acc: 33.3333 \t Val Loss: 9.7094\t Val Acc: 0.0000\n",
      "[Epoch 331/1000] \t Loss: 0.6600\t Acc: 33.3333 \t Val Loss: 9.7548\t Val Acc: 0.0000\n",
      "[Epoch 332/1000] \t Loss: 0.6896\t Acc: 66.6667 \t Val Loss: 9.0347\t Val Acc: 0.0000\n",
      "[Epoch 333/1000] \t Loss: 0.7139\t Acc: 33.3333 \t Val Loss: 9.5807\t Val Acc: 0.0000\n",
      "[Epoch 334/1000] \t Loss: 0.7139\t Acc: 33.3333 \t Val Loss: 9.4462\t Val Acc: 0.0000\n",
      "[Epoch 335/1000] \t Loss: 0.6886\t Acc: 66.6667 \t Val Loss: 9.7740\t Val Acc: 0.0000\n",
      "[Epoch 336/1000] \t Loss: 0.6947\t Acc: 33.3333 \t Val Loss: 10.1205\t Val Acc: 0.0000\n",
      "[Epoch 337/1000] \t Loss: 0.7038\t Acc: 33.3333 \t Val Loss: 9.1450\t Val Acc: 0.0000\n",
      "[Epoch 338/1000] \t Loss: 0.6794\t Acc: 66.6667 \t Val Loss: 9.6132\t Val Acc: 0.0000\n",
      "[Epoch 339/1000] \t Loss: 0.6889\t Acc: 66.6667 \t Val Loss: 9.6865\t Val Acc: 0.0000\n",
      "[Epoch 340/1000] \t Loss: 0.6335\t Acc: 33.3333 \t Val Loss: 8.8312\t Val Acc: 0.0000\n",
      "[Epoch 341/1000] \t Loss: 0.6698\t Acc: 66.6667 \t Val Loss: 9.3986\t Val Acc: 0.0000\n",
      "[Epoch 342/1000] \t Loss: 0.7079\t Acc: 33.3333 \t Val Loss: 9.2237\t Val Acc: 0.0000\n",
      "[Epoch 343/1000] \t Loss: 0.6579\t Acc: 66.6667 \t Val Loss: 9.7700\t Val Acc: 0.0000\n",
      "[Epoch 344/1000] \t Loss: 0.6600\t Acc: 66.6667 \t Val Loss: 9.3325\t Val Acc: 0.0000\n",
      "[Epoch 345/1000] \t Loss: 0.6675\t Acc: 66.6667 \t Val Loss: 9.7084\t Val Acc: 0.0000\n",
      "[Epoch 346/1000] \t Loss: 0.6745\t Acc: 33.3333 \t Val Loss: 9.9028\t Val Acc: 0.0000\n",
      "[Epoch 347/1000] \t Loss: 0.6512\t Acc: 66.6667 \t Val Loss: 9.4401\t Val Acc: 0.0000\n",
      "[Epoch 348/1000] \t Loss: 0.6366\t Acc: 66.6667 \t Val Loss: 9.7961\t Val Acc: 0.0000\n",
      "[Epoch 349/1000] \t Loss: 0.6737\t Acc: 66.6667 \t Val Loss: 9.3639\t Val Acc: 0.0000\n",
      "[Epoch 350/1000] \t Loss: 0.6596\t Acc: 33.3333 \t Val Loss: 9.6949\t Val Acc: 0.0000\n",
      "[Epoch 351/1000] \t Loss: 0.6289\t Acc: 66.6667 \t Val Loss: 9.5114\t Val Acc: 0.0000\n",
      "[Epoch 352/1000] \t Loss: 0.6326\t Acc: 33.3333 \t Val Loss: 9.1874\t Val Acc: 0.0000\n",
      "[Epoch 353/1000] \t Loss: 0.7331\t Acc: 33.3333 \t Val Loss: 9.5769\t Val Acc: 0.0000\n",
      "[Epoch 354/1000] \t Loss: 0.6818\t Acc: 33.3333 \t Val Loss: 9.7173\t Val Acc: 0.0000\n",
      "[Epoch 355/1000] \t Loss: 0.5791\t Acc: 66.6667 \t Val Loss: 9.6190\t Val Acc: 0.0000\n",
      "[Epoch 356/1000] \t Loss: 0.6512\t Acc: 66.6667 \t Val Loss: 9.4417\t Val Acc: 0.0000\n",
      "[Epoch 357/1000] \t Loss: 0.6857\t Acc: 66.6667 \t Val Loss: 9.2070\t Val Acc: 0.0000\n",
      "[Epoch 358/1000] \t Loss: 0.6543\t Acc: 0.0000 \t Val Loss: 9.8069\t Val Acc: 0.0000\n",
      "[Epoch 359/1000] \t Loss: 0.5894\t Acc: 66.6667 \t Val Loss: 9.8149\t Val Acc: 0.0000\n",
      "[Epoch 360/1000] \t Loss: 0.7112\t Acc: 66.6667 \t Val Loss: 9.7425\t Val Acc: 0.0000\n",
      "[Epoch 361/1000] \t Loss: 0.5850\t Acc: 66.6667 \t Val Loss: 9.1967\t Val Acc: 0.0000\n",
      "[Epoch 362/1000] \t Loss: 0.6910\t Acc: 33.3333 \t Val Loss: 9.6066\t Val Acc: 0.0000\n",
      "[Epoch 363/1000] \t Loss: 0.6844\t Acc: 33.3333 \t Val Loss: 10.0466\t Val Acc: 0.0000\n",
      "[Epoch 364/1000] \t Loss: 0.6186\t Acc: 66.6667 \t Val Loss: 9.0898\t Val Acc: 0.0000\n",
      "[Epoch 365/1000] \t Loss: 0.5893\t Acc: 66.6667 \t Val Loss: 8.9752\t Val Acc: 0.0000\n",
      "[Epoch 366/1000] \t Loss: 0.5757\t Acc: 66.6667 \t Val Loss: 9.9470\t Val Acc: 0.0000\n",
      "[Epoch 367/1000] \t Loss: 0.6274\t Acc: 66.6667 \t Val Loss: 9.3140\t Val Acc: 0.0000\n",
      "[Epoch 368/1000] \t Loss: 0.6683\t Acc: 66.6667 \t Val Loss: 9.6565\t Val Acc: 0.0000\n",
      "[Epoch 369/1000] \t Loss: 0.5617\t Acc: 66.6667 \t Val Loss: 9.1222\t Val Acc: 0.0000\n",
      "[Epoch 370/1000] \t Loss: 0.6539\t Acc: 33.3333 \t Val Loss: 9.7606\t Val Acc: 0.0000\n",
      "[Epoch 371/1000] \t Loss: 0.5666\t Acc: 66.6667 \t Val Loss: 9.4694\t Val Acc: 0.0000\n",
      "[Epoch 372/1000] \t Loss: 0.6425\t Acc: 0.0000 \t Val Loss: 9.7051\t Val Acc: 0.0000\n",
      "[Epoch 373/1000] \t Loss: 0.6152\t Acc: 66.6667 \t Val Loss: 9.5163\t Val Acc: 0.0000\n",
      "[Epoch 374/1000] \t Loss: 0.6071\t Acc: 66.6667 \t Val Loss: 9.6185\t Val Acc: 0.0000\n",
      "[Epoch 375/1000] \t Loss: 0.5831\t Acc: 33.3333 \t Val Loss: 10.0200\t Val Acc: 0.0000\n",
      "[Epoch 376/1000] \t Loss: 0.5483\t Acc: 66.6667 \t Val Loss: 9.9318\t Val Acc: 0.0000\n",
      "[Epoch 377/1000] \t Loss: 0.5896\t Acc: 33.3333 \t Val Loss: 9.8860\t Val Acc: 0.0000\n",
      "[Epoch 378/1000] \t Loss: 0.5416\t Acc: 66.6667 \t Val Loss: 9.6232\t Val Acc: 0.0000\n",
      "[Epoch 379/1000] \t Loss: 0.7478\t Acc: 33.3333 \t Val Loss: 10.4360\t Val Acc: 0.0000\n",
      "[Epoch 380/1000] \t Loss: 0.5423\t Acc: 33.3333 \t Val Loss: 9.7068\t Val Acc: 0.0000\n",
      "[Epoch 381/1000] \t Loss: 0.6261\t Acc: 66.6667 \t Val Loss: 9.8450\t Val Acc: 0.0000\n",
      "[Epoch 382/1000] \t Loss: 0.5950\t Acc: 66.6667 \t Val Loss: 10.2949\t Val Acc: 0.0000\n",
      "[Epoch 383/1000] \t Loss: 0.5834\t Acc: 66.6667 \t Val Loss: 9.9115\t Val Acc: 0.0000\n",
      "[Epoch 384/1000] \t Loss: 0.7084\t Acc: 66.6667 \t Val Loss: 10.0681\t Val Acc: 0.0000\n",
      "[Epoch 385/1000] \t Loss: 0.5668\t Acc: 66.6667 \t Val Loss: 9.9499\t Val Acc: 0.0000\n",
      "[Epoch 386/1000] \t Loss: 0.6611\t Acc: 66.6667 \t Val Loss: 10.2961\t Val Acc: 0.0000\n",
      "[Epoch 387/1000] \t Loss: 0.5591\t Acc: 66.6667 \t Val Loss: 10.2886\t Val Acc: 0.0000\n",
      "[Epoch 388/1000] \t Loss: 0.5902\t Acc: 66.6667 \t Val Loss: 10.2202\t Val Acc: 0.0000\n",
      "[Epoch 389/1000] \t Loss: 0.6634\t Acc: 66.6667 \t Val Loss: 9.8068\t Val Acc: 0.0000\n",
      "[Epoch 390/1000] \t Loss: 0.5403\t Acc: 33.3333 \t Val Loss: 9.9612\t Val Acc: 0.0000\n",
      "[Epoch 391/1000] \t Loss: 0.5561\t Acc: 66.6667 \t Val Loss: 10.0901\t Val Acc: 0.0000\n",
      "[Epoch 392/1000] \t Loss: 0.5542\t Acc: 66.6667 \t Val Loss: 9.6926\t Val Acc: 0.0000\n",
      "[Epoch 393/1000] \t Loss: 0.5082\t Acc: 100.0000 \t Val Loss: 9.8531\t Val Acc: 0.0000\n",
      "[Epoch 394/1000] \t Loss: 0.6187\t Acc: 66.6667 \t Val Loss: 10.2529\t Val Acc: 0.0000\n",
      "[Epoch 395/1000] \t Loss: 0.6166\t Acc: 66.6667 \t Val Loss: 9.6385\t Val Acc: 0.0000\n",
      "[Epoch 396/1000] \t Loss: 0.5559\t Acc: 66.6667 \t Val Loss: 10.0553\t Val Acc: 0.0000\n",
      "[Epoch 397/1000] \t Loss: 0.5290\t Acc: 100.0000 \t Val Loss: 9.8260\t Val Acc: 0.0000\n",
      "[Epoch 398/1000] \t Loss: 0.6286\t Acc: 66.6667 \t Val Loss: 9.4318\t Val Acc: 0.0000\n",
      "[Epoch 399/1000] \t Loss: 0.5337\t Acc: 66.6667 \t Val Loss: 9.6034\t Val Acc: 0.0000\n",
      "[Epoch 400/1000] \t Loss: 0.6566\t Acc: 66.6667 \t Val Loss: 9.9414\t Val Acc: 0.0000\n",
      "[Epoch 401/1000] \t Loss: 0.6631\t Acc: 66.6667 \t Val Loss: 9.7519\t Val Acc: 0.0000\n",
      "[Epoch 402/1000] \t Loss: 0.5370\t Acc: 66.6667 \t Val Loss: 10.0159\t Val Acc: 0.0000\n",
      "[Epoch 403/1000] \t Loss: 0.6090\t Acc: 33.3333 \t Val Loss: 9.5701\t Val Acc: 0.0000\n",
      "[Epoch 404/1000] \t Loss: 0.5216\t Acc: 66.6667 \t Val Loss: 10.0624\t Val Acc: 0.0000\n",
      "[Epoch 405/1000] \t Loss: 0.5108\t Acc: 66.6667 \t Val Loss: 9.8676\t Val Acc: 0.0000\n",
      "[Epoch 406/1000] \t Loss: 0.6414\t Acc: 33.3333 \t Val Loss: 9.9230\t Val Acc: 0.0000\n",
      "[Epoch 407/1000] \t Loss: 0.5209\t Acc: 100.0000 \t Val Loss: 10.4725\t Val Acc: 0.0000\n",
      "[Epoch 408/1000] \t Loss: 0.5186\t Acc: 66.6667 \t Val Loss: 10.0556\t Val Acc: 0.0000\n",
      "[Epoch 409/1000] \t Loss: 0.5088\t Acc: 66.6667 \t Val Loss: 9.5803\t Val Acc: 0.0000\n",
      "[Epoch 410/1000] \t Loss: 0.5680\t Acc: 66.6667 \t Val Loss: 10.3322\t Val Acc: 0.0000\n",
      "[Epoch 411/1000] \t Loss: 0.5239\t Acc: 66.6667 \t Val Loss: 10.2826\t Val Acc: 0.0000\n",
      "[Epoch 412/1000] \t Loss: 0.5179\t Acc: 66.6667 \t Val Loss: 10.0649\t Val Acc: 0.0000\n",
      "[Epoch 413/1000] \t Loss: 0.5241\t Acc: 66.6667 \t Val Loss: 9.9973\t Val Acc: 0.0000\n",
      "[Epoch 414/1000] \t Loss: 0.5697\t Acc: 66.6667 \t Val Loss: 9.3689\t Val Acc: 0.0000\n",
      "[Epoch 415/1000] \t Loss: 0.4832\t Acc: 66.6667 \t Val Loss: 10.3245\t Val Acc: 0.0000\n",
      "[Epoch 416/1000] \t Loss: 0.6008\t Acc: 66.6667 \t Val Loss: 10.4482\t Val Acc: 0.0000\n",
      "[Epoch 417/1000] \t Loss: 0.5130\t Acc: 66.6667 \t Val Loss: 9.8855\t Val Acc: 0.0000\n",
      "[Epoch 418/1000] \t Loss: 0.5409\t Acc: 66.6667 \t Val Loss: 10.4057\t Val Acc: 0.0000\n",
      "[Epoch 419/1000] \t Loss: 0.4792\t Acc: 100.0000 \t Val Loss: 9.7642\t Val Acc: 0.0000\n",
      "[Epoch 420/1000] \t Loss: 0.5096\t Acc: 100.0000 \t Val Loss: 10.6850\t Val Acc: 0.0000\n",
      "[Epoch 421/1000] \t Loss: 0.4954\t Acc: 66.6667 \t Val Loss: 10.6234\t Val Acc: 0.0000\n",
      "[Epoch 422/1000] \t Loss: 0.5216\t Acc: 66.6667 \t Val Loss: 10.2733\t Val Acc: 0.0000\n",
      "[Epoch 423/1000] \t Loss: 0.5137\t Acc: 66.6667 \t Val Loss: 10.5277\t Val Acc: 0.0000\n",
      "[Epoch 424/1000] \t Loss: 0.4943\t Acc: 66.6667 \t Val Loss: 10.1764\t Val Acc: 0.0000\n",
      "[Epoch 425/1000] \t Loss: 0.4548\t Acc: 100.0000 \t Val Loss: 9.8399\t Val Acc: 0.0000\n",
      "[Epoch 426/1000] \t Loss: 0.5040\t Acc: 66.6667 \t Val Loss: 9.8958\t Val Acc: 0.0000\n",
      "[Epoch 427/1000] \t Loss: 0.6249\t Acc: 33.3333 \t Val Loss: 9.8648\t Val Acc: 0.0000\n",
      "[Epoch 428/1000] \t Loss: 0.6370\t Acc: 66.6667 \t Val Loss: 9.8251\t Val Acc: 0.0000\n",
      "[Epoch 429/1000] \t Loss: 0.5926\t Acc: 66.6667 \t Val Loss: 9.6958\t Val Acc: 0.0000\n",
      "[Epoch 430/1000] \t Loss: 0.4997\t Acc: 66.6667 \t Val Loss: 9.7111\t Val Acc: 0.0000\n",
      "[Epoch 431/1000] \t Loss: 0.4566\t Acc: 100.0000 \t Val Loss: 9.9728\t Val Acc: 0.0000\n",
      "[Epoch 432/1000] \t Loss: 0.4886\t Acc: 66.6667 \t Val Loss: 9.8739\t Val Acc: 0.0000\n",
      "[Epoch 433/1000] \t Loss: 0.4739\t Acc: 66.6667 \t Val Loss: 9.5933\t Val Acc: 0.0000\n",
      "[Epoch 434/1000] \t Loss: 0.5888\t Acc: 66.6667 \t Val Loss: 9.6272\t Val Acc: 0.0000\n",
      "[Epoch 435/1000] \t Loss: 0.5034\t Acc: 66.6667 \t Val Loss: 10.1526\t Val Acc: 0.0000\n",
      "[Epoch 436/1000] \t Loss: 0.5325\t Acc: 66.6667 \t Val Loss: 9.6484\t Val Acc: 0.0000\n",
      "[Epoch 437/1000] \t Loss: 0.4658\t Acc: 66.6667 \t Val Loss: 9.5670\t Val Acc: 0.0000\n",
      "[Epoch 438/1000] \t Loss: 0.4859\t Acc: 66.6667 \t Val Loss: 9.7696\t Val Acc: 0.0000\n",
      "[Epoch 439/1000] \t Loss: 0.4853\t Acc: 66.6667 \t Val Loss: 9.4782\t Val Acc: 0.0000\n",
      "[Epoch 440/1000] \t Loss: 0.4673\t Acc: 66.6667 \t Val Loss: 10.5623\t Val Acc: 0.0000\n",
      "[Epoch 441/1000] \t Loss: 0.4840\t Acc: 66.6667 \t Val Loss: 9.9311\t Val Acc: 0.0000\n",
      "[Epoch 442/1000] \t Loss: 0.4699\t Acc: 33.3333 \t Val Loss: 9.8972\t Val Acc: 0.0000\n",
      "[Epoch 443/1000] \t Loss: 0.4641\t Acc: 66.6667 \t Val Loss: 10.1405\t Val Acc: 0.0000\n",
      "[Epoch 444/1000] \t Loss: 0.5277\t Acc: 33.3333 \t Val Loss: 10.3304\t Val Acc: 0.0000\n",
      "[Epoch 445/1000] \t Loss: 0.4770\t Acc: 66.6667 \t Val Loss: 10.3489\t Val Acc: 0.0000\n",
      "[Epoch 446/1000] \t Loss: 0.4589\t Acc: 100.0000 \t Val Loss: 10.8657\t Val Acc: 0.0000\n",
      "[Epoch 447/1000] \t Loss: 0.4584\t Acc: 100.0000 \t Val Loss: 10.1917\t Val Acc: 0.0000\n",
      "[Epoch 448/1000] \t Loss: 0.5368\t Acc: 33.3333 \t Val Loss: 9.6333\t Val Acc: 0.0000\n",
      "[Epoch 449/1000] \t Loss: 0.4495\t Acc: 100.0000 \t Val Loss: 10.5105\t Val Acc: 0.0000\n",
      "[Epoch 450/1000] \t Loss: 0.4761\t Acc: 66.6667 \t Val Loss: 10.0532\t Val Acc: 0.0000\n",
      "[Epoch 451/1000] \t Loss: 0.4609\t Acc: 100.0000 \t Val Loss: 9.9677\t Val Acc: 0.0000\n",
      "[Epoch 452/1000] \t Loss: 0.4940\t Acc: 66.6667 \t Val Loss: 10.1058\t Val Acc: 0.0000\n",
      "[Epoch 453/1000] \t Loss: 0.4654\t Acc: 66.6667 \t Val Loss: 10.1734\t Val Acc: 0.0000\n",
      "[Epoch 454/1000] \t Loss: 0.4432\t Acc: 100.0000 \t Val Loss: 10.4850\t Val Acc: 0.0000\n",
      "[Epoch 455/1000] \t Loss: 0.4514\t Acc: 100.0000 \t Val Loss: 9.6368\t Val Acc: 0.0000\n",
      "[Epoch 456/1000] \t Loss: 0.4644\t Acc: 66.6667 \t Val Loss: 9.7930\t Val Acc: 0.0000\n",
      "[Epoch 457/1000] \t Loss: 0.4600\t Acc: 66.6667 \t Val Loss: 10.3640\t Val Acc: 0.0000\n",
      "[Epoch 458/1000] \t Loss: 0.4282\t Acc: 100.0000 \t Val Loss: 9.5365\t Val Acc: 0.0000\n",
      "[Epoch 459/1000] \t Loss: 0.4115\t Acc: 100.0000 \t Val Loss: 9.7559\t Val Acc: 0.0000\n",
      "[Epoch 460/1000] \t Loss: 0.4124\t Acc: 100.0000 \t Val Loss: 10.6536\t Val Acc: 0.0000\n",
      "[Epoch 461/1000] \t Loss: 0.4310\t Acc: 66.6667 \t Val Loss: 10.1087\t Val Acc: 0.0000\n",
      "[Epoch 462/1000] \t Loss: 0.7418\t Acc: 33.3333 \t Val Loss: 10.1623\t Val Acc: 0.0000\n",
      "[Epoch 463/1000] \t Loss: 0.6227\t Acc: 66.6667 \t Val Loss: 9.9373\t Val Acc: 0.0000\n",
      "[Epoch 464/1000] \t Loss: 0.7165\t Acc: 33.3333 \t Val Loss: 10.3783\t Val Acc: 0.0000\n",
      "[Epoch 465/1000] \t Loss: 0.5424\t Acc: 66.6667 \t Val Loss: 10.4350\t Val Acc: 0.0000\n",
      "[Epoch 466/1000] \t Loss: 0.4437\t Acc: 100.0000 \t Val Loss: 10.0354\t Val Acc: 0.0000\n",
      "[Epoch 467/1000] \t Loss: 0.4103\t Acc: 100.0000 \t Val Loss: 10.0319\t Val Acc: 0.0000\n",
      "[Epoch 468/1000] \t Loss: 0.4153\t Acc: 100.0000 \t Val Loss: 10.1984\t Val Acc: 0.0000\n",
      "[Epoch 469/1000] \t Loss: 0.4175\t Acc: 100.0000 \t Val Loss: 10.0834\t Val Acc: 0.0000\n",
      "[Epoch 470/1000] \t Loss: 0.5563\t Acc: 66.6667 \t Val Loss: 11.2154\t Val Acc: 0.0000\n",
      "[Epoch 471/1000] \t Loss: 0.4106\t Acc: 66.6667 \t Val Loss: 10.2036\t Val Acc: 0.0000\n",
      "[Epoch 472/1000] \t Loss: 0.3952\t Acc: 100.0000 \t Val Loss: 10.2076\t Val Acc: 0.0000\n",
      "[Epoch 473/1000] \t Loss: 0.5421\t Acc: 66.6667 \t Val Loss: 9.9428\t Val Acc: 0.0000\n",
      "[Epoch 474/1000] \t Loss: 0.5530\t Acc: 66.6667 \t Val Loss: 10.4947\t Val Acc: 0.0000\n",
      "[Epoch 475/1000] \t Loss: 0.4174\t Acc: 100.0000 \t Val Loss: 10.0185\t Val Acc: 0.0000\n",
      "[Epoch 476/1000] \t Loss: 0.3974\t Acc: 100.0000 \t Val Loss: 10.7454\t Val Acc: 0.0000\n",
      "[Epoch 477/1000] \t Loss: 0.5026\t Acc: 66.6667 \t Val Loss: 9.8981\t Val Acc: 0.0000\n",
      "[Epoch 478/1000] \t Loss: 0.4176\t Acc: 100.0000 \t Val Loss: 9.8859\t Val Acc: 0.0000\n",
      "[Epoch 479/1000] \t Loss: 0.4955\t Acc: 66.6667 \t Val Loss: 10.2879\t Val Acc: 0.0000\n",
      "[Epoch 480/1000] \t Loss: 0.4228\t Acc: 100.0000 \t Val Loss: 10.5637\t Val Acc: 0.0000\n",
      "[Epoch 481/1000] \t Loss: 0.5114\t Acc: 33.3333 \t Val Loss: 10.1652\t Val Acc: 0.0000\n",
      "[Epoch 482/1000] \t Loss: 0.5844\t Acc: 66.6667 \t Val Loss: 10.8581\t Val Acc: 0.0000\n",
      "[Epoch 483/1000] \t Loss: 0.3991\t Acc: 100.0000 \t Val Loss: 10.3581\t Val Acc: 0.0000\n",
      "[Epoch 484/1000] \t Loss: 0.4790\t Acc: 66.6667 \t Val Loss: 10.4143\t Val Acc: 0.0000\n",
      "[Epoch 485/1000] \t Loss: 0.4438\t Acc: 100.0000 \t Val Loss: 10.0676\t Val Acc: 0.0000\n",
      "[Epoch 486/1000] \t Loss: 0.3883\t Acc: 100.0000 \t Val Loss: 10.4489\t Val Acc: 0.0000\n",
      "[Epoch 487/1000] \t Loss: 0.3969\t Acc: 100.0000 \t Val Loss: 10.1554\t Val Acc: 0.0000\n",
      "[Epoch 488/1000] \t Loss: 0.4991\t Acc: 66.6667 \t Val Loss: 11.0648\t Val Acc: 0.0000\n",
      "[Epoch 489/1000] \t Loss: 0.3982\t Acc: 100.0000 \t Val Loss: 10.7201\t Val Acc: 0.0000\n",
      "[Epoch 490/1000] \t Loss: 0.5198\t Acc: 66.6667 \t Val Loss: 11.0588\t Val Acc: 0.0000\n",
      "[Epoch 491/1000] \t Loss: 0.5070\t Acc: 66.6667 \t Val Loss: 10.1664\t Val Acc: 0.0000\n",
      "[Epoch 492/1000] \t Loss: 0.3943\t Acc: 100.0000 \t Val Loss: 10.5997\t Val Acc: 0.0000\n",
      "[Epoch 493/1000] \t Loss: 0.5057\t Acc: 66.6667 \t Val Loss: 9.8986\t Val Acc: 0.0000\n",
      "[Epoch 494/1000] \t Loss: 0.4095\t Acc: 100.0000 \t Val Loss: 10.6689\t Val Acc: 0.0000\n",
      "[Epoch 495/1000] \t Loss: 0.4162\t Acc: 100.0000 \t Val Loss: 10.3073\t Val Acc: 0.0000\n",
      "[Epoch 496/1000] \t Loss: 0.3788\t Acc: 100.0000 \t Val Loss: 10.3478\t Val Acc: 0.0000\n",
      "[Epoch 497/1000] \t Loss: 0.4039\t Acc: 66.6667 \t Val Loss: 11.2326\t Val Acc: 0.0000\n",
      "[Epoch 498/1000] \t Loss: 0.3746\t Acc: 100.0000 \t Val Loss: 10.1637\t Val Acc: 0.0000\n",
      "[Epoch 499/1000] \t Loss: 0.3828\t Acc: 100.0000 \t Val Loss: 10.4036\t Val Acc: 0.0000\n",
      "[Epoch 500/1000] \t Loss: 0.4250\t Acc: 66.6667 \t Val Loss: 11.3779\t Val Acc: 0.0000\n",
      "[Epoch 501/1000] \t Loss: 0.4278\t Acc: 66.6667 \t Val Loss: 10.0903\t Val Acc: 0.0000\n",
      "[Epoch 502/1000] \t Loss: 0.3854\t Acc: 100.0000 \t Val Loss: 10.8441\t Val Acc: 0.0000\n",
      "[Epoch 503/1000] \t Loss: 0.4236\t Acc: 66.6667 \t Val Loss: 10.5002\t Val Acc: 0.0000\n",
      "[Epoch 504/1000] \t Loss: 0.4370\t Acc: 66.6667 \t Val Loss: 10.1347\t Val Acc: 0.0000\n",
      "[Epoch 505/1000] \t Loss: 0.3597\t Acc: 100.0000 \t Val Loss: 10.2450\t Val Acc: 0.0000\n",
      "[Epoch 506/1000] \t Loss: 0.3881\t Acc: 100.0000 \t Val Loss: 10.1136\t Val Acc: 0.0000\n",
      "[Epoch 507/1000] \t Loss: 0.3785\t Acc: 100.0000 \t Val Loss: 10.3123\t Val Acc: 0.0000\n",
      "[Epoch 508/1000] \t Loss: 0.3445\t Acc: 100.0000 \t Val Loss: 10.5689\t Val Acc: 0.0000\n",
      "[Epoch 509/1000] \t Loss: 0.3835\t Acc: 100.0000 \t Val Loss: 10.3457\t Val Acc: 0.0000\n",
      "[Epoch 510/1000] \t Loss: 0.3839\t Acc: 66.6667 \t Val Loss: 11.2998\t Val Acc: 0.0000\n",
      "[Epoch 511/1000] \t Loss: 0.3761\t Acc: 100.0000 \t Val Loss: 10.5343\t Val Acc: 0.0000\n",
      "[Epoch 512/1000] \t Loss: 0.3582\t Acc: 100.0000 \t Val Loss: 10.2730\t Val Acc: 0.0000\n",
      "[Epoch 513/1000] \t Loss: 0.3595\t Acc: 100.0000 \t Val Loss: 10.7350\t Val Acc: 0.0000\n",
      "[Epoch 514/1000] \t Loss: 0.3555\t Acc: 100.0000 \t Val Loss: 10.1850\t Val Acc: 0.0000\n",
      "[Epoch 515/1000] \t Loss: 0.3529\t Acc: 100.0000 \t Val Loss: 9.9641\t Val Acc: 0.0000\n",
      "[Epoch 516/1000] \t Loss: 0.3572\t Acc: 100.0000 \t Val Loss: 10.1266\t Val Acc: 0.0000\n",
      "[Epoch 517/1000] \t Loss: 0.3660\t Acc: 100.0000 \t Val Loss: 10.2641\t Val Acc: 0.0000\n",
      "[Epoch 518/1000] \t Loss: 0.3657\t Acc: 100.0000 \t Val Loss: 10.4020\t Val Acc: 0.0000\n",
      "[Epoch 519/1000] \t Loss: 0.3772\t Acc: 100.0000 \t Val Loss: 9.8129\t Val Acc: 0.0000\n",
      "[Epoch 520/1000] \t Loss: 0.3584\t Acc: 100.0000 \t Val Loss: 10.2454\t Val Acc: 0.0000\n",
      "[Epoch 521/1000] \t Loss: 0.3585\t Acc: 100.0000 \t Val Loss: 10.6393\t Val Acc: 0.0000\n",
      "[Epoch 522/1000] \t Loss: 0.3507\t Acc: 100.0000 \t Val Loss: 10.9952\t Val Acc: 0.0000\n",
      "[Epoch 523/1000] \t Loss: 0.3742\t Acc: 100.0000 \t Val Loss: 9.6679\t Val Acc: 0.0000\n",
      "[Epoch 524/1000] \t Loss: 0.3385\t Acc: 100.0000 \t Val Loss: 10.6092\t Val Acc: 0.0000\n",
      "[Epoch 525/1000] \t Loss: 0.3388\t Acc: 100.0000 \t Val Loss: 10.7399\t Val Acc: 0.0000\n",
      "[Epoch 526/1000] \t Loss: 0.3505\t Acc: 100.0000 \t Val Loss: 10.6615\t Val Acc: 0.0000\n",
      "[Epoch 527/1000] \t Loss: 0.3617\t Acc: 100.0000 \t Val Loss: 10.0064\t Val Acc: 0.0000\n",
      "[Epoch 528/1000] \t Loss: 0.3361\t Acc: 100.0000 \t Val Loss: 10.8665\t Val Acc: 0.0000\n",
      "[Epoch 529/1000] \t Loss: 0.3394\t Acc: 100.0000 \t Val Loss: 11.5112\t Val Acc: 0.0000\n",
      "[Epoch 530/1000] \t Loss: 0.3398\t Acc: 100.0000 \t Val Loss: 10.7785\t Val Acc: 0.0000\n",
      "[Epoch 531/1000] \t Loss: 0.3622\t Acc: 100.0000 \t Val Loss: 10.1501\t Val Acc: 0.0000\n",
      "[Epoch 532/1000] \t Loss: 0.3552\t Acc: 100.0000 \t Val Loss: 10.8697\t Val Acc: 0.0000\n",
      "[Epoch 533/1000] \t Loss: 0.3443\t Acc: 100.0000 \t Val Loss: 10.5339\t Val Acc: 0.0000\n",
      "[Epoch 534/1000] \t Loss: 0.3196\t Acc: 100.0000 \t Val Loss: 10.8779\t Val Acc: 0.0000\n",
      "[Epoch 535/1000] \t Loss: 0.3295\t Acc: 100.0000 \t Val Loss: 10.6980\t Val Acc: 0.0000\n",
      "[Epoch 536/1000] \t Loss: 0.3355\t Acc: 100.0000 \t Val Loss: 10.7915\t Val Acc: 0.0000\n",
      "[Epoch 537/1000] \t Loss: 0.3478\t Acc: 100.0000 \t Val Loss: 11.2463\t Val Acc: 0.0000\n",
      "[Epoch 538/1000] \t Loss: 0.3291\t Acc: 100.0000 \t Val Loss: 11.1107\t Val Acc: 0.0000\n",
      "[Epoch 539/1000] \t Loss: 0.3249\t Acc: 100.0000 \t Val Loss: 10.5226\t Val Acc: 0.0000\n",
      "[Epoch 540/1000] \t Loss: 0.3471\t Acc: 100.0000 \t Val Loss: 10.6734\t Val Acc: 0.0000\n",
      "[Epoch 541/1000] \t Loss: 0.3284\t Acc: 100.0000 \t Val Loss: 11.0537\t Val Acc: 0.0000\n",
      "[Epoch 542/1000] \t Loss: 0.3619\t Acc: 100.0000 \t Val Loss: 10.6908\t Val Acc: 0.0000\n",
      "[Epoch 543/1000] \t Loss: 0.3493\t Acc: 100.0000 \t Val Loss: 10.3264\t Val Acc: 0.0000\n",
      "[Epoch 544/1000] \t Loss: 0.3391\t Acc: 100.0000 \t Val Loss: 10.5681\t Val Acc: 0.0000\n",
      "[Epoch 545/1000] \t Loss: 0.3282\t Acc: 100.0000 \t Val Loss: 10.5812\t Val Acc: 0.0000\n",
      "[Epoch 546/1000] \t Loss: 0.3494\t Acc: 100.0000 \t Val Loss: 10.5822\t Val Acc: 0.0000\n",
      "[Epoch 547/1000] \t Loss: 0.3460\t Acc: 100.0000 \t Val Loss: 11.4724\t Val Acc: 0.0000\n",
      "[Epoch 548/1000] \t Loss: 0.3406\t Acc: 66.6667 \t Val Loss: 10.0841\t Val Acc: 0.0000\n",
      "[Epoch 549/1000] \t Loss: 0.3236\t Acc: 100.0000 \t Val Loss: 10.9582\t Val Acc: 0.0000\n",
      "[Epoch 550/1000] \t Loss: 0.3161\t Acc: 100.0000 \t Val Loss: 11.8900\t Val Acc: 0.0000\n",
      "[Epoch 551/1000] \t Loss: 0.3282\t Acc: 100.0000 \t Val Loss: 10.9841\t Val Acc: 0.0000\n",
      "[Epoch 552/1000] \t Loss: 0.3302\t Acc: 100.0000 \t Val Loss: 10.8076\t Val Acc: 0.0000\n",
      "[Epoch 553/1000] \t Loss: 0.3425\t Acc: 66.6667 \t Val Loss: 11.6696\t Val Acc: 0.0000\n",
      "[Epoch 554/1000] \t Loss: 0.5424\t Acc: 66.6667 \t Val Loss: 10.6058\t Val Acc: 0.0000\n",
      "[Epoch 555/1000] \t Loss: 0.3361\t Acc: 100.0000 \t Val Loss: 10.7869\t Val Acc: 0.0000\n",
      "[Epoch 556/1000] \t Loss: 0.5260\t Acc: 66.6667 \t Val Loss: 10.0898\t Val Acc: 0.0000\n",
      "[Epoch 557/1000] \t Loss: 0.3148\t Acc: 100.0000 \t Val Loss: 10.9744\t Val Acc: 0.0000\n",
      "[Epoch 558/1000] \t Loss: 0.3435\t Acc: 66.6667 \t Val Loss: 11.3527\t Val Acc: 0.0000\n",
      "[Epoch 559/1000] \t Loss: 0.3145\t Acc: 100.0000 \t Val Loss: 10.2023\t Val Acc: 0.0000\n",
      "[Epoch 560/1000] \t Loss: 0.3093\t Acc: 100.0000 \t Val Loss: 10.6172\t Val Acc: 0.0000\n",
      "[Epoch 561/1000] \t Loss: 0.3320\t Acc: 100.0000 \t Val Loss: 10.4923\t Val Acc: 0.0000\n",
      "[Epoch 562/1000] \t Loss: 0.5600\t Acc: 66.6667 \t Val Loss: 9.9505\t Val Acc: 0.0000\n",
      "[Epoch 563/1000] \t Loss: 0.3106\t Acc: 100.0000 \t Val Loss: 10.8308\t Val Acc: 0.0000\n",
      "[Epoch 564/1000] \t Loss: 0.3037\t Acc: 100.0000 \t Val Loss: 10.4734\t Val Acc: 0.0000\n",
      "[Epoch 565/1000] \t Loss: 0.2948\t Acc: 100.0000 \t Val Loss: 11.2009\t Val Acc: 0.0000\n",
      "[Epoch 566/1000] \t Loss: 0.3091\t Acc: 100.0000 \t Val Loss: 10.7438\t Val Acc: 0.0000\n",
      "[Epoch 567/1000] \t Loss: 0.3363\t Acc: 100.0000 \t Val Loss: 10.3088\t Val Acc: 0.0000\n",
      "[Epoch 568/1000] \t Loss: 0.3148\t Acc: 100.0000 \t Val Loss: 10.8909\t Val Acc: 0.0000\n",
      "[Epoch 569/1000] \t Loss: 0.3090\t Acc: 100.0000 \t Val Loss: 10.7041\t Val Acc: 0.0000\n",
      "[Epoch 570/1000] \t Loss: 0.3128\t Acc: 100.0000 \t Val Loss: 10.2231\t Val Acc: 0.0000\n",
      "[Epoch 571/1000] \t Loss: 0.3080\t Acc: 100.0000 \t Val Loss: 10.4697\t Val Acc: 0.0000\n",
      "[Epoch 572/1000] \t Loss: 0.3210\t Acc: 100.0000 \t Val Loss: 10.2167\t Val Acc: 0.0000\n",
      "[Epoch 573/1000] \t Loss: 0.3044\t Acc: 100.0000 \t Val Loss: 10.2076\t Val Acc: 0.0000\n",
      "[Epoch 574/1000] \t Loss: 0.2991\t Acc: 100.0000 \t Val Loss: 10.4896\t Val Acc: 0.0000\n",
      "[Epoch 575/1000] \t Loss: 0.3570\t Acc: 100.0000 \t Val Loss: 10.3085\t Val Acc: 0.0000\n",
      "[Epoch 576/1000] \t Loss: 0.3181\t Acc: 100.0000 \t Val Loss: 10.7466\t Val Acc: 0.0000\n",
      "[Epoch 577/1000] \t Loss: 0.3127\t Acc: 100.0000 \t Val Loss: 11.0222\t Val Acc: 0.0000\n",
      "[Epoch 578/1000] \t Loss: 0.3027\t Acc: 100.0000 \t Val Loss: 10.6639\t Val Acc: 0.0000\n",
      "[Epoch 579/1000] \t Loss: 0.3116\t Acc: 100.0000 \t Val Loss: 10.5982\t Val Acc: 0.0000\n",
      "[Epoch 580/1000] \t Loss: 0.3122\t Acc: 100.0000 \t Val Loss: 11.5325\t Val Acc: 0.0000\n",
      "[Epoch 581/1000] \t Loss: 0.3078\t Acc: 100.0000 \t Val Loss: 10.6647\t Val Acc: 0.0000\n",
      "[Epoch 582/1000] \t Loss: 0.3178\t Acc: 100.0000 \t Val Loss: 10.6981\t Val Acc: 0.0000\n",
      "[Epoch 583/1000] \t Loss: 0.5250\t Acc: 66.6667 \t Val Loss: 10.2621\t Val Acc: 0.0000\n",
      "[Epoch 584/1000] \t Loss: 0.3067\t Acc: 100.0000 \t Val Loss: 11.0158\t Val Acc: 0.0000\n",
      "[Epoch 585/1000] \t Loss: 0.3138\t Acc: 100.0000 \t Val Loss: 11.6474\t Val Acc: 0.0000\n",
      "[Epoch 586/1000] \t Loss: 0.2957\t Acc: 100.0000 \t Val Loss: 10.4625\t Val Acc: 0.0000\n",
      "[Epoch 587/1000] \t Loss: 0.2867\t Acc: 100.0000 \t Val Loss: 10.2603\t Val Acc: 0.0000\n",
      "[Epoch 588/1000] \t Loss: 0.3000\t Acc: 100.0000 \t Val Loss: 10.7646\t Val Acc: 0.0000\n",
      "[Epoch 589/1000] \t Loss: 0.2983\t Acc: 100.0000 \t Val Loss: 10.6771\t Val Acc: 0.0000\n",
      "[Epoch 590/1000] \t Loss: 0.2870\t Acc: 100.0000 \t Val Loss: 11.4707\t Val Acc: 0.0000\n",
      "[Epoch 591/1000] \t Loss: 0.2844\t Acc: 100.0000 \t Val Loss: 10.6427\t Val Acc: 0.0000\n",
      "[Epoch 592/1000] \t Loss: 0.2964\t Acc: 100.0000 \t Val Loss: 10.0307\t Val Acc: 0.0000\n",
      "[Epoch 593/1000] \t Loss: 0.2907\t Acc: 100.0000 \t Val Loss: 10.6354\t Val Acc: 0.0000\n",
      "[Epoch 594/1000] \t Loss: 0.2919\t Acc: 100.0000 \t Val Loss: 10.4070\t Val Acc: 0.0000\n",
      "[Epoch 595/1000] \t Loss: 0.3052\t Acc: 100.0000 \t Val Loss: 10.6340\t Val Acc: 0.0000\n",
      "[Epoch 596/1000] \t Loss: 0.2850\t Acc: 100.0000 \t Val Loss: 10.3776\t Val Acc: 0.0000\n",
      "[Epoch 597/1000] \t Loss: 0.3035\t Acc: 100.0000 \t Val Loss: 10.3472\t Val Acc: 0.0000\n",
      "[Epoch 598/1000] \t Loss: 0.2799\t Acc: 100.0000 \t Val Loss: 11.0737\t Val Acc: 0.0000\n",
      "[Epoch 599/1000] \t Loss: 0.2829\t Acc: 100.0000 \t Val Loss: 10.6024\t Val Acc: 0.0000\n",
      "[Epoch 600/1000] \t Loss: 0.2940\t Acc: 100.0000 \t Val Loss: 11.6587\t Val Acc: 0.0000\n",
      "[Epoch 601/1000] \t Loss: 0.2904\t Acc: 100.0000 \t Val Loss: 10.2165\t Val Acc: 0.0000\n",
      "[Epoch 602/1000] \t Loss: 0.2996\t Acc: 100.0000 \t Val Loss: 11.1395\t Val Acc: 0.0000\n",
      "[Epoch 603/1000] \t Loss: 0.2796\t Acc: 100.0000 \t Val Loss: 11.2103\t Val Acc: 0.0000\n",
      "[Epoch 604/1000] \t Loss: 0.2711\t Acc: 100.0000 \t Val Loss: 10.0989\t Val Acc: 0.0000\n",
      "[Epoch 605/1000] \t Loss: 0.2907\t Acc: 100.0000 \t Val Loss: 11.1108\t Val Acc: 0.0000\n",
      "[Epoch 606/1000] \t Loss: 0.2841\t Acc: 100.0000 \t Val Loss: 10.4599\t Val Acc: 0.0000\n",
      "[Epoch 607/1000] \t Loss: 0.2888\t Acc: 100.0000 \t Val Loss: 11.0980\t Val Acc: 0.0000\n",
      "[Epoch 608/1000] \t Loss: 0.2850\t Acc: 100.0000 \t Val Loss: 10.9650\t Val Acc: 0.0000\n",
      "[Epoch 609/1000] \t Loss: 0.2859\t Acc: 100.0000 \t Val Loss: 10.8210\t Val Acc: 0.0000\n",
      "[Epoch 610/1000] \t Loss: 0.2899\t Acc: 100.0000 \t Val Loss: 10.8239\t Val Acc: 0.0000\n",
      "[Epoch 611/1000] \t Loss: 0.2871\t Acc: 100.0000 \t Val Loss: 10.4967\t Val Acc: 0.0000\n",
      "[Epoch 612/1000] \t Loss: 0.2794\t Acc: 100.0000 \t Val Loss: 11.0831\t Val Acc: 0.0000\n",
      "[Epoch 613/1000] \t Loss: 0.2728\t Acc: 100.0000 \t Val Loss: 10.5931\t Val Acc: 0.0000\n",
      "[Epoch 614/1000] \t Loss: 0.2851\t Acc: 100.0000 \t Val Loss: 11.1481\t Val Acc: 0.0000\n",
      "[Epoch 615/1000] \t Loss: 0.2916\t Acc: 100.0000 \t Val Loss: 10.9213\t Val Acc: 0.0000\n",
      "[Epoch 616/1000] \t Loss: 0.2754\t Acc: 100.0000 \t Val Loss: 10.2633\t Val Acc: 0.0000\n",
      "[Epoch 617/1000] \t Loss: 0.3061\t Acc: 100.0000 \t Val Loss: 11.2033\t Val Acc: 0.0000\n",
      "[Epoch 618/1000] \t Loss: 0.2778\t Acc: 100.0000 \t Val Loss: 11.2389\t Val Acc: 0.0000\n",
      "[Epoch 619/1000] \t Loss: 0.2780\t Acc: 100.0000 \t Val Loss: 10.5686\t Val Acc: 0.0000\n",
      "[Epoch 620/1000] \t Loss: 0.2774\t Acc: 100.0000 \t Val Loss: 10.7766\t Val Acc: 0.0000\n",
      "[Epoch 621/1000] \t Loss: 0.2682\t Acc: 100.0000 \t Val Loss: 10.5122\t Val Acc: 0.0000\n",
      "[Epoch 622/1000] \t Loss: 0.2758\t Acc: 100.0000 \t Val Loss: 10.6869\t Val Acc: 0.0000\n",
      "[Epoch 623/1000] \t Loss: 0.2745\t Acc: 100.0000 \t Val Loss: 11.0992\t Val Acc: 0.0000\n",
      "[Epoch 624/1000] \t Loss: 0.2745\t Acc: 100.0000 \t Val Loss: 11.1103\t Val Acc: 0.0000\n",
      "[Epoch 625/1000] \t Loss: 0.2907\t Acc: 100.0000 \t Val Loss: 11.5973\t Val Acc: 0.0000\n",
      "[Epoch 626/1000] \t Loss: 0.2750\t Acc: 100.0000 \t Val Loss: 11.8693\t Val Acc: 0.0000\n",
      "[Epoch 627/1000] \t Loss: 0.2902\t Acc: 66.6667 \t Val Loss: 10.8885\t Val Acc: 0.0000\n",
      "[Epoch 628/1000] \t Loss: 0.2781\t Acc: 100.0000 \t Val Loss: 10.8546\t Val Acc: 0.0000\n",
      "[Epoch 629/1000] \t Loss: 0.2791\t Acc: 100.0000 \t Val Loss: 11.7457\t Val Acc: 0.0000\n",
      "[Epoch 630/1000] \t Loss: 0.2693\t Acc: 100.0000 \t Val Loss: 11.1725\t Val Acc: 0.0000\n",
      "[Epoch 631/1000] \t Loss: 0.2663\t Acc: 100.0000 \t Val Loss: 10.6345\t Val Acc: 0.0000\n",
      "[Epoch 632/1000] \t Loss: 0.2819\t Acc: 100.0000 \t Val Loss: 11.1326\t Val Acc: 0.0000\n",
      "[Epoch 633/1000] \t Loss: 0.2694\t Acc: 100.0000 \t Val Loss: 11.1301\t Val Acc: 0.0000\n",
      "[Epoch 634/1000] \t Loss: 0.2753\t Acc: 100.0000 \t Val Loss: 10.6308\t Val Acc: 0.0000\n",
      "[Epoch 635/1000] \t Loss: 0.2698\t Acc: 100.0000 \t Val Loss: 11.9415\t Val Acc: 0.0000\n",
      "[Epoch 636/1000] \t Loss: 0.2858\t Acc: 100.0000 \t Val Loss: 11.6946\t Val Acc: 0.0000\n",
      "[Epoch 637/1000] \t Loss: 0.2699\t Acc: 100.0000 \t Val Loss: 10.7259\t Val Acc: 0.0000\n",
      "[Epoch 638/1000] \t Loss: 0.2798\t Acc: 100.0000 \t Val Loss: 11.1310\t Val Acc: 0.0000\n",
      "[Epoch 639/1000] \t Loss: 0.2724\t Acc: 100.0000 \t Val Loss: 10.5023\t Val Acc: 0.0000\n",
      "[Epoch 640/1000] \t Loss: 0.2771\t Acc: 100.0000 \t Val Loss: 11.0094\t Val Acc: 0.0000\n",
      "[Epoch 641/1000] \t Loss: 0.2677\t Acc: 100.0000 \t Val Loss: 10.9826\t Val Acc: 0.0000\n",
      "[Epoch 642/1000] \t Loss: 0.2642\t Acc: 100.0000 \t Val Loss: 10.7082\t Val Acc: 0.0000\n",
      "[Epoch 643/1000] \t Loss: 0.2762\t Acc: 100.0000 \t Val Loss: 10.3107\t Val Acc: 0.0000\n",
      "[Epoch 644/1000] \t Loss: 0.2709\t Acc: 100.0000 \t Val Loss: 11.7452\t Val Acc: 0.0000\n",
      "[Epoch 645/1000] \t Loss: 0.2595\t Acc: 100.0000 \t Val Loss: 11.5221\t Val Acc: 0.0000\n",
      "[Epoch 646/1000] \t Loss: 0.2644\t Acc: 100.0000 \t Val Loss: 11.1048\t Val Acc: 0.0000\n",
      "[Epoch 647/1000] \t Loss: 0.2748\t Acc: 100.0000 \t Val Loss: 11.3121\t Val Acc: 0.0000\n",
      "[Epoch 648/1000] \t Loss: 0.2641\t Acc: 100.0000 \t Val Loss: 10.7164\t Val Acc: 0.0000\n",
      "[Epoch 649/1000] \t Loss: 0.2657\t Acc: 100.0000 \t Val Loss: 11.5397\t Val Acc: 0.0000\n",
      "[Epoch 650/1000] \t Loss: 0.2646\t Acc: 100.0000 \t Val Loss: 10.7952\t Val Acc: 0.0000\n",
      "[Epoch 651/1000] \t Loss: 0.2773\t Acc: 100.0000 \t Val Loss: 11.0957\t Val Acc: 0.0000\n",
      "[Epoch 652/1000] \t Loss: 0.2689\t Acc: 100.0000 \t Val Loss: 11.1232\t Val Acc: 0.0000\n",
      "[Epoch 653/1000] \t Loss: 0.2670\t Acc: 100.0000 \t Val Loss: 11.2338\t Val Acc: 0.0000\n",
      "[Epoch 654/1000] \t Loss: 0.2659\t Acc: 100.0000 \t Val Loss: 11.1001\t Val Acc: 0.0000\n",
      "[Epoch 655/1000] \t Loss: 0.2766\t Acc: 100.0000 \t Val Loss: 11.5867\t Val Acc: 0.0000\n",
      "[Epoch 656/1000] \t Loss: 0.2763\t Acc: 100.0000 \t Val Loss: 11.7405\t Val Acc: 0.0000\n",
      "[Epoch 657/1000] \t Loss: 0.2599\t Acc: 100.0000 \t Val Loss: 11.8213\t Val Acc: 0.0000\n",
      "[Epoch 658/1000] \t Loss: 0.2707\t Acc: 100.0000 \t Val Loss: 11.0060\t Val Acc: 0.0000\n",
      "[Epoch 659/1000] \t Loss: 0.2575\t Acc: 100.0000 \t Val Loss: 11.8890\t Val Acc: 0.0000\n",
      "[Epoch 660/1000] \t Loss: 0.2781\t Acc: 66.6667 \t Val Loss: 10.8279\t Val Acc: 0.0000\n",
      "[Epoch 661/1000] \t Loss: 0.2613\t Acc: 100.0000 \t Val Loss: 10.7161\t Val Acc: 0.0000\n",
      "[Epoch 662/1000] \t Loss: 0.2514\t Acc: 100.0000 \t Val Loss: 11.7822\t Val Acc: 0.0000\n",
      "[Epoch 663/1000] \t Loss: 0.2597\t Acc: 100.0000 \t Val Loss: 11.4556\t Val Acc: 0.0000\n",
      "[Epoch 664/1000] \t Loss: 0.2607\t Acc: 100.0000 \t Val Loss: 11.0084\t Val Acc: 0.0000\n",
      "[Epoch 665/1000] \t Loss: 0.2556\t Acc: 100.0000 \t Val Loss: 11.8106\t Val Acc: 0.0000\n",
      "[Epoch 666/1000] \t Loss: 0.2674\t Acc: 100.0000 \t Val Loss: 12.4447\t Val Acc: 0.0000\n",
      "[Epoch 667/1000] \t Loss: 0.2708\t Acc: 100.0000 \t Val Loss: 10.8129\t Val Acc: 0.0000\n",
      "[Epoch 668/1000] \t Loss: 0.2600\t Acc: 100.0000 \t Val Loss: 11.7285\t Val Acc: 0.0000\n",
      "[Epoch 669/1000] \t Loss: 0.2579\t Acc: 100.0000 \t Val Loss: 11.1914\t Val Acc: 0.0000\n",
      "[Epoch 670/1000] \t Loss: 0.2571\t Acc: 100.0000 \t Val Loss: 11.1034\t Val Acc: 0.0000\n",
      "[Epoch 671/1000] \t Loss: 0.2581\t Acc: 100.0000 \t Val Loss: 11.1481\t Val Acc: 0.0000\n",
      "[Epoch 672/1000] \t Loss: 0.2835\t Acc: 100.0000 \t Val Loss: 11.6376\t Val Acc: 0.0000\n",
      "[Epoch 673/1000] \t Loss: 0.2561\t Acc: 100.0000 \t Val Loss: 11.3927\t Val Acc: 0.0000\n",
      "[Epoch 674/1000] \t Loss: 0.2571\t Acc: 100.0000 \t Val Loss: 10.5254\t Val Acc: 0.0000\n",
      "[Epoch 675/1000] \t Loss: 0.2674\t Acc: 100.0000 \t Val Loss: 11.1357\t Val Acc: 0.0000\n",
      "[Epoch 676/1000] \t Loss: 0.2632\t Acc: 100.0000 \t Val Loss: 10.6855\t Val Acc: 0.0000\n",
      "[Epoch 677/1000] \t Loss: 0.2536\t Acc: 100.0000 \t Val Loss: 11.5630\t Val Acc: 0.0000\n",
      "[Epoch 678/1000] \t Loss: 0.2577\t Acc: 100.0000 \t Val Loss: 11.1560\t Val Acc: 0.0000\n",
      "[Epoch 679/1000] \t Loss: 0.2523\t Acc: 100.0000 \t Val Loss: 11.5812\t Val Acc: 0.0000\n",
      "[Epoch 680/1000] \t Loss: 0.2535\t Acc: 100.0000 \t Val Loss: 11.2401\t Val Acc: 0.0000\n",
      "[Epoch 681/1000] \t Loss: 0.2607\t Acc: 100.0000 \t Val Loss: 11.6351\t Val Acc: 0.0000\n",
      "[Epoch 682/1000] \t Loss: 0.2688\t Acc: 100.0000 \t Val Loss: 12.4681\t Val Acc: 0.0000\n",
      "[Epoch 683/1000] \t Loss: 0.2526\t Acc: 100.0000 \t Val Loss: 12.3609\t Val Acc: 0.0000\n",
      "[Epoch 684/1000] \t Loss: 0.2548\t Acc: 100.0000 \t Val Loss: 11.2479\t Val Acc: 0.0000\n",
      "[Epoch 685/1000] \t Loss: 0.2539\t Acc: 100.0000 \t Val Loss: 10.5481\t Val Acc: 0.0000\n",
      "[Epoch 686/1000] \t Loss: 0.2647\t Acc: 100.0000 \t Val Loss: 10.9171\t Val Acc: 0.0000\n",
      "[Epoch 687/1000] \t Loss: 0.2641\t Acc: 100.0000 \t Val Loss: 11.2238\t Val Acc: 0.0000\n",
      "[Epoch 688/1000] \t Loss: 0.2564\t Acc: 100.0000 \t Val Loss: 10.6191\t Val Acc: 0.0000\n",
      "[Epoch 689/1000] \t Loss: 0.2501\t Acc: 100.0000 \t Val Loss: 11.1424\t Val Acc: 0.0000\n",
      "[Epoch 690/1000] \t Loss: 0.2498\t Acc: 100.0000 \t Val Loss: 11.7094\t Val Acc: 0.0000\n",
      "[Epoch 691/1000] \t Loss: 0.2574\t Acc: 100.0000 \t Val Loss: 10.8585\t Val Acc: 0.0000\n",
      "[Epoch 692/1000] \t Loss: 0.2603\t Acc: 100.0000 \t Val Loss: 12.5029\t Val Acc: 0.0000\n",
      "[Epoch 693/1000] \t Loss: 0.2526\t Acc: 100.0000 \t Val Loss: 11.4911\t Val Acc: 0.0000\n",
      "[Epoch 694/1000] \t Loss: 0.2570\t Acc: 100.0000 \t Val Loss: 11.5566\t Val Acc: 0.0000\n",
      "[Epoch 695/1000] \t Loss: 0.2518\t Acc: 100.0000 \t Val Loss: 11.2221\t Val Acc: 0.0000\n",
      "[Epoch 696/1000] \t Loss: 0.2501\t Acc: 100.0000 \t Val Loss: 11.1720\t Val Acc: 0.0000\n",
      "[Epoch 697/1000] \t Loss: 0.2487\t Acc: 100.0000 \t Val Loss: 11.3178\t Val Acc: 0.0000\n",
      "[Epoch 698/1000] \t Loss: 0.2510\t Acc: 100.0000 \t Val Loss: 10.6430\t Val Acc: 0.0000\n",
      "[Epoch 699/1000] \t Loss: 0.2557\t Acc: 100.0000 \t Val Loss: 11.1574\t Val Acc: 0.0000\n",
      "[Epoch 700/1000] \t Loss: 0.2539\t Acc: 100.0000 \t Val Loss: 11.1943\t Val Acc: 0.0000\n",
      "[Epoch 701/1000] \t Loss: 0.2434\t Acc: 100.0000 \t Val Loss: 10.9601\t Val Acc: 0.0000\n",
      "[Epoch 702/1000] \t Loss: 0.2925\t Acc: 100.0000 \t Val Loss: 11.1884\t Val Acc: 0.0000\n",
      "[Epoch 703/1000] \t Loss: 0.2453\t Acc: 100.0000 \t Val Loss: 10.4395\t Val Acc: 0.0000\n",
      "[Epoch 704/1000] \t Loss: 0.2494\t Acc: 100.0000 \t Val Loss: 11.0708\t Val Acc: 0.0000\n",
      "[Epoch 705/1000] \t Loss: 0.2604\t Acc: 100.0000 \t Val Loss: 12.0499\t Val Acc: 0.0000\n",
      "[Epoch 706/1000] \t Loss: 0.2558\t Acc: 100.0000 \t Val Loss: 11.1841\t Val Acc: 0.0000\n",
      "[Epoch 707/1000] \t Loss: 0.2483\t Acc: 100.0000 \t Val Loss: 11.4334\t Val Acc: 0.0000\n",
      "[Epoch 708/1000] \t Loss: 0.2373\t Acc: 100.0000 \t Val Loss: 10.8570\t Val Acc: 0.0000\n",
      "[Epoch 709/1000] \t Loss: 0.2582\t Acc: 100.0000 \t Val Loss: 10.7409\t Val Acc: 0.0000\n",
      "[Epoch 710/1000] \t Loss: 0.2452\t Acc: 100.0000 \t Val Loss: 11.6096\t Val Acc: 0.0000\n",
      "[Epoch 711/1000] \t Loss: 0.2550\t Acc: 100.0000 \t Val Loss: 10.7998\t Val Acc: 0.0000\n",
      "[Epoch 712/1000] \t Loss: 0.2510\t Acc: 100.0000 \t Val Loss: 11.6271\t Val Acc: 0.0000\n",
      "[Epoch 713/1000] \t Loss: 0.2416\t Acc: 100.0000 \t Val Loss: 11.0151\t Val Acc: 0.0000\n",
      "[Epoch 714/1000] \t Loss: 0.2425\t Acc: 100.0000 \t Val Loss: 11.2423\t Val Acc: 0.0000\n",
      "[Epoch 715/1000] \t Loss: 0.2486\t Acc: 100.0000 \t Val Loss: 11.1827\t Val Acc: 0.0000\n",
      "[Epoch 716/1000] \t Loss: 0.2550\t Acc: 100.0000 \t Val Loss: 11.5619\t Val Acc: 0.0000\n",
      "[Epoch 717/1000] \t Loss: 0.2469\t Acc: 100.0000 \t Val Loss: 11.3139\t Val Acc: 0.0000\n",
      "[Epoch 718/1000] \t Loss: 0.2473\t Acc: 100.0000 \t Val Loss: 12.4352\t Val Acc: 0.0000\n",
      "[Epoch 719/1000] \t Loss: 0.2500\t Acc: 100.0000 \t Val Loss: 11.4349\t Val Acc: 0.0000\n",
      "[Epoch 720/1000] \t Loss: 0.2497\t Acc: 100.0000 \t Val Loss: 11.4821\t Val Acc: 0.0000\n",
      "[Epoch 721/1000] \t Loss: 0.2437\t Acc: 100.0000 \t Val Loss: 11.6397\t Val Acc: 0.0000\n",
      "[Epoch 722/1000] \t Loss: 0.2464\t Acc: 100.0000 \t Val Loss: 10.7728\t Val Acc: 0.0000\n",
      "[Epoch 723/1000] \t Loss: 0.2389\t Acc: 100.0000 \t Val Loss: 11.3217\t Val Acc: 0.0000\n",
      "[Epoch 724/1000] \t Loss: 0.2369\t Acc: 100.0000 \t Val Loss: 10.8769\t Val Acc: 0.0000\n",
      "[Epoch 725/1000] \t Loss: 0.2428\t Acc: 100.0000 \t Val Loss: 11.9241\t Val Acc: 0.0000\n",
      "[Epoch 726/1000] \t Loss: 0.2342\t Acc: 100.0000 \t Val Loss: 11.6557\t Val Acc: 0.0000\n",
      "[Epoch 727/1000] \t Loss: 0.2547\t Acc: 100.0000 \t Val Loss: 11.8141\t Val Acc: 0.0000\n",
      "[Epoch 728/1000] \t Loss: 0.2417\t Acc: 100.0000 \t Val Loss: 10.8241\t Val Acc: 0.0000\n",
      "[Epoch 729/1000] \t Loss: 0.2403\t Acc: 100.0000 \t Val Loss: 10.9909\t Val Acc: 0.0000\n",
      "[Epoch 730/1000] \t Loss: 0.2486\t Acc: 100.0000 \t Val Loss: 11.3554\t Val Acc: 0.0000\n",
      "[Epoch 731/1000] \t Loss: 0.2567\t Acc: 100.0000 \t Val Loss: 11.7036\t Val Acc: 0.0000\n",
      "[Epoch 732/1000] \t Loss: 0.2517\t Acc: 100.0000 \t Val Loss: 11.6742\t Val Acc: 0.0000\n",
      "[Epoch 733/1000] \t Loss: 0.2414\t Acc: 100.0000 \t Val Loss: 11.6709\t Val Acc: 0.0000\n",
      "[Epoch 734/1000] \t Loss: 0.2571\t Acc: 100.0000 \t Val Loss: 11.1790\t Val Acc: 0.0000\n",
      "[Epoch 735/1000] \t Loss: 0.2488\t Acc: 100.0000 \t Val Loss: 12.1667\t Val Acc: 0.0000\n",
      "[Epoch 736/1000] \t Loss: 0.2493\t Acc: 100.0000 \t Val Loss: 10.6260\t Val Acc: 0.0000\n",
      "[Epoch 737/1000] \t Loss: 0.2378\t Acc: 100.0000 \t Val Loss: 11.9248\t Val Acc: 0.0000\n",
      "[Epoch 738/1000] \t Loss: 0.2576\t Acc: 100.0000 \t Val Loss: 11.6936\t Val Acc: 0.0000\n",
      "[Epoch 739/1000] \t Loss: 0.2461\t Acc: 100.0000 \t Val Loss: 12.0700\t Val Acc: 0.0000\n",
      "[Epoch 740/1000] \t Loss: 0.2554\t Acc: 100.0000 \t Val Loss: 12.0708\t Val Acc: 0.0000\n",
      "[Epoch 741/1000] \t Loss: 0.2378\t Acc: 100.0000 \t Val Loss: 11.6724\t Val Acc: 0.0000\n",
      "[Epoch 742/1000] \t Loss: 0.2322\t Acc: 100.0000 \t Val Loss: 11.1439\t Val Acc: 0.0000\n",
      "[Epoch 743/1000] \t Loss: 0.2331\t Acc: 100.0000 \t Val Loss: 12.0096\t Val Acc: 0.0000\n",
      "[Epoch 744/1000] \t Loss: 0.2357\t Acc: 100.0000 \t Val Loss: 11.0136\t Val Acc: 0.0000\n",
      "[Epoch 745/1000] \t Loss: 0.2451\t Acc: 100.0000 \t Val Loss: 11.4626\t Val Acc: 0.0000\n",
      "[Epoch 746/1000] \t Loss: 0.2303\t Acc: 100.0000 \t Val Loss: 11.4709\t Val Acc: 0.0000\n",
      "[Epoch 747/1000] \t Loss: 0.2501\t Acc: 100.0000 \t Val Loss: 11.4904\t Val Acc: 0.0000\n",
      "[Epoch 748/1000] \t Loss: 0.2360\t Acc: 100.0000 \t Val Loss: 11.4726\t Val Acc: 0.0000\n",
      "[Epoch 749/1000] \t Loss: 0.2450\t Acc: 100.0000 \t Val Loss: 11.0928\t Val Acc: 0.0000\n",
      "[Epoch 750/1000] \t Loss: 0.2376\t Acc: 100.0000 \t Val Loss: 11.5476\t Val Acc: 0.0000\n",
      "[Epoch 751/1000] \t Loss: 0.2364\t Acc: 100.0000 \t Val Loss: 12.9437\t Val Acc: 0.0000\n",
      "[Epoch 752/1000] \t Loss: 0.2361\t Acc: 100.0000 \t Val Loss: 11.7850\t Val Acc: 0.0000\n",
      "[Epoch 753/1000] \t Loss: 0.2339\t Acc: 100.0000 \t Val Loss: 11.2107\t Val Acc: 0.0000\n",
      "[Epoch 754/1000] \t Loss: 0.2404\t Acc: 100.0000 \t Val Loss: 11.5120\t Val Acc: 0.0000\n",
      "[Epoch 755/1000] \t Loss: 0.2335\t Acc: 100.0000 \t Val Loss: 11.7156\t Val Acc: 0.0000\n",
      "[Epoch 756/1000] \t Loss: 0.2296\t Acc: 100.0000 \t Val Loss: 11.6375\t Val Acc: 0.0000\n",
      "[Epoch 757/1000] \t Loss: 0.2369\t Acc: 100.0000 \t Val Loss: 11.4829\t Val Acc: 0.0000\n",
      "[Epoch 758/1000] \t Loss: 0.2314\t Acc: 100.0000 \t Val Loss: 11.2749\t Val Acc: 0.0000\n",
      "[Epoch 759/1000] \t Loss: 0.2344\t Acc: 100.0000 \t Val Loss: 10.9966\t Val Acc: 0.0000\n",
      "[Epoch 760/1000] \t Loss: 0.2336\t Acc: 100.0000 \t Val Loss: 13.1631\t Val Acc: 0.0000\n",
      "[Epoch 761/1000] \t Loss: 0.2322\t Acc: 100.0000 \t Val Loss: 11.4136\t Val Acc: 0.0000\n",
      "[Epoch 762/1000] \t Loss: 0.2407\t Acc: 100.0000 \t Val Loss: 11.5105\t Val Acc: 0.0000\n",
      "[Epoch 763/1000] \t Loss: 0.2493\t Acc: 100.0000 \t Val Loss: 12.0904\t Val Acc: 0.0000\n",
      "[Epoch 764/1000] \t Loss: 0.2438\t Acc: 100.0000 \t Val Loss: 11.5846\t Val Acc: 0.0000\n",
      "[Epoch 765/1000] \t Loss: 0.2387\t Acc: 100.0000 \t Val Loss: 11.1245\t Val Acc: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Education\\IITM\\Second Sem\\DL\\3\\main_copy.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Education/IITM/Second%20Sem/DL/3/main_copy.ipynb#Y111sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, target)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Education/IITM/Second%20Sem/DL/3/main_copy.ipynb#Y111sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# Back prop\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Education/IITM/Second%20Sem/DL/3/main_copy.ipynb#Y111sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Education/IITM/Second%20Sem/DL/3/main_copy.ipynb#Y111sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(mod\u001b[39m.\u001b[39mparameters(), max_norm\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Education/IITM/Second%20Sem/DL/3/main_copy.ipynb#Y111sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\madhe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\madhe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_list, acc_list = [], []\n",
    "\n",
    "init_teacher_forcing_ratio = 0.8\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    no_of_batch = 0\n",
    "    running_accuracy = 0\n",
    "\n",
    "    for i in range(0, len(train_source), batch_size):\n",
    "        src_strings = train_source[i: i+batch_size]\n",
    "        tar_strings = train_target[i: i+batch_size]\n",
    "\n",
    "        src_strings = preprocess(src_strings, start_token, end_token, pad_token)\n",
    "        tar_strings = preprocess(tar_strings, start_token, end_token, pad_token)\n",
    "\n",
    "        #transposing to make the shape as expected\n",
    "        inp_data = string_to_tensor(src_strings, en_l2i).transpose(0,1)\n",
    "        target = string_to_tensor(tar_strings, tr_l2i).transpose(0,1)\n",
    "\n",
    "        #here teacher forcing ratio will reduces linearly from init_teacher_forcing_ratio to 0 in half the number of epochs\n",
    "        teacher_forcing_ratio = max(0, init_teacher_forcing_ratio * (1 - (epoch*2/num_epochs)))\n",
    "        output = mod(inp_data, target, teacher_forcing_ratio)\n",
    "\n",
    "        running_accuracy += mod.calc_accuracy(output.argmax(2), target)\n",
    "\n",
    "        # print(\"train: output : \", output.shape)\n",
    "        # result = temp_print(output.argmax(2))\n",
    "        # print(\"result: \", result)\n",
    "        # print(\"target: \", tar_strings)\n",
    "        \n",
    "        # print(\"output: \",result)\n",
    "        # print(\"target:\", tar_strings)\n",
    "\n",
    "        # print(\"op before reshopsed \", output.shape)\n",
    "        # print(\"tar before reshape: \", target.shape)\n",
    "\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        target = target.reshape(-1)\n",
    "\n",
    "        # print(\"op: \", output.shape)\n",
    "        # print(\"tar: \", target.shape)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Back prop\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(mod.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        no_of_batch += 1        \n",
    "\n",
    "    # val_output = mod(val_source_tensor, val_target_tensor)\n",
    "    # val_output = val_output.reshape(-1, val_output.shape[2])\n",
    "    # val_target_tensor = val_target_tensor.reshape(-1)\n",
    "    # # print(f\"val output : {val_output.shape} \\t val_target: {val_target_tensor.shape}\")\n",
    "    # val_loss = criterion(val_output, val_target_tensor)\n",
    "\n",
    "    val_loss, val_accuracy= mod.calc_evaluation_metrics(val_source, val_target)\n",
    "    # val_loss = 0\n",
    "\n",
    "    print(f\"[Epoch {epoch+1:3d}/{num_epochs}] \\t Loss: {(running_loss/no_of_batch):.4f}\\t Acc: {(running_accuracy/no_of_batch):2.2f} \\t Val Loss: {val_loss:2.4f}\\t Val Acc: {val_accuracy:2.2f}\")\n",
    "    loss_list.append(running_loss/no_of_batch)\n",
    "    acc_list.append(running_accuracy/no_of_batch)\n",
    "\n",
    "plt.plot(loss_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
