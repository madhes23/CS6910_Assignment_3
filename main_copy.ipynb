{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "from Utils import plot_graphs\n",
    "from AkshrantarDataset import AksharantarDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'tam'\n",
    "start_token = '<'\n",
    "end_token = '>'\n",
    "pad_token = ' '\n",
    "unk_token = '~'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = AksharantarDataset(language, start_token, end_token, pad_token, unk_token)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=data.tr_l2i[pad_token])\n",
    "\n",
    "target_dict_count = len(data.tr_l2i)\n",
    "english_dict_count = len(data.en_l2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers = 1, p = 0, bi_dir = False, rnn_class = nn.GRU):\n",
    "        \"\"\"\n",
    "        Init Parameters:\n",
    "        input_size : english_dict_count\n",
    "        embedding_size : size of each embedding vector\n",
    "        hidden_size : size of hidden state vector\n",
    "        num_layers : number of recurrent layers of RNN\n",
    "        p : dropout probability\n",
    "        rnn_class: type of RNN to be used in the encoder\n",
    "\n",
    "        Input:\n",
    "        x : torch.Tensor of shape (seq_length, N)\n",
    "            where seq_length - len of longest string in the batch\n",
    "            N - batch size\n",
    "        \n",
    "        Outpus:\n",
    "        outputs: torch.Tensor of shape (seq_len, N, hidden_size * D), where D = 2 if bi_dir = True else 1\n",
    "        hidden: torch.Tensor of shape (num_layers * D, N, hidden_size)\n",
    "        \n",
    "        cell: torch.Tensor of shape (num_layers * D, N, hidden_size) if(rnn_class == \"LSTM\")\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn_class= rnn_class\n",
    "        self.rnn = rnn_class(embedding_size, hidden_size, num_layers, dropout=p, bidirectional = bi_dir)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_length, N) where N is batch size\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (seq_length, N, embedding_size)\n",
    "\n",
    "        if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "            outputs, (hidden, cell) = self.rnn(embedding)\n",
    "            # outputs shape: (seq_length, N, hidden_size)\n",
    "        else:\n",
    "            outputs, hidden = self.rnn(embedding)\n",
    "        \n",
    "        if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "            return outputs, hidden, cell\n",
    "        else:\n",
    "            return outputs, hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers = 1, p = 0, bi_dir = False, rnn_class = nn.GRU):\n",
    "        \"\"\"input size = output size = target language charecters\n",
    "        Init Parameters:\n",
    "        input_size: target_dict_count\n",
    "        embedding_size: size of each embedding vector\n",
    "        hidden_size: size of hidden state vector\n",
    "        output_size: number of output features in fully connected layer\n",
    "        num_layers : number of recurrent layers of RNN\n",
    "        p : dropout probability\n",
    "        rnn_class: type of RNN to be used in the encoder\n",
    "\n",
    "        Input:\n",
    "        x: torch.Tensor of shape (N)\n",
    "        hidden: torch.Tensor of shape (num_layers * D, N, hidden_size)\n",
    "        cell: torch.Tensor of shape (num_layers * D, N, hidden_size)\n",
    "\n",
    "        Outputs:\n",
    "        predications: torch.Tensor of shape (N, target_dict_count), where D = 2 if bi_dir = True else 1\n",
    "        hidden: torch.Tensor of shape (num_layers * D, N, hidden_size)\n",
    "        \n",
    "        cell: torch.Tensor of shape (num_layers * D, N, hidden_size) if(rnn_class == \"LSTM\")\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn_class = rnn_class\n",
    "        self.rnn = rnn_class(embedding_size, hidden_size, num_layers, dropout=p, bidirectional = bi_dir)\n",
    "\n",
    "        self.D = 1\n",
    "        if(bi_dir == True):\n",
    "            self.D = 2\n",
    "        self.fc = nn.Linear(hidden_size * self.D, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden, cell = None):\n",
    "        #cell is set to none, for GRU and RNN\n",
    "\n",
    "        # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n",
    "        # is 1 here because we are sending in a single word and not a sentence\n",
    "        # print(x.shape, hidden.shape, cell.shape)\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (1, N, embedding_size)\n",
    "        \n",
    "        if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "            outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
    "            # outputs shape: (1, N, hidden_size * D)\n",
    "        else:\n",
    "            outputs, hidden = self.rnn(embedding, hidden)\n",
    "            \n",
    "\n",
    "        predictions = self.fc(outputs)\n",
    "\n",
    "        # predictions shape: (1, N, length_target_vocabulary) to send it to\n",
    "        # loss function we want it to be (N, length_target_vocabulary) so we're\n",
    "        # just gonna remove the first dim\n",
    "        predictions = predictions.squeeze(0)\n",
    "\n",
    "        if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "            return predictions, hidden, cell\n",
    "        else:\n",
    "            return predictions, hidden"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, output_size, num_layers = 1, dropout_p=0.1, max_length=30, bi_dir = False, rnn_class = nn.GRU):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size= embedding_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        self.num_layers = num_layers\n",
    "        self.D = 1\n",
    "        if(bi_dir == True):\n",
    "            self.D = 2\n",
    "        self.rnn_class = rnn_class\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "        self.attn = nn.Linear((self.D * num_layers * hidden_size) + embedding_size, max_length)\n",
    "        self.attn_combine = nn.Linear((self.D * hidden_size) + embedding_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.rnn = self.rnn_class(hidden_size, hidden_size, num_layers, dropout = dropout_p, bidirectional = bi_dir)\n",
    "        self.out = nn.Linear(hidden_size * self.D, output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input)\n",
    "        embedded = self.dropout(embedded).unsqueeze(0)\n",
    "        print(\"embed \",embedded.shape)\n",
    "        # (1, N, es)\n",
    "        \n",
    "        print(\"hidd\", hidden.shape)\n",
    "        #IDEA: cat for each in 0th dim of hidd\n",
    "\n",
    "        temp = torch.cat((embedded, hidden[0].unsqueeze(0)), 2)\n",
    "        for i in range(hidden.shape[0]-1):\n",
    "            temp = torch.cat((temp, hidden[i].unsqueeze(0)), 2)\n",
    "\n",
    "        print(\"temp \", temp.shape) # (1, N, (d*nl).hs+es)\n",
    "\n",
    "        temp = self.attn(temp)\n",
    "        print(\"after attn \", temp.shape) # (1, N, max)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            temp, dim=1)\n",
    "        print(\"attn_weights :\",attn_weights.shape) # (1, N, max)\n",
    "        print(\"ecn_op: \", encoder_outputs.shape)\n",
    "        attn_applied = torch.bmm(attn_weights.transpose(0,1),\n",
    "                                 encoder_outputs.transpose(0,1))\n",
    "        #attn_applied (N, 1, hs)\n",
    "\n",
    "        attn_applied = attn_applied.transpose(0,1) #(1, N, d.hs)\n",
    "        \n",
    "        print(\"attn appld \", attn_applied.shape) # (1, N, d.hs)\n",
    "\n",
    "        output = torch.cat((embedded, attn_applied), 2)\n",
    "        print(\"outpt after cat \",output.shape) # (1, N, (D)hs+es)\n",
    "\n",
    "        output = self.attn_combine(output)\n",
    "        print(\"after atn comb: \",output.shape) # (1, N, hs)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.rnn(output, hidden) \n",
    "        print(\"out \", output.shape, \"hid \", hidden.shape)\n",
    "\n",
    "        prob = self.out(output).squeeze(0) #(1, N, op)\n",
    "        print(\"prob \", prob.shape)\n",
    "\n",
    "        # output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return prob, hidden, attn_weights\n",
    "\n",
    "    # def initHidden(self):\n",
    "    #     return torch.zeros(1, batch_size, self.hidden_size, device=device)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 2, 10]) torch.Size([1, 2, 10])\n",
      "embed  torch.Size([1, 2, 5])\n",
      "hidd torch.Size([1, 2, 10])\n",
      "temp  torch.Size([1, 2, 15])\n",
      "after attn  torch.Size([1, 2, 50])\n",
      "attn_weights : torch.Size([1, 2, 50])\n",
      "ecn_op:  torch.Size([50, 2, 10])\n",
      "attn appld  torch.Size([1, 2, 10])\n",
      "outpt after cat  torch.Size([1, 2, 15])\n",
      "after atn comb:  torch.Size([1, 2, 10])\n",
      "out  torch.Size([1, 2, 10]) hid  torch.Size([1, 2, 10])\n",
      "prob  torch.Size([2, 40])\n",
      "torch.Size([2, 40]) torch.Size([1, 2, 10]) torch.Size([1, 2, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\madhe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 50\n",
    "enc = Encoder(input_size= 20, embedding_size=5, hidden_size=10)\n",
    "o, h = enc(torch.rand(MAX_LEN, 2).int())\n",
    "print(o.shape, h.shape)\n",
    "dec = AttnDecoderRNN(embedding_size = 5, hidden_size=10, output_size=40)\n",
    "p, h, at = dec(torch.zeros(2).int(), h, o)\n",
    "print(p.shape, h.shape, at.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        encoder_layers = encoder.num_layers\n",
    "        decoder_layers = decoder.num_layers\n",
    "        D = decoder.D #we set bidiretion as common in both encoder and decoder, so no need to check for D value seperately\n",
    "        self.enc_to_dec = nn.Linear(encoder_layers*D, decoder_layers*D)\n",
    "        self.rnn_class = decoder.rnn_class #we use same rnn in both encoder and decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_forcing = False):\n",
    "        \"\"\"source : (source_len, N) - not sure\n",
    "        teacher_forching_ratio : probability in which original values is favored over predicted values\n",
    "                                if 0 : predicted values is passed for all chars in target\n",
    "                                if 1 : true values is passed for all chars in target\n",
    "\n",
    "        \"\"\"\n",
    "        batch_size = source.shape[1] \n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = target_dict_count\n",
    "\n",
    "        # print(\"source shape \", source.shape)\n",
    "        # print(\"target shape \", target.shape)\n",
    "        # print(\"N : \", batch_size)\n",
    "        # print(\"tar len : \", target_len)\n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size)\n",
    "        # print(\"outputs shape : \", outputs.shape)\n",
    "\n",
    "        \n",
    "        if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "            enc_ops, hidden, cell = self.encoder(source)\n",
    "        else:\n",
    "            enc_ops, hidden = self.encoder(source)\n",
    "\n",
    "        N = hidden.shape[1]\n",
    "        hidden_size= hidden.shape[2]\n",
    "        # hidden, cell shape: (D*encoder_layers, N, hidden_size)\n",
    "\n",
    "        #UNCOMMENT THIS TO HANDLE dinstinct encoder and decoder layers\n",
    "        # hidden = hidden.transpose(0, 2) # hidden shape: (hidden_size, N, D*encoder_layers)\n",
    "        # hidden = hidden.reshape(-1, hidden.shape[2]) # hidden shape: (hidden_size * N, D*encoder_layers)\n",
    "        # hidden = self.enc_to_dec(hidden) # hidden shape: (hidden_size * N, D*decoder_layers)\n",
    "        # hidden = hidden.reshape(hidden_size, N, hidden.shape[1]) # hidden shape: (hidden_size, N, D*decoder_layers)\n",
    "        # hidden = hidden.transpose(0,2) # hidden shape: (D*decoder_layers, N, hidden_size)\n",
    "\n",
    "        # if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "        #     #at all the below steps, cell will have the shape of hidden\n",
    "        #     cell = cell.transpose(0,2)\n",
    "        #     cell = cell.reshape(-1, cell.shape[2])\n",
    "        #     cell = self.enc_to_dec(cell)\n",
    "        #     cell = cell.reshape(hidden_size, N, cell.shape[1])\n",
    "        #     cell = cell.transpose(0,2)\n",
    "\n",
    "\n",
    "        # Grab the first input to the Decoder\n",
    "        x = target[0]\n",
    "        outputs[:, :, data.tr_l2i[start_token]] = 1 #setting prob = 1 for starting token \n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            # Use previous hidden, cell as context from encoder at start\n",
    "            if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "                # output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "                output, hidden, cell, attention = self.decoder(x, hidden, cell, enc_ops)\n",
    "            else:\n",
    "                output, hidden, atn = self.decoder(x, hidden, enc_ops)\n",
    "\n",
    "            # Store next output prediction\n",
    "            outputs[t] = output\n",
    "\n",
    "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
    "            best_guess = output.argmax(1)\n",
    "\n",
    "            # With probability of teacher_force_ratio we take the actual next word\n",
    "            # otherwise we take the word that the Decoder predicted it to be.\n",
    "            # Teacher Forcing is used so that the model gets used to seeing\n",
    "            # similar inputs at training and testing time, if teacher forcing is 1\n",
    "            # then inputs at test time might be completely different than what the\n",
    "            # network is used to. This was a long comment.\n",
    "            x = target[t] if teacher_forcing == True else best_guess\n",
    "        # print(\"OUTPUTS: \", outputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def calc_accuracy(self, output, target):\n",
    "        \"\"\"\n",
    "        output: torch.Tensor of shape (seq_len, N)\n",
    "        target: torch.Tensor of shape (seq_len, N)\n",
    "        \"\"\"\n",
    "        # batch_size = 32\n",
    "        running_acc = 0\n",
    "        seq_len = output.shape[0]\n",
    "        N = output.shape[1]\n",
    "        matched_strings = 0\n",
    "        with torch.no_grad():\n",
    "            for j in range(N):\n",
    "                current_word_matched = True\n",
    "                for i in range(seq_len):\n",
    "                    if(target[i][j] == data.tr_l2i[pad_token]): #we dont care whatever prediction in the pad_token place\n",
    "                        break\n",
    "                    if(output[i][j] != target[i][j]): #compare the predictions of charecters, start and end_token places\n",
    "                        current_word_matched = False\n",
    "                        break\n",
    "                if(current_word_matched == True):\n",
    "                    matched_strings += 1\n",
    "        return matched_strings*100 / N \n",
    "    \n",
    "\n",
    "    def calc_evaluation_metrics(self, src_tar_pair):\n",
    "        \"\"\"\n",
    "        src_tar_pair: a list of tuples, where each tuple consists of two tensors (source, target)\n",
    "                      source:tensors of shape (seq_len * N), note: seq_len might not be same across all the tensors in the list\n",
    "                      target:tensors of shape (seq_len * N) \n",
    "        \"\"\"\n",
    "        num_batches = len(src_tar_pair)\n",
    "        with torch.no_grad():\n",
    "            acc = 0\n",
    "            running_loss = 0\n",
    "            for source, target in src_tar_pair:\n",
    "                source = source.to(device)\n",
    "                target = target.to(device)\n",
    "                \n",
    "                output = self(source, target).to(device)\n",
    "                acc += self.calc_accuracy(output.argmax(2), target)\n",
    "\n",
    "                output = output.reshape(-1, output.shape[2])\n",
    "                target = target.reshape(-1)\n",
    "\n",
    "                loss = criterion(output, target)\n",
    "                running_loss += loss.item()\n",
    "        return running_loss/num_batches, acc/num_batches\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 150\n",
    "learning_rate = 0.01\n",
    "batch_size = 2\n",
    "\n",
    "# Model hyperparameters\n",
    "input_size_encoder = english_dict_count\n",
    "input_size_decoder = target_dict_count\n",
    "output_size = target_dict_count\n",
    "embedding_size = 5\n",
    "encoder_layers = 1\n",
    "decoder_layers = 1\n",
    "enc_dropout = 0.3\n",
    "dec_dropout = 0.3\n",
    "hidden_size = 10\n",
    "bi_directional = False\n",
    "rnn = nn.GRU\n",
    "MAX_LEN = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.load_data(\"train\", batch_size, num_batches=1, padding_lower_bound=MAX_LEN)\n",
    "valid = data.load_data(\"valid\", batch_size, num_batches=1, padding_lower_bound=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\madhe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "enc = Encoder(english_dict_count, embedding_size, hidden_size, \n",
    "              num_layers=encoder_layers, \n",
    "              bi_dir=bi_directional,\n",
    "              p=enc_dropout,\n",
    "              rnn_class=rnn).to(device)\n",
    "# dec = Decoder(target_dict_count, embedding_size, hidden_size, target_dict_count, \n",
    "#               num_layers=decoder_layers, \n",
    "#               bi_dir=bi_directional, \n",
    "#               p = dec_dropout,\n",
    "#               rnn_class=rnn).to(device)\n",
    "\n",
    "dec = AttnDecoderRNN(embedding_size, hidden_size, output_size=target_dict_count, \n",
    "                     num_layers=1,\n",
    "                     bi_dir = bi_directional,\n",
    "                     rnn_class= rnn,\n",
    "                     max_length=MAX_LEN).to(device)\n",
    "\n",
    "mod = Seq2Seq(enc, dec).to(device)\n",
    "\n",
    "optimizer = optim.Adam(mod.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list, acc_list, val_loss_list, val_acc_list = [], [], [], []\n",
    "teacher_forcing_ratio = 0.5\n",
    "print_batches = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    running_accuracy = 0\n",
    "    total_batches = len(train)\n",
    "\n",
    "    # for inp_data, target in tqdm(train, desc=f\"[Epoch {epoch+1:3d}/{num_epochs}] \", leave = False):        \n",
    "    for inp_data, target in train:        \n",
    "        inp_data = inp_data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        teacher_forcing = False\n",
    "        if(epoch < 0.5*num_epochs): #for inital epochs, batch wise tfr\n",
    "            teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "        \n",
    "        output = mod(inp_data, target, teacher_forcing).to(device)\n",
    "        \n",
    "        if(epoch == num_epochs-1 and print_batches != 0):\n",
    "            print(data.tensor_to_string(output.argmax(2)))\n",
    "            print(data.tensor_to_string(target))\n",
    "            print_batches -= 1\n",
    "\n",
    "        running_accuracy += mod.calc_accuracy(output.argmax(2), target)\n",
    "\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        target = target.reshape(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(mod.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    val_loss, val_accuracy= mod.calc_evaluation_metrics(valid)\n",
    "\n",
    "    print(f\"[Epoch {epoch+1:3d}/{num_epochs}] \\t Loss: {(running_loss/total_batches):.3f}\\t Acc: {(running_accuracy/total_batches):2.2f} \\t Val Loss: {val_loss:2.3f}\\t Val Acc: {val_accuracy:2.2f}\")\n",
    "    loss_list.append(running_loss/total_batches)\n",
    "    acc_list.append(running_accuracy/total_batches)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_acc_list.append(val_accuracy)\n",
    "\n",
    "fig = plot_graphs(loss_list, val_loss_list, acc_list, val_acc_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
