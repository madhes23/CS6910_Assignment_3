{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from Utils import plot_graphs\n",
    "from AkshrantarDataset import AksharantarDataset\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'tam'\n",
    "start_token = '<'\n",
    "end_token = '>'\n",
    "pad_token = ' '\n",
    "unk_token = '~'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = AksharantarDataset(language, start_token, end_token, pad_token, unk_token)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=data.tr_l2i[pad_token])\n",
    "\n",
    "target_dict_count = len(data.tr_l2i)\n",
    "english_dict_count = len(data.en_l2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers = 1, p = 0, bi_dir = False, rnn_class = nn.GRU):\n",
    "        \"\"\"\n",
    "        Init Parameters:\n",
    "        input_size : english_dict_count\n",
    "        embedding_size : size of each embedding vector\n",
    "        hidden_size : size of hidden state vector\n",
    "        num_layers : number of recurrent layers of RNN\n",
    "        p : dropout probability\n",
    "        rnn_class: type of RNN to be used in the encoder\n",
    "\n",
    "        Input:\n",
    "        x : torch.Tensor of shape (seq_length, N)\n",
    "            where seq_length - len of longest string in the batch\n",
    "            N - batch size\n",
    "        \n",
    "        Outpus:\n",
    "        outputs: torch.Tensor of shape (seq_len, N, hidden_size * D), where D = 2 if bi_dir = True else 1\n",
    "        hidden: torch.Tensor of shape (num_layers * D, N, hidden_size)\n",
    "        \n",
    "        cell: torch.Tensor of shape (num_layers * D, N, hidden_size) if(rnn_class == \"LSTM\")\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn_class= rnn_class\n",
    "        self.rnn = rnn_class(embedding_size, hidden_size, num_layers, dropout=p, bidirectional = bi_dir)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_length, N) where N is batch size\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (seq_length, N, embedding_size)\n",
    "\n",
    "        if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "            outputs, (hidden, cell) = self.rnn(embedding)\n",
    "            # outputs shape: (seq_length, N, hidden_size)\n",
    "        else:\n",
    "            outputs, hidden = self.rnn(embedding)\n",
    "        \n",
    "        if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "            return outputs, hidden, cell\n",
    "        else:\n",
    "            return outputs, hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers = 1, p = 0, bi_dir = False, rnn_class = nn.GRU):\n",
    "        \"\"\"input size = output size = target language charecters\n",
    "        Init Parameters:\n",
    "        input_size: target_dict_count\n",
    "        embedding_size: size of each embedding vector\n",
    "        hidden_size: size of hidden state vector\n",
    "        output_size: number of output features in fully connected layer\n",
    "        num_layers : number of recurrent layers of RNN\n",
    "        p : dropout probability\n",
    "        rnn_class: type of RNN to be used in the encoder\n",
    "\n",
    "        Input:\n",
    "        x: torch.Tensor of shape (N)\n",
    "        hidden: torch.Tensor of shape (num_layers * D, N, hidden_size)\n",
    "        cell: torch.Tensor of shape (num_layers * D, N, hidden_size)\n",
    "\n",
    "        Outputs:\n",
    "        predications: torch.Tensor of shape (N, target_dict_count), where D = 2 if bi_dir = True else 1\n",
    "        hidden: torch.Tensor of shape (num_layers * D, N, hidden_size)\n",
    "        \n",
    "        cell: torch.Tensor of shape (num_layers * D, N, hidden_size) if(rnn_class == \"LSTM\")\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.used_attn = False\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn_class = rnn_class\n",
    "        self.rnn = rnn_class(embedding_size, hidden_size, num_layers, dropout=p, bidirectional = bi_dir)\n",
    "\n",
    "        self.D = 1\n",
    "        if(bi_dir == True):\n",
    "            self.D = 2\n",
    "        self.fc = nn.Linear(hidden_size * self.D, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden, cell = None, encoder_outputs = None):\n",
    "        #cell is set to none, for GRU and RNN\n",
    "\n",
    "        # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n",
    "        # is 1 here because we are sending in a single word and not a sentence\n",
    "        # print(x.shape, hidden.shape, cell.shape)\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (1, N, embedding_size)\n",
    "        \n",
    "        if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "            outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
    "            # outputs shape: (1, N, hidden_size * D)\n",
    "        else:\n",
    "            outputs, hidden = self.rnn(embedding, hidden)\n",
    "            \n",
    "\n",
    "        predictions = self.fc(outputs)\n",
    "\n",
    "        # predictions shape: (1, N, length_target_vocabulary) to send it to\n",
    "        # loss function we want it to be (N, length_target_vocabulary) so we're\n",
    "        # just gonna remove the first dim\n",
    "        predictions = predictions.squeeze(0)\n",
    "\n",
    "        if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "            return predictions, hidden, cell\n",
    "        else:\n",
    "            return predictions, hidden"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoder(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, output_size, num_layers = 1, dropout_p=0.1, max_length=30, bi_dir = False, rnn_class = nn.GRU):\n",
    "        super(AttnDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size= embedding_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        self.num_layers = num_layers\n",
    "        self.used_attn = True\n",
    "        self.D = 1\n",
    "        if(bi_dir == True):\n",
    "            self.D = 2\n",
    "        self.rnn_class = rnn_class\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "        self.attn = nn.Linear((self.D * num_layers * hidden_size) + embedding_size, max_length)\n",
    "        self.attn_combine = nn.Linear((self.D * hidden_size) + embedding_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.rnn = self.rnn_class(hidden_size, hidden_size, num_layers, dropout = dropout_p, bidirectional = bi_dir)\n",
    "        self.out = nn.Linear(hidden_size * self.D, output_size)\n",
    "\n",
    "    def forward(self, input, hidden, cell = None, encoder_outputs = None):\n",
    "        embedded = self.embedding(input)\n",
    "        embedded = self.dropout(embedded).unsqueeze(0)\n",
    "        # print(\"embed \",embedded.shape)\n",
    "        # (1, N, es)\n",
    "        \n",
    "        # print(\"hidd\", hidden.shape)\n",
    "        #IDEA: cat for each in 0th dim of hidd\n",
    "\n",
    "        temp = torch.cat((embedded, hidden[0].unsqueeze(0)), 2)\n",
    "        for i in range(hidden.shape[0]-1):\n",
    "            temp = torch.cat((temp, hidden[i].unsqueeze(0)), 2)\n",
    "\n",
    "        # print(\"temp \", temp.shape) # (1, N, (d*nl).hs+es)\n",
    "\n",
    "        temp = self.attn(temp)\n",
    "        # print(\"after attn \", temp.shape) # (1, N, max)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            temp, dim=1)\n",
    "        # print(\"attn_weights :\",attn_weights.shape) # (1, N, max)\n",
    "        # print(\"ecn_op: \", encoder_outputs.shape)\n",
    "        # print(\"attn wei: \", attn_weights.transpose(0,1).shape)\n",
    "        # print(\"enc_opts \", encoder_outputs.transpose(0,1).shape)\n",
    "        \n",
    "        attn_applied = torch.bmm(attn_weights.transpose(0,1),\n",
    "                                 encoder_outputs.transpose(0,1))\n",
    "        # attn_applied (N, 1, hs)\n",
    "\n",
    "        attn_applied = attn_applied.transpose(0,1) #(1, N, d.hs)\n",
    "        \n",
    "        # print(\"attn appld \", attn_applied.shape) # (1, N, d.hs)\n",
    "\n",
    "        output = torch.cat((embedded, attn_applied), 2)\n",
    "        # print(\"outpt after cat \",output.shape) # (1, N, (D)hs+es)\n",
    "\n",
    "        output = self.attn_combine(output)\n",
    "        # print(\"after atn comb: \",output.shape) # (1, N, hs)\n",
    "        \n",
    "        output = F.relu(output)\n",
    "        if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "            output, (hidden, cell) = self.rnn(output, (hidden, cell)) \n",
    "        else:\n",
    "            output, hidden = self.rnn(output, hidden)\n",
    "\n",
    "        # print(\"out \", output.shape, \"hid \", hidden.shape)\n",
    "\n",
    "        prob = self.out(output).squeeze(0) #(1, N, op)\n",
    "        # print(\"prob \", prob.shape)\n",
    "\n",
    "        # output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "            return prob, hidden, cell, attn_weights\n",
    "        else:\n",
    "            return prob, hidden, attn_weights\n",
    "\n",
    "    # def initHidden(self):\n",
    "    #     return torch.zeros(1, batch_size, self.hidden_size, device=device)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.attn_dec = decoder.used_attn\n",
    "        self.encoder_layers = encoder.num_layers\n",
    "        self.decoder_layers = decoder.num_layers\n",
    "        self.D = decoder.D #we set bidiretion as common in both encoder and decoder, so no need to check for D value seperately\n",
    "        self.enc_to_dec = nn.Linear(self.encoder_layers*self.D, self.decoder_layers*self.D)\n",
    "        self.rnn_class = decoder.rnn_class #we use same rnn in both encoder and decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_forcing = False):\n",
    "        \"\"\"source : (source_len, N) - not sure\n",
    "        teacher_forching_ratio : probability in which original values is favored over predicted values\n",
    "                                if 0 : predicted values is passed for all chars in target\n",
    "                                if 1 : true values is passed for all chars in target\n",
    "\n",
    "        \"\"\"\n",
    "        batch_size = source.shape[1] \n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = target_dict_count\n",
    "\n",
    "        # print(\"source shape \", source.shape)\n",
    "        # print(\"target shape \", target.shape)\n",
    "        # print(\"N : \", batch_size)\n",
    "        # print(\"tar len : \", target_len)\n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size)\n",
    "        # print(\"outputs shape : \", outputs.shape)\n",
    "\n",
    "        \n",
    "        if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "            enc_ops, hidden, cell = self.encoder(source)\n",
    "        else:\n",
    "            enc_ops, hidden = self.encoder(source)\n",
    "        \n",
    "        if(self.encoder_layers > self.decoder_layers): # take only the top layers\n",
    "            hidden = hidden[-self.D * self.decoder_layers: , : , : ]\n",
    "        elif(self.encoder_layers < self.decoder_layers): #repeat the top most layer to decoder_layer number of times (without the loss of bidirectional info)\n",
    "            last = hidden[-self.D * 1: , : , :]\n",
    "            # print(\"last : \", last.shape)\n",
    "            hidden = last.repeat(self.decoder_layers, 1, 1)\n",
    "            # print(\"hidden after \", hidden.shape)        \n",
    "\n",
    "\n",
    "        if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "            if(self.encoder_layers > self.decoder_layers): # take only the top layers\n",
    "                cell = cell[-self.D * self.decoder_layers: , : , : ]\n",
    "            elif(self.encoder_layers < self.decoder_layers): #repeat the top most layer to decoder_layer number of times (without the loss of bidirectional info)\n",
    "                last = cell[-self.D * 1: , : , :]\n",
    "                cell = last.repeat(self.decoder_layers, 1, 1)\n",
    "\n",
    "        # hidden, cell shape: (D*encoder_layers, N, hidden_size)\n",
    "\n",
    "        #UNCOMMENT THIS TO HANDLE dinstinct encoder and decoder layers\n",
    "        # hidden = hidden.transpose(0, 2) # hidden shape: (hidden_size, N, D*encoder_layers)\n",
    "        # hidden = hidden.reshape(-1, hidden.shape[2]) # hidden shape: (hidden_size * N, D*encoder_layers)\n",
    "        # hidden = self.enc_to_dec(hidden) # hidden shape: (hidden_size * N, D*decoder_layers)\n",
    "        # hidden = hidden.reshape(hidden_size, N, hidden.shape[1]) # hidden shape: (hidden_size, N, D*decoder_layers)\n",
    "        # hidden = hidden.transpose(0,2) # hidden shape: (D*decoder_layers, N, hidden_size)\n",
    "\n",
    "        # if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "        #     #at all the below steps, cell will have the shape of hidden\n",
    "        #     cell = cell.transpose(0,2)\n",
    "        #     cell = cell.reshape(-1, cell.shape[2])\n",
    "        #     cell = self.enc_to_dec(cell)\n",
    "        #     cell = cell.reshape(hidden_size, N, cell.shape[1])\n",
    "        #     cell = cell.transpose(0,2)\n",
    "\n",
    "\n",
    "        # Grab the first input to the Decoder\n",
    "        x = target[0]\n",
    "        outputs[:, :, data.tr_l2i[start_token]] = 1 #setting prob = 1 for starting token \n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            # Use previous hidden, cell as context from encoder at start\n",
    "            if(self.rnn_class.__name__ == \"LSTM\"):\n",
    "                if(self.attn_dec == True):\n",
    "                    output, hidden, cell, attention = self.decoder(x, hidden, cell, encoder_outputs= enc_ops)\n",
    "                else:\n",
    "                    output, hidden, cell = self.decoder(x, hidden, cell, encoder_outputs= enc_ops)\n",
    "            else:\n",
    "                if(self.attn_dec == True):\n",
    "                    output, hidden, attention = self.decoder(x, hidden, encoder_outputs=enc_ops)\n",
    "                else:\n",
    "                    output, hidden = self.decoder(x, hidden, encoder_outputs=enc_ops)\n",
    "\n",
    "            # Store next output prediction\n",
    "            outputs[t] = output\n",
    "\n",
    "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
    "            best_guess = output.argmax(1)\n",
    "\n",
    "            # With probability of teacher_force_ratio we take the actual next word\n",
    "            # otherwise we take the word that the Decoder predicted it to be.\n",
    "            # Teacher Forcing is used so that the model gets used to seeing\n",
    "            # similar inputs at training and testing time, if teacher forcing is 1\n",
    "            # then inputs at test time might be completely different than what the\n",
    "            # network is used to. This was a long comment.\n",
    "            x = target[t] if teacher_forcing == True else best_guess\n",
    "        # print(\"OUTPUTS: \", outputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def calc_accuracy(self, output, target):\n",
    "        \"\"\"\n",
    "        output: torch.Tensor of shape (seq_len, N)\n",
    "        target: torch.Tensor of shape (seq_len, N)\n",
    "        \"\"\"\n",
    "        # batch_size = 32\n",
    "        seq_len = output.shape[0]\n",
    "        N = output.shape[1]\n",
    "        matched_strings = 0\n",
    "        with torch.no_grad():\n",
    "            for j in range(N):\n",
    "                current_word_matched = True\n",
    "                for i in range(seq_len):\n",
    "                    if(target[i][j] == data.tr_l2i[pad_token]): #we dont care whatever prediction in the pad_token place\n",
    "                        break\n",
    "                    if(output[i][j] != target[i][j]): #compare the predictions of charecters, start and end_token places\n",
    "                        current_word_matched = False\n",
    "                        break\n",
    "                if(current_word_matched == True):\n",
    "                    matched_strings += 1\n",
    "        return matched_strings*100 / N \n",
    "    \n",
    "\n",
    "    def calc_evaluation_metrics(self, src_tar_pair):\n",
    "        \"\"\"\n",
    "        src_tar_pair: a list of tuples, where each tuple consists of two tensors (source, target)\n",
    "                      source:tensors of shape (seq_len * N), note: seq_len might not be same across all the tensors in the list\n",
    "                      target:tensors of shape (seq_len * N) \n",
    "        \"\"\"\n",
    "        num_batches = len(src_tar_pair)\n",
    "        with torch.no_grad():\n",
    "            acc = 0\n",
    "            running_loss = 0\n",
    "            for source, target in src_tar_pair:\n",
    "                source = source.to(device)\n",
    "                target = target.to(device)\n",
    "                \n",
    "                output = self(source, target).to(device)\n",
    "                acc += self.calc_accuracy(output.argmax(2), target)\n",
    "\n",
    "                output = output.reshape(-1, output.shape[2])\n",
    "                target = target.reshape(-1)\n",
    "\n",
    "                loss = criterion(output, target)\n",
    "                running_loss += loss.item()\n",
    "        return running_loss/num_batches, acc/num_batches\n",
    "\n",
    "\n",
    "    def learn(self, train, valid, num_epochs, optimizer):\n",
    "        loss_list, acc_list, val_loss_list, val_acc_list = [], [], [], []\n",
    "        teacher_forcing_ratio = 0.5\n",
    "        print_batches = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            running_loss = 0\n",
    "            running_accuracy = 0\n",
    "            total_batches = len(train)\n",
    "\n",
    "            for inp_data, target in tqdm(train, desc=f\"[Epoch {epoch+1:3d}/{num_epochs}] \", leave = False):        \n",
    "            # for inp_data, target in train:        \n",
    "                inp_data = inp_data.to(device)\n",
    "                target = target.to(device)\n",
    "                \n",
    "                teacher_forcing = False\n",
    "                if(epoch < 0.5*num_epochs): #for inital epochs, batch wise tfr\n",
    "                    teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "                \n",
    "                output = self(inp_data, target, teacher_forcing).to(device)\n",
    "                \n",
    "                if(epoch == num_epochs-1 and print_batches != 0):\n",
    "                    print(data.tensor_to_string(output.argmax(2)))\n",
    "                    print(data.tensor_to_string(target))\n",
    "                    print_batches -= 1\n",
    "\n",
    "                running_accuracy += self.calc_accuracy(output.argmax(2), target)\n",
    "\n",
    "                output = output.reshape(-1, output.shape[2])\n",
    "                target = target.reshape(-1)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1)\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            val_loss, val_accuracy= self.calc_evaluation_metrics(valid)\n",
    "\n",
    "            print(f\"[Epoch {epoch+1:3d}/{num_epochs}] \\t Loss: {(running_loss/total_batches):.3f}\\t Acc: {(running_accuracy/total_batches):2.2f} \\t Val Loss: {val_loss:2.3f}\\t Val Acc: {val_accuracy:2.2f}\")\n",
    "            loss_list.append(running_loss/total_batches)\n",
    "            acc_list.append(running_accuracy/total_batches)\n",
    "            val_loss_list.append(val_loss)\n",
    "            val_acc_list.append(val_accuracy)\n",
    "        return loss_list, val_loss_list, acc_list, val_acc_list\n",
    "\n",
    "        # fig = plot_graphs(loss_list, val_loss_list, acc_list, val_acc_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 15\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "# Model hyperparameters\n",
    "input_size_encoder = english_dict_count\n",
    "input_size_decoder = target_dict_count\n",
    "output_size = target_dict_count\n",
    "MAX_LEN = 32\n",
    "\n",
    "embedding_size = 32\n",
    "encoder_layers = 3\n",
    "decoder_layers = 3\n",
    "enc_dropout = 0.3\n",
    "dec_dropout = 0.3\n",
    "hidden_size = 256\n",
    "bi_directional = True\n",
    "rnn = nn.LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(english_dict_count, embedding_size, hidden_size, \n",
    "              num_layers=encoder_layers, \n",
    "              bi_dir=bi_directional,\n",
    "              p=enc_dropout,\n",
    "              rnn_class=rnn).to(device)\n",
    "\n",
    "dec = Decoder(target_dict_count, embedding_size, hidden_size, target_dict_count, \n",
    "              num_layers=decoder_layers, \n",
    "              bi_dir=bi_directional, \n",
    "              p = dec_dropout,\n",
    "              rnn_class=rnn).to(device)\n",
    "\n",
    "# dec = AttnDecoder(embedding_size, hidden_size, output_size=target_dict_count, \n",
    "#                      num_layers=decoder_layers,\n",
    "#                      bi_dir = bi_directional,\n",
    "#                      rnn_class= rnn,\n",
    "#                      max_length=MAX_LEN).to(device)\n",
    "\n",
    "mod = Seq2Seq(enc, dec).to(device)\n",
    "\n",
    "optimizer = optim.Adam(mod.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(dec.used_attn == True):\n",
    "    train = data.load_data(\"train\", batch_size, padding_upper_bound=MAX_LEN)\n",
    "    valid = data.load_data(\"valid\", batch_size, padding_upper_bound=MAX_LEN)\n",
    "else:\n",
    "    train = data.load_data(\"train\", batch_size)\n",
    "    valid = data.load_data(\"valid\", batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l, vl, a, va = mod.learn(train, valid, num_epochs, optimizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sweep (wihout Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key = \"0a917cda293aea77d7c547537888b7551134c78d\")\n",
    "sweep_config = {\n",
    "    \"method\": 'bayes',\n",
    "    \"metric\": {\n",
    "    'name': 'accuracy',\n",
    "    'goal': 'maximize'\n",
    "    },\n",
    "    'parameters' :{\n",
    "        \"num_epochs\" : {\"min\": 5, \"max\": 15}, \n",
    "        \"learning_rate\" : {\"values\": [1e-2, 1e-3, 1e-4]},\n",
    "        \"encoder_layers\": {\"values\": [1,2,3]},\n",
    "        \"decoder_layers\": {\"values\": [1,2,3]},\n",
    "        \"hidden_size\" : {\"values\": [16, 32, 64, 128, 256]},\n",
    "        \"rnn\" : {\"values\" : [\"LSTM\", \"GRU\", \"RNN\"]},\n",
    "        \"bi_directional\" : {\"values\": [True, False]},\n",
    "        \"enc_dropout\" : {\"values\": [0.2, 0.3]},\n",
    "        \"dec_dropout\" : {\"values\": [0.2, 0.3]}\n",
    "    }\n",
    "}\n",
    "\n",
    "def tune_rnn():\n",
    "    \"\"\"A utility function for performing the sweep\"\"\"\n",
    "    wandb.init()\n",
    "\n",
    "    name = \"{wandb.config.encoder_layers}_enc_{wandb.config.decoder_layers}_dec_{wandb.config.hidden_size}_hs_without_attn\"\n",
    "    if(wandb.config.bi_directional == True):\n",
    "       name += \"bidir\"\n",
    "    wandb.run.name = name\n",
    "\n",
    "    if(wandb.config.rnn == \"LSTM\"):\n",
    "       rnn = nn.LSTM\n",
    "    if(wandb.config.rnn == \"GRU\"):\n",
    "       rnn = nn.GRU\n",
    "    if(wandb.config.rnn == \"RNN\"):\n",
    "       rnn = nn.RNN\n",
    "\n",
    "    enc = Encoder(english_dict_count, embedding_size, hidden_size, \n",
    "                num_layers=wandb.config.encoder_layers, \n",
    "                bi_dir=wandb.config.bi_directional,\n",
    "                p=wandb.config.enc_dropout,\n",
    "                rnn_class=rnn).to(device)\n",
    "    \n",
    "    dec = Decoder(target_dict_count, embedding_size, hidden_size, target_dict_count, \n",
    "                num_layers=wandb.config.decoder_layers, \n",
    "                bi_dir=wandb.config.bi_directional, \n",
    "                p =wandb.config.dec_dropout,\n",
    "                rnn_class=rnn).to(device)\n",
    "    \n",
    "    mod = Seq2Seq(enc, dec).to(device)\n",
    "    optimizer = optim.Adam(mod.parameters(), lr=learning_rate)\n",
    "    tr_loss, val_loss, tr_acc, val_acc = mod.learn(train, valid, wandb.config.num_epochs, optimizer)\n",
    "\n",
    "    for i in range(len(tr_loss)):\n",
    "      wandb.log({\"tr_loss\":tr_loss[i],\n",
    "                 \"tr_acc\" : tr_acc[i],\n",
    "                 \"val_loss\" : val_loss[i],\n",
    "                 \"val_acc\" : val_acc[i],\n",
    "                 \"epoch\":(i+1)})\n",
    "      \n",
    "    wandb.log({\"accuracy\": val_acc[-1]})\n",
    "\n",
    "\n",
    "sweep_id=wandb.sweep(sweep_config,project=\"CS6910_Assignment_3\")\n",
    "wandb.agent(sweep_id,function=tune_rnn,count=1)\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
