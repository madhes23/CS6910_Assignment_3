{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from Utils import plot_graphs, plot_attention_heatmaps\n",
    "import wandb\n",
    "from SequenceLearning import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters (for both attention and vanilla models)\n",
    "batch_size = 128\n",
    "input_size_encoder = english_char_count\n",
    "input_size_decoder = target_char_count\n",
    "output_size = target_char_count\n",
    "MAX_LEN = 35"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.load_data(\"train\", batch_size, num_batches=1)\n",
    "valid = data.load_data(\"valid\", batch_size, num_batches=1)\n",
    "test = data.load_data(  \"test\", batch_size, num_batches=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "sweep_config = {\n",
    "    \"method\": 'bayes',\n",
    "    \"metric\": {\n",
    "    'name': 'accuracy',\n",
    "    'goal': 'maximize'\n",
    "    },\n",
    "    'parameters' :{\n",
    "        \"num_epochs\" : {\"min\": 7, \"max\": 18}, \n",
    "        \"learning_rate\" : {\"values\": [1e-2, 1e-3]},\n",
    "        \"encoder_layers\": {\"values\": [1,2,3]},\n",
    "        \"decoder_layers\": {\"values\": [1,2,3]},\n",
    "        \"hidden_size\" : {\"values\": [64, 128, 256]},\n",
    "        \"rnn\" : {\"values\" : [\"LSTM\", \"GRU\", \"RNN\"]},\n",
    "        \"bi_directional\" : {\"values\": [True, False]},\n",
    "        \"dropout\" : {\"values\": [0.2, 0.3]},\n",
    "        \"embedding_size\" : {\"values\" : [32, 64, 128, 256]}}\n",
    "}\n",
    "\n",
    "def tune_rnn():\n",
    "   \"\"\"A utility function for performing the sweep\"\"\"\n",
    "   wandb.init()\n",
    "\n",
    "   if(wandb.config.rnn == \"LSTM\"):\n",
    "      rnn = nn.LSTM\n",
    "   if(wandb.config.rnn == \"GRU\"):\n",
    "      rnn = nn.GRU\n",
    "   if(wandb.config.rnn == \"RNN\"):\n",
    "      rnn = nn.RNN\n",
    "\n",
    "   enc = Encoder(english_char_count, wandb.config.embedding_size, wandb.config.hidden_size, \n",
    "               num_layers=wandb.config.encoder_layers, \n",
    "               bi_dir=wandb.config.bi_directional,\n",
    "               p=wandb.config.dropout,\n",
    "               rnn_class=rnn).to(device)\n",
    "\n",
    "   dec = Decoder(target_char_count, wandb.config.embedding_size, wandb.config.hidden_size, target_char_count, \n",
    "               num_layers=wandb.config.decoder_layers, \n",
    "               bi_dir=wandb.config.bi_directional, \n",
    "               p =wandb.config.dropout,\n",
    "               rnn_class=rnn).to(device)\n",
    "\n",
    "   mod = Seq2Seq(enc, dec).to(device)\n",
    "\n",
    "   optimizer = optim.Adam(mod.parameters(), lr=wandb.config.learning_rate)\n",
    "   tr_loss, val_loss, tr_acc, val_acc = mod.learn(train, valid, wandb.config.num_epochs, optimizer)\n",
    "\n",
    "\n",
    "   name = f\"{wandb.config.encoder_layers}_enc_{wandb.config.decoder_layers}_dec_{wandb.config.hidden_size}_hs_\"\n",
    "   if(wandb.config.bi_directional == True):\n",
    "      name += \"bidir_\"\n",
    "   if(dec.used_attn == True):\n",
    "      name += \"attn_\"\n",
    "   if(len(tr_loss) != wandb.config.num_epochs):\n",
    "      name += \"early_stop\"\n",
    "   wandb.run.name = name\n",
    "\n",
    "   for i in range(len(tr_loss)):\n",
    "      wandb.log({\"tr_loss\":tr_loss[i],\n",
    "                  \"tr_acc\" : tr_acc[i],\n",
    "                  \"val_loss\" : val_loss[i],\n",
    "                  \"val_acc\" : val_acc[i],\n",
    "                  \"epoch\":(i+1)})\n",
    "\n",
    "      wandb.log({\"accuracy\": val_acc[-1]})\n",
    "\n",
    "sweep_id=wandb.sweep(sweep_config,project=\"CS6910_Assignment_3\")\n",
    "wandb.agent(sweep_id,function=tune_rnn)\n",
    "wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "learning_rate = 0.001\n",
    "\n",
    "embedding_size = 32\n",
    "encoder_layers = 3\n",
    "decoder_layers = 2\n",
    "enc_dropout = 0.3\n",
    "dec_dropout = 0.3\n",
    "hidden_size = 256\n",
    "bi_directional = True\n",
    "rnn = nn.LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(english_char_count, \n",
    "              embedding_size, hidden_size, \n",
    "              num_layers=encoder_layers, \n",
    "              bi_dir=bi_directional,\n",
    "              p=enc_dropout,\n",
    "              rnn_class=rnn).to(device)\n",
    "\n",
    "dec = Decoder(target_char_count, embedding_size, hidden_size, target_char_count, \n",
    "              num_layers=decoder_layers, \n",
    "              bi_dir=bi_directional, \n",
    "              p = dec_dropout,\n",
    "              rnn_class=rnn).to(device)\n",
    "\n",
    "model = Seq2Seq(enc, dec).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, val_loss, acc, val_acc = model.learn(train, valid, num_epochs, optimizer)\n",
    "fig = plot_graphs(loss, val_loss, acc, val_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.calc_evaluation_metrics(test, path_to_store_predictions=\"predictions_attention/results.csv\")\n",
    "print(f\"Test dataset loss: {test_loss:.2f} \\nAccuracy: {test_acc}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.load_data(\"train\", batch_size, padding_upper_bound=MAX_LEN, num_batches=1)\n",
    "valid = data.load_data(\"valid\", batch_size, padding_upper_bound=MAX_LEN, num_batches=1)\n",
    "test = data.load_data(  \"test\", batch_size, padding_upper_bound=MAX_LEN, num_batches=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "sweep_config = {\n",
    "    \"method\": 'bayes',\n",
    "    \"metric\": {\n",
    "    'name': 'accuracy',\n",
    "    'goal': 'maximize'\n",
    "    },\n",
    "    'parameters' :{\n",
    "        \"num_epochs\" : {\"min\": 7, \"max\": 20}, \n",
    "        \"learning_rate\" : {\"values\": [1e-2, 1e-3]},\n",
    "        \"encoder_layers\": {\"values\": [1,2,3]},\n",
    "        \"decoder_layers\": {\"values\": [1,2,3]},\n",
    "        \"hidden_size\" : {\"values\": [64, 128, 256]},\n",
    "        \"rnn\" : {\"values\" : [\"LSTM\", \"GRU\", \"RNN\"]},\n",
    "        \"bi_directional\" : {\"values\": [True, False]},\n",
    "        \"dropout\" : {\"values\": [0.2, 0.3]},\n",
    "        \"embedding_size\" : {\"values\" : [32, 64, 128, 256]}}\n",
    "}\n",
    "\n",
    "def tune_rnn():\n",
    "    \"\"\"A utility function for performing the sweep\"\"\"\n",
    "    wandb.init()\n",
    "\n",
    "    if(wandb.config.rnn == \"LSTM\"):\n",
    "        rnn = nn.LSTM\n",
    "    if(wandb.config.rnn == \"GRU\"):\n",
    "        rnn = nn.GRU\n",
    "    if(wandb.config.rnn == \"RNN\"):\n",
    "        rnn = nn.RNN\n",
    "\n",
    "    enc = Encoder(english_char_count, wandb.config.embedding_size, wandb.config.hidden_size, \n",
    "               num_layers=wandb.config.encoder_layers, \n",
    "               bi_dir=wandb.config.bi_directional,\n",
    "               p=wandb.config.dropout,\n",
    "               rnn_class=rnn).to(device)\n",
    "\n",
    "    dec = AttnDecoder(wandb.config.embedding_size, wandb.config.hidden_size, output_size=target_char_count, \n",
    "                         num_layers=wandb.config.decoder_layers,\n",
    "                         dropout_p = wandb.config.dropout, \n",
    "                         bi_dir = wandb.config.bi_directional,\n",
    "                         rnn_class= rnn,\n",
    "                         max_length=MAX_LEN).to(device)\n",
    "\n",
    "    mod = Seq2Seq(enc, dec).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(mod.parameters(), lr=wandb.config.learning_rate)\n",
    "    tr_loss, val_loss, tr_acc, val_acc = mod.learn(train, valid, wandb.config.num_epochs, optimizer)\n",
    "\n",
    "\n",
    "    name = f\"{wandb.config.encoder_layers}_enc_{wandb.config.decoder_layers}_dec_{wandb.config.hidden_size}_hs_\"\n",
    "    if(wandb.config.bi_directional == True):\n",
    "        name += \"bidir_\"\n",
    "    if(dec.used_attn == True):\n",
    "        name += \"attn_\"\n",
    "    if(len(tr_loss) != wandb.config.num_epochs):\n",
    "        name += \"early_stop\"\n",
    "    wandb.run.name = name\n",
    "\n",
    "    for i in range(len(tr_loss)):\n",
    "        wandb.log({\"tr_loss\":tr_loss[i],\n",
    "                  \"tr_acc\" : tr_acc[i],\n",
    "                  \"val_loss\" : val_loss[i],\n",
    "                  \"val_acc\" : val_acc[i],\n",
    "                  \"epoch\":(i+1)})\n",
    "\n",
    "        wandb.log({\"accuracy\": val_acc[-1]})\n",
    "\n",
    "sweep_id=wandb.sweep(sweep_config,project=\"CS6910_Assignment_3_Attn\")\n",
    "wandb.agent(sweep_id,function=tune_rnn)\n",
    "wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "learning_rate = 0.001\n",
    "\n",
    "embedding_size = 128\n",
    "encoder_layers = 3\n",
    "decoder_layers = 2\n",
    "enc_dropout = 0.2\n",
    "dec_dropout = 0.2\n",
    "hidden_size = 256\n",
    "bi_directional = True\n",
    "rnn = nn.LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(english_char_count, \n",
    "              embedding_size, hidden_size, \n",
    "              num_layers=encoder_layers, \n",
    "              bi_dir=bi_directional,\n",
    "              p=enc_dropout,\n",
    "              rnn_class=rnn).to(device)\n",
    "\n",
    "dec = AttnDecoder(embedding_size, hidden_size, output_size=target_char_count, \n",
    "                     num_layers=decoder_layers,\n",
    "                     bi_dir = bi_directional,\n",
    "                          rnn_class= rnn,\n",
    "                     max_length=MAX_LEN).to(device)\n",
    "\n",
    "attn_model = Seq2Seq(enc, dec).to(device)\n",
    "\n",
    "optimizer = optim.Adam(attn_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, val_loss, acc, val_acc = attn_model.learn(train, valid, num_epochs, optimizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing attention heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test_data = data.load_data(\"test\", batch_size=12, padding_upper_bound=MAX_LEN, num_batches=1)\n",
    "src, tar = sample_test_data[0]\n",
    "src, tar = src.to(device), tar.to(device)\n",
    "\n",
    "src_strings = data.tensor_to_string(src, string_type=\"source\")\n",
    "tar_strings = data.tensor_to_string(tar, string_type=\"target\")\n",
    "attn = attn_model.get_attention_matrix(src, tar)\n",
    "\n",
    "fig = plot_attention_heatmaps(attn, src_strings, tar_strings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = attn_model.calc_evaluation_metrics(test)\n",
    "print(f\"Test dataset loss: {test_loss:.2f} \\nAccuracy: {test_acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
